============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
django: settings: posthog.settings (from ini)
rootdir: /workspaces/posthog, configfile: pytest.ini
plugins: celery-4.4.7, flaky-3.7.0, env-0.6.2, cov-2.12.1, icdiff-0.5, syrupy-1.4.6, asyncio-0.20.3, Faker-17.5.0, mock-3.5.1, django-4.1.0, split-0.6.0
asyncio: mode=strict
collected 2649 items
run-last-failure: no previously failed tests, not deselecting items.

hogvm/python/test/test_execute.py ....                                   [  0%]
posthog/api/dashboards/test/test_dashboard_templates.py ..............   [  0%]
posthog/api/test/test_action.py .................                        [  1%]
posthog/api/test/test_activity_log.py ..                                 [  1%]
posthog/api/test/test_annotation.py .........                            [  1%]
posthog/api/test/test_app_metrics.py ....                                [  1%]
posthog/api/test/test_async_migrations.py ........                       [  2%]
posthog/api/test/test_authentication.py ................................ [  3%]
                                                                         [  3%]
posthog/api/test/test_capture.py ....................................... [  4%]
..............FF.....                                                    [  5%]
posthog/api/test/test_cohort.py ..................                       [  6%]
posthog/api/test/test_decide.py ........................................ [  7%]
...............                                                          [  8%]
posthog/api/test/test_early_access_feature.py ..................         [  9%]
posthog/api/test/test_element.py ...............                         [  9%]
posthog/api/test/test_event.py ..............F.........                  [ 10%]
posthog/api/test/test_event_definition.py ........                       [ 10%]
posthog/api/test/test_exports.py ............                            [ 11%]
posthog/api/test/test_feature_flag.py .................................. [ 12%]
..................FFFFF.....                                             [ 13%]
posthog/api/test/test_geoip.py ...                                       [ 13%]
posthog/api/test/test_ingestion_warnings.py .                            [ 13%]
posthog/api/test/test_insight.py .......s.............................FF [ 15%]
..................................s.                                     [ 16%]
posthog/api/test/test_insight_funnels.py .................               [ 17%]
posthog/api/test/test_insight_query.py ..........                        [ 17%]
posthog/api/test/test_instance_settings.py .........                     [ 18%]
posthog/api/test/test_instance_status.py ...F...                         [ 18%]
posthog/api/test/test_kafka_inspector.py ..                              [ 18%]
posthog/api/test/test_organization.py .F.......                          [ 18%]
posthog/api/test/test_organization_domain.py ............F.FFF.....      [ 19%]
posthog/api/test/test_organization_invites.py ..........                 [ 19%]
posthog/api/test/test_organization_members.py ..........                 [ 20%]
posthog/api/test/test_person.py .............................            [ 21%]
posthog/api/test/test_personal_api_keys.py .............                 [ 21%]
posthog/api/test/test_persons_trends.py ....................FF.          [ 22%]
posthog/api/test/test_plugin.py .................F...................... [ 24%]
......                                                                   [ 24%]
posthog/api/test/test_preflight.py ...FFFF.....                          [ 24%]
posthog/api/test/test_prompt.py .....                                    [ 25%]
posthog/api/test/test_property_definition.py ............                [ 25%]
posthog/api/test/test_query.py ................FF                        [ 26%]
posthog/api/test/test_session_recordings.py ..........................   [ 27%]
posthog/api/test/test_sharing.py ................                        [ 27%]
posthog/api/test/test_signup.py ..FF..FF...F.F.FFFFF.........F.......... [ 29%]
..                                                                       [ 29%]
posthog/api/test/test_signup_demo.py FFFF                                [ 29%]
posthog/api/test/test_site_app.py ..                                     [ 29%]
posthog/api/test/test_survey.py .........                                [ 29%]
posthog/api/test/test_tagged_item.py ......                              [ 30%]
posthog/api/test/test_team.py ........................                   [ 31%]
posthog/api/test/test_uploaded_media.py ......                           [ 31%]
posthog/api/test/test_user.py .......................................... [ 32%]
..                                                                       [ 32%]
posthog/api/test/test_utils.py .......                                   [ 33%]
posthog/api/test/batch_exports/test_backfill.py EEEEE                    [ 33%]
posthog/api/test/batch_exports/test_create.py EE                         [ 33%]
posthog/api/test/batch_exports/test_delete.py EEE                        [ 33%]
posthog/api/test/batch_exports/test_get.py EEE                           [ 33%]
posthog/api/test/batch_exports/test_list.py EEE                          [ 33%]
posthog/api/test/batch_exports/test_pause.py EEEEEEE                     [ 34%]
posthog/api/test/batch_exports/test_runs.py EEE                          [ 34%]
posthog/api/test/batch_exports/test_update.py EE                         [ 34%]
posthog/api/test/dashboards/test_dashboard.py .......................... [ 35%]
.............................                                            [ 36%]
posthog/api/test/dashboards/test_dashboard_duplication.py ..             [ 36%]
posthog/api/test/dashboards/test_dashboard_text_tiles.py .....           [ 36%]
posthog/api/test/notebooks/test_notebook.py ...........                  [ 37%]
posthog/async_migrations/test/test_0007_persons_and_groups_on_events_backfill.py . [ 37%]
..............                                                           [ 37%]
posthog/async_migrations/test/test_0010_move_old_partitions.py .         [ 37%]
posthog/async_migrations/test/test_definition.py ..                      [ 37%]
posthog/async_migrations/test/test_migrations_not_required.py .          [ 37%]
posthog/async_migrations/test/test_runner.py ......                      [ 38%]
posthog/async_migrations/test/test_utils.py .......                      [ 38%]
posthog/caching/test/test_fetch_from_cache.py .....                      [ 38%]
posthog/caching/test/test_insight_cache.py ...................           [ 39%]
posthog/caching/test/test_insight_caching_state.py ..................... [ 39%]
...............                                                          [ 40%]
posthog/caching/test/test_should_refresh_insight.py ..........           [ 40%]
posthog/caching/test/test_tolerant_zlib_compressor.py ......             [ 41%]
posthog/clickhouse/test/test_person_overrides.py F.                      [ 41%]
posthog/demo/test/test_matrix_manager.py ....                            [ 41%]
posthog/hogql/database/test/test_argmax.py ..                            [ 41%]
posthog/hogql/database/test/test_database.py ...                         [ 41%]
posthog/hogql/database/test/test_s3_table.py .....                       [ 41%]
posthog/hogql/functions/test/test_cohort.py ....                         [ 41%]
posthog/hogql/functions/test/test_sparkline.py ..                        [ 41%]
posthog/hogql/test/test_bytecode.py ..                                   [ 42%]
posthog/hogql/test/test_escape_sql.py ....                               [ 42%]
posthog/hogql/test/test_metadata.py .....                                [ 42%]
posthog/hogql/test/test_parse_string.py ......                           [ 42%]
posthog/hogql/test/test_parser.py ...................................... [ 44%]
...........                                                              [ 44%]
posthog/hogql/test/test_placeholders.py ....                             [ 44%]
posthog/hogql/test/test_printer.py ..................................... [ 46%]
................                                                         [ 46%]
posthog/hogql/test/test_property.py .................                    [ 47%]
posthog/hogql/test/test_query.py ..F..............................       [ 48%]
posthog/hogql/test/test_resolver.py ................................     [ 49%]
posthog/hogql/test/test_visitor.py .....                                 [ 49%]
posthog/hogql/transforms/test/test_lazy_tables.py ...........            [ 50%]
posthog/hogql/transforms/test/test_property_types.py ......              [ 50%]
posthog/kafka_client/test/test_client.py ......                          [ 50%]
posthog/management/commands/test/test_backfill_persons_and_groups_on_events.py . [ 50%]
.                                                                        [ 50%]
posthog/management/commands/test/test_fix_person_distinct_ids_after_delete.py . [ 50%]
..                                                                       [ 50%]
posthog/management/commands/test/test_run_async_migrations.py .....      [ 51%]
posthog/management/commands/test/test_sync_persons_to_clickhouse.py .... [ 51%]
.......                                                                  [ 51%]
posthog/management/commands/test/test_sync_replicated_schema.py ...      [ 51%]
posthog/models/async_deletion/test_delete_person.py .                    [ 51%]
posthog/models/cohort/test/test_util.py F...........                     [ 52%]
posthog/models/filters/test/test_filter.py ............................. [ 53%]
...                                                                      [ 53%]
posthog/models/filters/test/test_filter_mixins.py ..                     [ 53%]
posthog/models/filters/test/test_lifecycle_filter.py .                   [ 53%]
posthog/models/filters/test/test_path_filter.py ..                       [ 53%]
posthog/models/filters/test/test_retention_filter.py ..                  [ 53%]
posthog/models/filters/test/test_stickiness_filter.py .                  [ 53%]
posthog/models/test/test_activity_logging.py .                           [ 53%]
posthog/models/test/test_async_deletion_model.py ................        [ 54%]
posthog/models/test/test_dashboard_tile_model.py ....                    [ 54%]
posthog/models/test/test_entity_model.py .......                         [ 54%]
posthog/models/test/test_event_model.py ............                     [ 55%]
posthog/models/test/test_exported_asset_model.py .....                   [ 55%]
posthog/models/test/test_insight_caching_state.py .....                  [ 55%]
posthog/models/test/test_insight_model.py ...........                    [ 55%]
posthog/models/test/test_integration_model.py .                          [ 56%]
posthog/models/test/test_organization_model.py .FFF.                     [ 56%]
posthog/models/test/test_person_model.py ...                             [ 56%]
posthog/models/test/test_subscription_model.py .............             [ 56%]
posthog/models/test/test_tagged_item_model.py .......                    [ 57%]
posthog/models/test/test_user_model.py F.                                [ 57%]
posthog/plugins/test/test_utils.py F...F...                              [ 57%]
posthog/queries/app_metrics/test/test_app_metrics.py ............        [ 57%]
posthog/queries/app_metrics/test/test_historical_exports.py ......       [ 58%]
posthog/queries/funnels/test/test_breakdowns_by_current_url.py ..        [ 58%]
posthog/queries/funnels/test/test_funnel.py ............................ [ 59%]
............................s..................                          [ 61%]
posthog/queries/funnels/test/test_funnel_persons.py .............        [ 61%]
posthog/queries/funnels/test/test_funnel_strict.py ..................... [ 62%]
............                                                             [ 62%]
posthog/queries/funnels/test/test_funnel_strict_persons.py .....         [ 62%]
posthog/queries/funnels/test/test_funnel_time_to_convert.py ......       [ 63%]
posthog/queries/funnels/test/test_funnel_trends.py ..................... [ 63%]
.                                                                        [ 64%]
posthog/queries/funnels/test/test_funnel_trends_persons.py ...           [ 64%]
posthog/queries/funnels/test/test_funnel_unordered.py .................. [ 64%]
..................                                                       [ 65%]
posthog/queries/funnels/test/test_funnel_unordered_persons.py ......     [ 65%]
posthog/queries/funnels/test/test_utils.py ....                          [ 65%]
posthog/queries/session_recordings/test/test_session_recording.py ...... [ 66%]
....                                                                     [ 66%]
posthog/queries/session_recordings/test/test_session_recording_list.py . [ 66%]
............................................                             [ 67%]
posthog/queries/session_recordings/test/test_session_recording_list_from_session_replay.py F [ 67%]
.............s.............                                              [ 69%]
posthog/queries/session_recordings/test/test_session_recording_properties.py . [ 69%]
                                                                         [ 69%]
posthog/queries/session_recordings/test/test_session_replay_summaries.py . [ 69%]
                                                                         [ 69%]
posthog/queries/test/test_base.py ........                               [ 69%]
posthog/queries/test/test_lifecycle.py ...............                   [ 69%]
posthog/queries/test/test_paths.py .......                               [ 70%]
posthog/queries/test/test_query_date_range.py ......                     [ 70%]
posthog/queries/test/test_retention.py .........................         [ 71%]
posthog/queries/test/test_trends.py ............FFFF.................... [ 72%]
..FFF................F.FFF.............................................. [ 75%]
........................................F............................... [ 78%]
....                                                                     [ 78%]
posthog/queries/test/test_util.py .                                      [ 78%]
posthog/queries/trends/test/test_breakdowns.py ...........               [ 78%]
posthog/queries/trends/test/test_breakdowns_by_current_url.py ..         [ 78%]
posthog/queries/trends/test/test_formula.py ......F.................     [ 79%]
posthog/queries/trends/test/test_paging_breakdowns.py ..                 [ 79%]
posthog/queries/trends/test/test_person.py F..                           [ 79%]
posthog/storage/test/test_object_storage.py .......                      [ 80%]
posthog/tasks/exports/test/test_csv_exporter.py ............             [ 80%]
posthog/tasks/exports/test/test_csv_exporter_renders.py ...........      [ 81%]
posthog/tasks/exports/test/test_csv_exporter_url_sanitising.py ......... [ 81%]
.                                                                        [ 81%]
posthog/tasks/exports/test/test_image_exporter.py ...                    [ 81%]
posthog/tasks/test/test_async_migrations.py ..                           [ 81%]
posthog/tasks/test/test_calculate_event_property_usage.py ........       [ 81%]
posthog/tasks/test/test_email.py .........                               [ 82%]
posthog/tasks/test/test_exporter.py .F                                   [ 82%]
posthog/temporal/tests/test_squash_person_overrides_workflow.py F....... [ 82%]
..........FFF                                                            [ 83%]
posthog/temporal/tests/batch_exports/test_s3_batch_export_workflow.py .F [ 83%]
                                                                         [ 83%]
posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py F [ 83%]
FF                                                                       [ 83%]
posthog/test/test_cache_utils.py ...                                     [ 83%]
posthog/test/test_cohort_model.py .......                                [ 83%]
posthog/test/test_database_healthcheck.py ..                             [ 83%]
posthog/test/test_dbrouter.py .                                          [ 83%]
posthog/test/test_element_model.py ..                                    [ 83%]
posthog/test/test_email.py ...                                           [ 84%]
posthog/test/test_feature_flag.py ..................s................... [ 85%]
............s..............                                              [ 86%]
posthog/test/test_feature_flag_analytics.py ..sss..                      [ 86%]
posthog/test/test_gzip_middleware.py s....                               [ 86%]
posthog/test/test_health.py ........                                     [ 87%]
posthog/test/test_instance_setting_model.py .....                        [ 87%]
posthog/test/test_middleware.py ......F.F...FFF.F.                       [ 88%]
posthog/test/test_migration_0219.py s                                    [ 88%]
posthog/test/test_migration_0220.py s                                    [ 88%]
posthog/test/test_migration_0222.py s                                    [ 88%]
posthog/test/test_migration_0227.py s                                    [ 88%]
posthog/test/test_migration_0228.py s                                    [ 88%]
posthog/test/test_migration_0259.py s                                    [ 88%]
posthog/test/test_migration_0273.py .                                    [ 88%]
posthog/test/test_plugin.py .............                                [ 88%]
posthog/test/test_plugin_log_entry.py ....                               [ 89%]
posthog/test/test_rate_limit.py ............                             [ 89%]
posthog/test/test_redis.py ...                                           [ 89%]
posthog/test/test_team.py .....F.                                        [ 89%]
posthog/test/test_templatetags.py ..                                     [ 89%]
posthog/test/test_urls.py ....                                           [ 90%]
posthog/test/test_utils.py .................................             [ 91%]
posthog/test/test_version_requirement.py ...                             [ 91%]
posthog/test/activity_logging/test_activity_logging.py .....             [ 91%]
posthog/test/activity_logging/test_feature_flag_activity_logging.py .... [ 91%]
.....                                                                    [ 91%]
posthog/test/activity_logging/test_insight_activity_logging.py .....     [ 92%]
posthog/warehouse/api/test/test_table.py ..                              [ 92%]
posthog/warehouse/models/test/test_table.py ..                           [ 92%]
posthog/api/test/test_decide.py ssssss                                   [ 92%]
posthog/api/test/test_feature_flag.py FEFEFEFEFEFE                       [ 92%]
posthog/api/test/batch_exports/test_reset.py EE                          [ 92%]
posthog/models/test/test_person_override_model.py FEFEFEFEFEFEFEFEFEFEFE [ 93%]
                                                                         [ 93%]
posthog/temporal/tests/batch_exports/test_run_updates.py FEFE            [ 93%]
posthog/test/test_feature_flag.py FEFEFEFE                               [ 93%]
posthog/test/test_migration_0218.py s                                    [ 93%]
posthog/test/test_migration_0287.py F                                    [ 93%]
posthog/api/test/test_geoip.py ...                                       [ 93%]
posthog/api/test/test_sharing.py .....                                   [ 93%]
posthog/clickhouse/client/test/test_connection.py ...                    [ 93%]
posthog/clickhouse/test/test_schema.py ................................. [ 95%]
..............................................                           [ 96%]
posthog/helpers/tests/test_multi_property_breakdown.py ....              [ 97%]
posthog/helpers/tests/test_session_recording_helpers.py ................ [ 97%]
..                                                                       [ 97%]
posthog/logging/test/test_timing.py .                                    [ 97%]
posthog/management/commands/test/test_migrate_kafka_data.py ......       [ 97%]
posthog/models/filters/mixins/test/test_groups.py .                      [ 98%]
posthog/models/filters/mixins/test/test_interval.py .........            [ 98%]
posthog/models/filters/mixins/test/test_property.py .........            [ 98%]
posthog/queries/time_to_see_data/test/test_hierarchy.py ................ [ 99%]
.                                                                        [ 99%]
posthog/queries/time_to_see_data/test/test_sessions.py ...               [ 99%]
posthog/tasks/test/test_check_clickhouse_schema_drift.py ....            [ 99%]
posthog/temporal/tests/test_encryption_codec.py F                        [ 99%]
posthog/temporal/tests/test_squash_person_overrides_workflow.py ....     [ 99%]
posthog/test/test_celery.py .                                            [ 99%]
posthog/test/test_health.py .                                            [ 99%]
posthog/test/test_latest_migrations.py F.                                [ 99%]
posthog/test/activity_logging/test_person_activity_logging.py .          [100%]

==================================== ERRORS ====================================
_________________ ERROR at setup of test_batch_export_backfill _________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___ ERROR at setup of test_batch_export_backfill_with_non_isoformatted_dates ___

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___ ERROR at setup of test_batch_export_backfill_with_start_at_after_end_at ____

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___ ERROR at setup of test_cannot_trigger_backfill_for_another_organization ____

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________ ERROR at setup of test_backfill_is_partitioned_by_team_id ___________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______ ERROR at setup of test_create_batch_export_with_interval_schedule _______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_ ERROR at setup of test_cannot_create_a_batch_export_for_another_organization _

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________________ ERROR at setup of test_delete_batch_export __________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______ ERROR at setup of test_cannot_delete_export_of_other_organizations ______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________ ERROR at setup of test_deletes_are_partitioned_by_team_id ___________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
________ ERROR at setup of test_can_get_exports_for_your_organizations _________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______ ERROR at setup of test_cannot_get_exports_for_other_organizations _______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_________ ERROR at setup of test_batch_exports_are_partitioned_by_team _________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________________ ERROR at setup of test_list_batch_exports ___________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___ ERROR at setup of test_cannot_list_batch_exports_for_other_organizations ___

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______________ ERROR at setup of test_list_is_partitioned_by_team ______________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
____________ ERROR at setup of test_pause_and_unpause_batch_export _____________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_ ERROR at setup of test_connot_pause_and_unpause_batch_exports_of_other_organizations _

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_____ ERROR at setup of test_pause_and_unpause_are_partitioned_by_team_id ______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_______ ERROR at setup of test_pause_batch_export_that_is_already_paused _______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_____ ERROR at setup of test_unpause_batch_export_that_is_already_unpaused _____

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
____________ ERROR at setup of test_pause_non_existent_batch_export ____________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
____________ ERROR at setup of test_unpause_can_trigger_a_backfill _____________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______ ERROR at setup of test_can_get_export_runs_for_your_organizations _______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
______ ERROR at setup of test_cannot_get_exports_for_other_organizations _______

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_________ ERROR at setup of test_batch_exports_are_partitioned_by_team _________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
____________________ ERROR at setup of test_can_put_config _____________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___________________ ERROR at setup of test_can_patch_config ____________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_a_working_slow_db _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7446c41b40>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f41ef940>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446c40550>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_a_working_slow_db>
result = <TestCaseFunction test_feature_flags_v3_with_a_working_slow_db>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7446c41b40>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_experience_continuity_working_slow_db _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7447074940>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f68ebec0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447076bf0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_experience_continuity_working_slow_db>
result = <TestCaseFunction test_feature_flags_v3_with_experience_continuity_working_slow_db>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7447074940>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_group_properties _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74470c3880>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f63d7e40>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74470c2320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_group_properties>
result = <TestCaseFunction test_feature_flags_v3_with_group_properties>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74470c3880>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_group_properties_and_slow_db _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74466d1b70>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f5d69340>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f59895a0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_group_properties_and_slow_db>
result = <TestCaseFunction test_feature_flags_v3_with_group_properties_and_slow_db>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74466d1b70>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_person_properties _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74844e6b00>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74dc4e7200>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74471f9150>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_person_properties>
result = <TestCaseFunction test_feature_flags_v3_with_person_properties>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74844e6b00>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestResiliency.test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again _

self = <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7446fcb820>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74a8117ec0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f750863b880>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again>
result = <TestCaseFunction test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7446fcb820>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_________________ ERROR at setup of test_can_reset_export_run __________________

    @pytest.fixture(autouse=True)
    def temporal():
        """Return a TemporalClient instance."""
>       client = sync_connect()

posthog/api/test/batch_exports/conftest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
________________ ERROR at teardown of test_can_reset_export_run ________________

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74f6acc040>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74dc45e440>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085d7310>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <django.test.testcases.TransactionTestCase testMethod=__init__>

    def _post_teardown(self):
        """
        Perform post-test things:
        * Flush the contents of the database to leave a clean slate. If the
          class has an 'available_apps' attribute, don't fire post_migrate.
        * Force-close the connection so the next test gets a clean cursor.
        """
        try:
>           self._fixture_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:1006: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74f6acc040>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_allows_duplicate_override_person_id _

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f75085bdc00>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f7508619dc0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085beb30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_duplicate_override_person_id>
result = <TestCaseFunction test_person_override_allows_duplicate_override_person_id>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f75085bdc00>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_allows_override_person_id_as_old_person_id_in_different_teams _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7444a3a2c0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f75083e14c0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a3a320>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_override_person_id_as_old_person_id_in_different_teams>
result = <TestCaseFunction test_person_override_allows_override_person_id_as_old_person_id_in_different_teams>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7444a3a2c0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_disallows_old_person_id_as_override_person_id _

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f75085bd630>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74846e4b00>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75085bdea0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_old_person_id_as_override_person_id>
result = <TestCaseFunction test_person_override_disallows_old_person_id_as_override_person_id>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f75085bd630>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_disallows_override_person_id_as_old_person_id _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74f6d868c0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f7484739d80>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d86da0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_override_person_id_as_old_person_id>
result = <TestCaseFunction test_person_override_disallows_override_person_id_as_old_person_id>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74f6d868c0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_disallows_same_old_person_id _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74462e0520>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f7444b7f580>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74462e2ad0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_same_old_person_id>
result = <TestCaseFunction test_person_override_disallows_same_old_person_id>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74462e0520>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_old_person_id_as_override_person_id_in_different_teams _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f75084e3310>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f657f840>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844e4670>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_old_person_id_as_override_person_id_in_different_teams>
result = <TestCaseFunction test_person_override_old_person_id_as_override_person_id_in_different_teams>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f75084e3310>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverride.test_person_override_same_old_person_id_in_different_teams _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7444acf040>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f6bc0880>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444acfd60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_same_old_person_id_in_different_teams>
result = <TestCaseFunction test_person_override_same_old_person_id_in_different_teams>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7444acf040>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverrideConcurrency.test_person_override_allow_consecutive_merges _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74f6d84160>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f75081e9580>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d85450>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_allow_consecutive_merges>
result = <TestCaseFunction test_person_override_allow_consecutive_merges>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74f6d84160>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f75084e1090>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f7508608680>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447130a30>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge>
result = <TestCaseFunction test_person_override_disallows_concurrent_merge>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f75084e1090>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge_different_order _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7446b94640>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74644cce80>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7446b944f0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge_different_order>
result = <TestCaseFunction test_person_override_disallows_concurrent_merge_different_order>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7446b94640>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestPersonOverrideConcurrency.test_person_override_merge _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74f6d2a4a0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f750896a4c0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6d2a590>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_merge>
result = <TestCaseFunction test_person_override_merge>, debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74f6d2a4a0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
__________ ERROR at teardown of RunUpdatesTest.test_create_export_run __________

self = <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7508657ac0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74847f18c0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7508657f40>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <test_run_updates.RunUpdatesTest testMethod=test_create_export_run>
result = <TestCaseFunction test_create_export_run>, debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7508657ac0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
______ ERROR at teardown of RunUpdatesTest.test_update_export_run_status _______

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7444a27bb0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f7444b3c100>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a26b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <test_run_updates.RunUpdatesTest testMethod=test_update_export_run_status>
result = <TestCaseFunction test_update_export_run_status>, debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7444a27bb0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_race_conditions _

self = <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74844d4e80>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74a85303c0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f75086980d0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_race_conditions>
result = <TestCaseFunction test_hash_key_overrides_with_race_conditions>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74844d4e80>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f7447023490>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f6cee440>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447022b60>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion>
result = <TestCaseFunction test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f7447023490>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f744776bc10>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f5777dc0>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f63d9cc0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging>
result = <TestCaseFunction test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f744776bc10>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_ ERROR at teardown of TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_simulated_race_conditions_on_person_merging _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74f692dde0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f74f637b500>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7444a90940>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_simulated_race_conditions_on_person_merging>
result = <TestCaseFunction test_hash_key_overrides_with_simulated_race_conditions_on_person_merging>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74f692dde0>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
=================================== FAILURES ===================================
________________________ TestCapture.test_quota_limits _________________________

self = <posthog.api.test.test_capture.TestCapture testMethod=test_quota_limits>
kafka_produce = <MagicMock name='produce' id='140140597312048'>

    @patch("posthog.kafka_client.client._KafkaProducer.produce")
    @pytest.mark.ee
    def test_quota_limits(self, kafka_produce) -> None:
>       from ee.billing.quota_limiting import QuotaResource, replace_limited_team_tokens
E       ModuleNotFoundError: No module named 'ee'

posthog/api/test/test_capture.py:1420: ModuleNotFoundError
______________ TestCapture.test_quota_limits_ignored_if_disabled _______________

self = <posthog.api.test.test_capture.TestCapture testMethod=test_quota_limits_ignored_if_disabled>
kafka_produce = <MagicMock name='produce' id='140140598122560'>

    @patch("posthog.kafka_client.client._KafkaProducer.produce")
    @pytest.mark.ee
    def test_quota_limits_ignored_if_disabled(self, kafka_produce) -> None:
>       from ee.billing.quota_limiting import QuotaResource, replace_limited_team_tokens
E       ModuleNotFoundError: No module named 'ee'

posthog/api/test/test_capture.py:1410: ModuleNotFoundError
_________________ TestEvents.test_filter_events_by_properties __________________

self = <posthog.api.test.test_event.TestEvents testMethod=test_filter_events_by_properties>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_filter_events_by_properties(self):
        _create_person(properties={"email": "tim@posthog.com"}, team=self.team, distinct_ids=["2", "some-random-uid"])
        _create_event(event="event_name", team=self.team, distinct_id="2", properties={"$browser": "Chrome"})
        event2_uuid = _create_event(
            event="event_name", team=self.team, distinct_id="2", properties={"$browser": "Safari"}
        )
        flush_persons_and_events()
    
        # Django session, PostHog user, PostHog team, PostHog org membership,
        # look up if rate limit is enabled (cached after first lookup), 5x non-cached instance
        # setting (poe, rate limit), person and distinct id
        expected_queries = 12
    
>       with self.assertNumQueries(expected_queries):

posthog/api/test/test_event.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 10 != 12 : 10 queries executed, 12 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:11:10.703906+00:00'::timestamptz AND "django_session"."session_key" = 'p9d5jwqkgyzzlfvfqrp1p9q9dlg8cgch') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 1853 LIMIT 21 /**/
E   3. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."id" = 1974 LIMIT 21 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   4. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 1853 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   5. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:RATE_LIMIT_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   6. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   7. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   8. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   9. SELECT "posthog_person"."id", "posthog_person"."created_at", "posthog_person"."properties_last_updated_at", "posthog_person"."properties_last_operation", "posthog_person"."team_id", "posthog_person"."properties", "posthog_person"."is_user_id", "posthog_person"."is_identified", "posthog_person"."uuid", "posthog_person"."version" FROM "posthog_person" INNER JOIN "posthog_persondistinctid" ON ("posthog_person"."id" = "posthog_persondistinctid"."person_id") WHERE ("posthog_persondistinctid"."distinct_id" IN ('2') AND "posthog_persondistinctid"."team_id" = 1974 AND "posthog_person"."team_id" = 1974) /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   10. SELECT "posthog_persondistinctid"."id", "posthog_persondistinctid"."team_id", "posthog_persondistinctid"."person_id", "posthog_persondistinctid"."distinct_id", "posthog_persondistinctid"."version" FROM "posthog_persondistinctid" WHERE "posthog_persondistinctid"."person_id" IN (3812) /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
______________ TestBlastRadius.test_user_blast_radius_with_groups ______________

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups>

    @snapshot_clickhouse_queries
    def test_user_blast_radius_with_groups(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3],
                            "operator": "exact",
                            "group_type_index": 0,
                        }
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 4, "total_users": 10}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 4, actual: 0,'total_users', expected: 10, actual: 0

posthog/api/test/test_feature_flag.py:2649: AssertionError
_______ TestBlastRadius.test_user_blast_radius_with_groups_all_selected ________

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_all_selected>

    def test_user_blast_radius_with_groups_all_selected(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(5):
            create_group(
                team_id=self.team.pk, group_type_index=1, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [],
                    "rollout_percentage": 25,
                },
                "group_type_index": 1,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 5, "total_users": 5}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 5, actual: 0,'total_users', expected: 5, actual: 0

posthog/api/test/test_feature_flag.py:2700: AssertionError
___ TestBlastRadius.test_user_blast_radius_with_groups_incorrect_group_type ____

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_incorrect_group_type>

    def test_user_blast_radius_with_groups_incorrect_group_type(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3, 4],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [2, 3, 4, 5, 6],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 1,
            },
        )
    
>       self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
E       AssertionError: 200 != 400

posthog/api/test/test_feature_flag.py:2778: AssertionError
_____ TestBlastRadius.test_user_blast_radius_with_groups_multiple_queries ______

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_multiple_queries>

    @snapshot_clickhouse_queries
    def test_user_blast_radius_with_groups_multiple_queries(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3, 4],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [2, 3, 4, 5, 6],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 3, "total_users": 10}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 3, actual: 0,'total_users', expected: 10, actual: 0

posthog/api/test/test_feature_flag.py:2741: AssertionError
_______ TestBlastRadius.test_user_blast_radius_with_groups_zero_selected _______

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_zero_selected>

    def test_user_blast_radius_with_groups_zero_selected(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
    
        for i in range(5):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {"key": "industry", "type": "group", "value": [8], "operator": "exact", "group_type_index": 0}
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 0, "total_users": 5}, response_json)
E       AssertionError: Mismatched values: 'total_users', expected: 5, actual: 0

posthog/api/test/test_feature_flag.py:2675: AssertionError
_______________ TestInsight.test_insight_funnels_hogql_breakdown _______________

self = <posthog.api.test.test_insight.TestInsight testMethod=test_insight_funnels_hogql_breakdown>

    @snapshot_clickhouse_queries
    @also_test_with_materialized_columns(event_properties=["int_value"], person_properties=["fish"])
    def test_insight_funnels_hogql_breakdown(self) -> None:
        with freeze_time("2012-01-15T04:01:34.000Z"):
            _create_person(team=self.team, distinct_ids=["1"], properties={"fish": "there is no fish"})
            _create_event(team=self.team, event="user signed up", distinct_id="1", properties={"int_value": 1})
            _create_event(team=self.team, event="user did things", distinct_id="1", properties={"int_value": 20})
            response = self.client.post(
                f"/api/projects/{self.team.id}/insights/funnel/",
                {
                    "breakdown_type": "hogql",
                    "breakdowns": [{"property": "person.properties.fish", "type": "hogql"}],
                    "events": [
                        {"id": "user signed up", "type": "events", "order": 0},
                        {"id": "user did things", "type": "events", "order": 1},
                    ],
                    "properties": json.dumps(
                        [
                            {"key": "toInt(properties.int_value) < 10 and 'bla' != 'a%sd'", "type": "hogql"},
                        ]
                    ),
                    "funnel_window_days": 14,
                },
            )
>           self.assertEqual(response.status_code, status.HTTP_200_OK)
E           AssertionError: 500 != 200

posthog/api/test/test_insight.py:2445: AssertionError
----------------------------- Captured stderr call -----------------------------
2023-07-10T13:12:01.284011Z [error    ] Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?
 [posthog.exceptions] host= ip=127.0.0.1 path=/api/projects/2051/insights/funnel/ pid=152128 request_id=8c45c17e-fb1f-4c5c-bb1a-fb63012f28b3 team_id=2051 tid=140141459998528 token=token123 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute
    result = client.execute(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1188, in _execute_mock_call
    return self._mock_wraps(*args, **kwargs)
  File "/workspaces/posthog/posthog/test/base.py", line 677, in execute_wrapper
    return original_client_execute(query, *args, **kwargs)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute
    rv = self.process_ordinary_query(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query
    return self.receive_result(with_column_types=with_column_types,
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result
    return result.get_result()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result
    for packet in self.packet_generator:
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator
    packet = self.receive_packet()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet
    raise packet.exception
clickhouse_driver.errors.ServerException: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel
    funnel = self.calculate_funnel(request)
  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper
    fresh_result_package = cast(T, f(self, request))
  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel
    "result": funnel_order_class(team=team, filter=filter).run(),
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run
    results = self._exec_query()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query
    query = self.get_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query
    {self.get_step_counts_query()}
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query
    steps_per_person_query = self.get_step_counts_without_aggregation_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query
    formatted_query = self.build_step_subquery(2, max_steps)
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery
    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query
    values = self._get_breakdown_conditions()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions
    return get_breakdown_prop_values(
  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values
    return insight_sync_execute(
  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner
    return inner._impl(*args, **kwargs)  # type: ignore
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute
    raise err
posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?

2023-07-10T13:12:01.288313Z [error    ] Internal Server Error: /api/projects/2051/insights/funnel/ [django.request] host= pid=152128 team_id=2051 tid=140141459998528 token=token123 x_forwarded_for=
2023-07-10T13:12:01.288604Z [error    ] Internal Server Error: /api/projects/2051/insights/funnel/ [django.request] host= pid=152128 team_id=2051 tid=140141459998528 token=token123 x_forwarded_for=
------------------------------ Captured log call -------------------------------
ERROR    posthog.exceptions:exceptions.py:56 {'request_id': '8c45c17e-fb1f-4c5c-bb1a-fb63012f28b3', 'ip': '127.0.0.1', 'path': '/api/projects/2051/insights/funnel/', 'event': CHQueryErrorUnknownIdentifier('DB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n'), 'token': 'token123', 'host': '', 'x_forwarded_for': '', 'team_id': 2051, 'timestamp': '2023-07-10T13:12:01.284011Z', 'logger': 'posthog.exceptions', 'level': 'error', 'pid': 152128, 'tid': 140141459998528, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute\n    result = client.execute(\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1188, in _execute_mock_call\n    return self._mock_wraps(*args, **kwargs)\n  File "/workspaces/posthog/posthog/test/base.py", line 677, in execute_wrapper\n    return original_client_execute(query, *args, **kwargs)\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute\n    rv = self.process_ordinary_query(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query\n    return self.receive_result(with_column_types=with_column_types,\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result\n    return result.get_result()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result\n    for packet in self.packet_generator:\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator\n    packet = self.receive_packet()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet\n    raise packet.exception\nclickhouse_driver.errors.ServerException: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch\n    response = handler(request, *args, **kwargs)\n  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel\n    funnel = self.calculate_funnel(request)\n  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper\n    fresh_result_package = cast(T, f(self, request))\n  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel\n    "result": funnel_order_class(team=team, filter=filter).run(),\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run\n    results = self._exec_query()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query\n    query = self.get_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query\n    {self.get_step_counts_query()}\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query\n    steps_per_person_query = self.get_step_counts_without_aggregation_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query\n    formatted_query = self.build_step_subquery(2, max_steps)\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery\n    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query\n    values = self._get_breakdown_conditions()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions\n    return get_breakdown_prop_values(\n  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values\n    return insight_sync_execute(\n  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute\n    return sync_execute(query, args=args, team_id=team_id, **kwargs)\n  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner\n    return inner._impl(*args, **kwargs)  # type: ignore\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute\n    raise err\nposthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2051) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n'}
ERROR    django.request:log.py:224 Internal Server Error: /api/projects/2051/insights/funnel/
___________ TestInsight.test_insight_funnels_hogql_breakdown_single ____________

self = <posthog.api.test.test_insight.TestInsight testMethod=test_insight_funnels_hogql_breakdown_single>

    @also_test_with_materialized_columns(event_properties=["int_value"], person_properties=["fish"])
    def test_insight_funnels_hogql_breakdown_single(self) -> None:
        with freeze_time("2012-01-15T04:01:34.000Z"):
            _create_person(team=self.team, distinct_ids=["1"], properties={"fish": "there is no fish"})
            _create_event(team=self.team, event="user signed up", distinct_id="1", properties={"int_value": 1})
            _create_event(team=self.team, event="user did things", distinct_id="1", properties={"int_value": 20})
            response = self.client.post(
                f"/api/projects/{self.team.id}/insights/funnel/",
                {
                    "breakdown_type": "hogql",
                    "breakdown": "person.properties.fish",
                    "events": [
                        {"id": "user signed up", "type": "events", "order": 0},
                        {"id": "user did things", "type": "events", "order": 1},
                    ],
                    "properties": json.dumps(
                        [
                            {"key": "toInt(properties.int_value) < 10 and 'bla' != 'a%sd'", "type": "hogql"},
                        ]
                    ),
                    "funnel_window_days": 14,
                },
            )
>           self.assertEqual(response.status_code, status.HTTP_200_OK)
E           AssertionError: 500 != 200

posthog/api/test/test_insight.py:2483: AssertionError
----------------------------- Captured stderr call -----------------------------
2023-07-10T13:12:01.919484Z [error    ] Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?
 [posthog.exceptions] host= ip=127.0.0.1 path=/api/projects/2052/insights/funnel/ pid=152128 request_id=1a34e997-d132-4340-80da-14dd4bfbcc04 team_id=2052 tid=140141459998528 token=token123 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute
    result = client.execute(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute
    rv = self.process_ordinary_query(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query
    return self.receive_result(with_column_types=with_column_types,
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result
    return result.get_result()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result
    for packet in self.packet_generator:
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator
    packet = self.receive_packet()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet
    raise packet.exception
clickhouse_driver.errors.ServerException: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel
    funnel = self.calculate_funnel(request)
  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper
    fresh_result_package = cast(T, f(self, request))
  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel
    "result": funnel_order_class(team=team, filter=filter).run(),
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run
    results = self._exec_query()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query
    query = self.get_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query
    {self.get_step_counts_query()}
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query
    steps_per_person_query = self.get_step_counts_without_aggregation_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query
    formatted_query = self.build_step_subquery(2, max_steps)
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery
    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query
    values = self._get_breakdown_conditions()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions
    return get_breakdown_prop_values(
  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values
    return insight_sync_execute(
  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner
    return inner._impl(*args, **kwargs)  # type: ignore
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute
    raise err
posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f2f2fccd609 in ?
24. __clone @ 0x00007f2f2fbf2133 in ?

2023-07-10T13:12:01.922619Z [error    ] Internal Server Error: /api/projects/2052/insights/funnel/ [django.request] host= pid=152128 team_id=2052 tid=140141459998528 token=token123 x_forwarded_for=
2023-07-10T13:12:01.922863Z [error    ] Internal Server Error: /api/projects/2052/insights/funnel/ [django.request] host= pid=152128 team_id=2052 tid=140141459998528 token=token123 x_forwarded_for=
------------------------------ Captured log call -------------------------------
ERROR    posthog.exceptions:exceptions.py:56 {'request_id': '1a34e997-d132-4340-80da-14dd4bfbcc04', 'ip': '127.0.0.1', 'path': '/api/projects/2052/insights/funnel/', 'event': CHQueryErrorUnknownIdentifier('DB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n'), 'token': 'token123', 'host': '', 'x_forwarded_for': '', 'team_id': 2052, 'timestamp': '2023-07-10T13:12:01.919484Z', 'logger': 'posthog.exceptions', 'level': 'error', 'pid': 152128, 'tid': 140141459998528, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute\n    result = client.execute(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute\n    rv = self.process_ordinary_query(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query\n    return self.receive_result(with_column_types=with_column_types,\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result\n    return result.get_result()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result\n    for packet in self.packet_generator:\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator\n    packet = self.receive_packet()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet\n    raise packet.exception\nclickhouse_driver.errors.ServerException: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch\n    response = handler(request, *args, **kwargs)\n  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel\n    funnel = self.calculate_funnel(request)\n  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper\n    fresh_result_package = cast(T, f(self, request))\n  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel\n    "result": funnel_order_class(team=team, filter=filter).run(),\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run\n    results = self._exec_query()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query\n    query = self.get_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query\n    {self.get_step_counts_query()}\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query\n    steps_per_person_query = self.get_step_counts_without_aggregation_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query\n    formatted_query = self.build_step_subquery(2, max_steps)\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery\n    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query\n    values = self._get_breakdown_conditions()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions\n    return get_breakdown_prop_values(\n  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values\n    return insight_sync_execute(\n  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute\n    return sync_execute(query, args=args, team_id=team_id, **kwargs)\n  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner\n    return inner._impl(*args, **kwargs)  # type: ignore\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute\n    raise err\nposthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 2052) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f2f2fccd609 in ?\n24. __clone @ 0x00007f2f2fbf2133 in ?\n'}
ERROR    django.request:log.py:224 Internal Server Error: /api/projects/2052/insights/funnel/
_________________ TestInstanceStatus.test_navigation_on_cloud __________________

self = <posthog.api.test.test_instance_status.TestInstanceStatus testMethod=test_navigation_on_cloud>
mocks = (<MagicMock name='dead_letter_queue_ratio_ok_cached' id='140140637997760'>, <MagicMock name='is_plugin_server_alive' i...0'>, <MagicMock name='is_redis_alive' id='140140592988272'>, <MagicMock name='is_postgres_alive' id='140140599002784'>)

    @patch("posthog.api.instance_status.is_postgres_alive")
    @patch("posthog.api.instance_status.is_redis_alive")
    @patch("posthog.api.instance_status.is_plugin_server_alive")
    @patch("posthog.api.instance_status.dead_letter_queue_ratio_ok_cached")
    def test_navigation_on_cloud(self, *mocks):
        self.user.is_staff = True
        self.user.save()
    
        with self.is_cloud(True):
>           response = self.client.get("/api/instance_status/navigation").json()

posthog/api/test/test_instance_status.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/rest_framework/test.py:289: in get
    response = super().get(path, data=data, **extra)
env/lib/python3.10/site-packages/rest_framework/test.py:206: in get
    return self.generic('GET', path, **r)
env/lib/python3.10/site-packages/rest_framework/test.py:234: in generic
    return super().generic(
env/lib/python3.10/site-packages/django/test/client.py:473: in generic
    return self.request(**r)
env/lib/python3.10/site-packages/rest_framework/test.py:286: in request
    return super().request(**kwargs)
env/lib/python3.10/site-packages/rest_framework/test.py:238: in request
    request = super().request(**kwargs)
env/lib/python3.10/site-packages/django/test/client.py:719: in request
    self.check_exception(response)
env/lib/python3.10/site-packages/django/test/client.py:580: in check_exception
    raise exc_value
env/lib/python3.10/site-packages/django/core/handlers/exception.py:47: in inner
    response = get_response(request)
env/lib/python3.10/site-packages/django/core/handlers/base.py:204: in _get_response
    response = response.render()
env/lib/python3.10/site-packages/django/template/response.py:105: in render
    self.content = self.rendered_content
env/lib/python3.10/site-packages/rest_framework/response.py:70: in rendered_content
    ret = renderer.render(self.data, accepted_media_type, context)
env/lib/python3.10/site-packages/rest_framework/renderers.py:99: in render
    ret = json.dumps(
env/lib/python3.10/site-packages/rest_framework/utils/json.py:25: in dumps
    return json.dumps(*args, **kwargs)
/usr/local/lib/python3.10/json/__init__.py:238: in dumps
    **kw).encode(obj)
/usr/local/lib/python3.10/json/encoder.py:199: in encode
    chunks = self.iterencode(o, _one_shot=True)
/usr/local/lib/python3.10/json/encoder.py:257: in iterencode
    return _iterencode(o, 0)
env/lib/python3.10/site-packages/rest_framework/utils/encoders.py:53: in default
    return obj.tolist()
/usr/local/lib/python3.10/unittest/mock.py:1114: in __call__
    return self._mock_call(*args, **kwargs)
/usr/local/lib/python3.10/unittest/mock.py:1118: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
/usr/local/lib/python3.10/unittest/mock.py:1190: in _execute_mock_call
    return self.return_value
/usr/local/lib/python3.10/unittest/mock.py:532: in __get_return_value
    ret = self._get_child_mock(
/usr/local/lib/python3.10/unittest/mock.py:1040: in _get_child_mock
    return klass(**kw)
/usr/local/lib/python3.10/unittest/mock.py:2111: in __init__
    _safe_super(MagicMixin, self).__init__(*args, **kw)
/usr/local/lib/python3.10/unittest/mock.py:1096: in __init__
    _safe_super(CallableMixin, self).__init__(
/usr/local/lib/python3.10/unittest/mock.py:450: in __init__
    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)
/usr/local/lib/python3.10/unittest/mock.py:505: in _mock_add_spec
    if iscoroutinefunction(getattr(spec, attr, None)):
/usr/local/lib/python3.10/asyncio/coroutines.py:166: in iscoroutinefunction
    return (inspect.iscoroutinefunction(func) or
/usr/local/lib/python3.10/inspect.py:313: in iscoroutinefunction
    return _has_code_flag(obj, CO_COROUTINE)
/usr/local/lib/python3.10/inspect.py:297: in _has_code_flag
    if not (isfunction(f) or _signature_is_functionlike(f)):
/usr/local/lib/python3.10/inspect.py:2004: in _signature_is_functionlike
    if not callable(obj) or isclass(obj):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <method-wrapper '__bool__' of NoneType object at 0x7f753a581960>

    def isclass(object):
        """Return true if the object is a class.
    
        Class objects provide these attributes:
            __doc__         documentation string
            __module__      name of module in which this class was defined"""
>       return isinstance(object, type)
E       RecursionError: maximum recursion depth exceeded while calling a Python object

/usr/local/lib/python3.10/inspect.py:197: RecursionError
----------------------------- Captured stderr call -----------------------------
2023-07-10T13:13:03.086043Z [error    ] request_failed                 [django_structlog.middlewares.request] code=500 host= ip=127.0.0.1 pid=152128 request=<WSGIRequest: GET '/api/instance_status/navigation'> request_id=ba78f42b-812d-46a1-b1d2-1bd8e55b48cd team_id=2117 tid=140141459998528 token=token123 user_id=2001 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/base.py", line 204, in _get_response
    response = response.render()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/template/response.py", line 105, in render
    self.content = self.rendered_content
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/response.py", line 70, in rendered_content
    ret = renderer.render(self.data, accepted_media_type, context)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/renderers.py", line 99, in render
    ret = json.dumps(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/json.py", line 25, in dumps
    return json.dumps(*args, **kwargs)
  File "/usr/local/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/usr/local/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/encoders.py", line 53, in default
    return obj.tolist()
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1190, in _execute_mock_call
    return self.return_value
  File "/usr/local/lib/python3.10/unittest/mock.py", line 532, in __get_return_value
    ret = self._get_child_mock(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1040, in _get_child_mock
    return klass(**kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 2111, in __init__
    _safe_super(MagicMixin, self).__init__(*args, **kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1096, in __init__
    _safe_super(CallableMixin, self).__init__(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 450, in __init__
    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 505, in _mock_add_spec
    if iscoroutinefunction(getattr(spec, attr, None)):
  File "/usr/local/lib/python3.10/asyncio/coroutines.py", line 166, in iscoroutinefunction
    return (inspect.iscoroutinefunction(func) or
  File "/usr/local/lib/python3.10/inspect.py", line 313, in iscoroutinefunction
    return _has_code_flag(obj, CO_COROUTINE)
  File "/usr/local/lib/python3.10/inspect.py", line 297, in _has_code_flag
    if not (isfunction(f) or _signature_is_functionlike(f)):
  File "/usr/local/lib/python3.10/inspect.py", line 2004, in _signature_is_functionlike
    if not callable(obj) or isclass(obj):
  File "/usr/local/lib/python3.10/inspect.py", line 197, in isclass
    return isinstance(object, type)
RecursionError: maximum recursion depth exceeded while calling a Python object
2023-07-10T13:13:03.089644Z [error    ] Internal Server Error: /api/instance_status/navigation [django.request] host= pid=152128 team_id=2117 tid=140141459998528 token=token123 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/exception.py", line 47, in inner
    response = get_response(request)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/base.py", line 204, in _get_response
    response = response.render()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/template/response.py", line 105, in render
    self.content = self.rendered_content
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/response.py", line 70, in rendered_content
    ret = renderer.render(self.data, accepted_media_type, context)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/renderers.py", line 99, in render
    ret = json.dumps(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/json.py", line 25, in dumps
    return json.dumps(*args, **kwargs)
  File "/usr/local/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/usr/local/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/encoders.py", line 53, in default
    return obj.tolist()
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1190, in _execute_mock_call
    return self.return_value
  File "/usr/local/lib/python3.10/unittest/mock.py", line 532, in __get_return_value
    ret = self._get_child_mock(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1040, in _get_child_mock
    return klass(**kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 2111, in __init__
    _safe_super(MagicMixin, self).__init__(*args, **kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1096, in __init__
    _safe_super(CallableMixin, self).__init__(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 450, in __init__
    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 505, in _mock_add_spec
    if iscoroutinefunction(getattr(spec, attr, None)):
  File "/usr/local/lib/python3.10/asyncio/coroutines.py", line 166, in iscoroutinefunction
    return (inspect.iscoroutinefunction(func) or
  File "/usr/local/lib/python3.10/inspect.py", line 313, in iscoroutinefunction
    return _has_code_flag(obj, CO_COROUTINE)
  File "/usr/local/lib/python3.10/inspect.py", line 297, in _has_code_flag
    if not (isfunction(f) or _signature_is_functionlike(f)):
  File "/usr/local/lib/python3.10/inspect.py", line 2004, in _signature_is_functionlike
    if not callable(obj) or isclass(obj):
  File "/usr/local/lib/python3.10/inspect.py", line 197, in isclass
    return isinstance(object, type)
RecursionError: maximum recursion depth exceeded while calling a Python object
2023-07-10T13:13:03.090348Z [error    ] Internal Server Error: /api/instance_status/navigation [django.request] host= pid=152128 team_id=2117 tid=140141459998528 token=token123 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/exception.py", line 47, in inner
    response = get_response(request)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/base.py", line 204, in _get_response
    response = response.render()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/template/response.py", line 105, in render
    self.content = self.rendered_content
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/response.py", line 70, in rendered_content
    ret = renderer.render(self.data, accepted_media_type, context)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/renderers.py", line 99, in render
    ret = json.dumps(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/json.py", line 25, in dumps
    return json.dumps(*args, **kwargs)
  File "/usr/local/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/usr/local/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/encoders.py", line 53, in default
    return obj.tolist()
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1190, in _execute_mock_call
    return self.return_value
  File "/usr/local/lib/python3.10/unittest/mock.py", line 532, in __get_return_value
    ret = self._get_child_mock(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1040, in _get_child_mock
    return klass(**kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 2111, in __init__
    _safe_super(MagicMixin, self).__init__(*args, **kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1096, in __init__
    _safe_super(CallableMixin, self).__init__(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 450, in __init__
    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 505, in _mock_add_spec
    if iscoroutinefunction(getattr(spec, attr, None)):
  File "/usr/local/lib/python3.10/asyncio/coroutines.py", line 166, in iscoroutinefunction
    return (inspect.iscoroutinefunction(func) or
  File "/usr/local/lib/python3.10/inspect.py", line 313, in iscoroutinefunction
    return _has_code_flag(obj, CO_COROUTINE)
  File "/usr/local/lib/python3.10/inspect.py", line 297, in _has_code_flag
    if not (isfunction(f) or _signature_is_functionlike(f)):
  File "/usr/local/lib/python3.10/inspect.py", line 2004, in _signature_is_functionlike
    if not callable(obj) or isclass(obj):
  File "/usr/local/lib/python3.10/inspect.py", line 197, in isclass
    return isinstance(object, type)
RecursionError: maximum recursion depth exceeded while calling a Python object
------------------------------ Captured log call -------------------------------
ERROR    django_structlog.middlewares.request:request.py:90 {'request_id': 'ba78f42b-812d-46a1-b1d2-1bd8e55b48cd', 'ip': '127.0.0.1', 'user_id': 2001, 'code': 500, 'request': <WSGIRequest: GET '/api/instance_status/navigation'>, 'event': 'request_failed', 'token': 'token123', 'host': '', 'x_forwarded_for': '', 'team_id': 2117, 'timestamp': '2023-07-10T13:13:03.086043Z', 'logger': 'django_structlog.middlewares.request', 'level': 'error', 'pid': 152128, 'tid': 140141459998528, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/base.py", line 204, in _get_response\n    response = response.render()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/template/response.py", line 105, in render\n    self.content = self.rendered_content\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/response.py", line 70, in rendered_content\n    ret = renderer.render(self.data, accepted_media_type, context)\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/renderers.py", line 99, in render\n    ret = json.dumps(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/json.py", line 25, in dumps\n    return json.dumps(*args, **kwargs)\n  File "/usr/local/lib/python3.10/json/__init__.py", line 238, in dumps\n    **kw).encode(obj)\n  File "/usr/local/lib/python3.10/json/encoder.py", line 199, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File "/usr/local/lib/python3.10/json/encoder.py", line 257, in iterencode\n    return _iterencode(o, 0)\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/encoders.py", line 53, in default\n    return obj.tolist()\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1190, in _execute_mock_call\n    return self.return_value\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 532, in __get_return_value\n    ret = self._get_child_mock(\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1040, in _get_child_mock\n    return klass(**kw)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 2111, in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1096, in __init__\n    _safe_super(CallableMixin, self).__init__(\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 450, in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 505, in _mock_add_spec\n    if iscoroutinefunction(getattr(spec, attr, None)):\n  File "/usr/local/lib/python3.10/asyncio/coroutines.py", line 166, in iscoroutinefunction\n    return (inspect.iscoroutinefunction(func) or\n  File "/usr/local/lib/python3.10/inspect.py", line 313, in iscoroutinefunction\n    return _has_code_flag(obj, CO_COROUTINE)\n  File "/usr/local/lib/python3.10/inspect.py", line 297, in _has_code_flag\n    if not (isfunction(f) or _signature_is_functionlike(f)):\n  File "/usr/local/lib/python3.10/inspect.py", line 2004, in _signature_is_functionlike\n    if not callable(obj) or isclass(obj):\n  File "/usr/local/lib/python3.10/inspect.py", line 197, in isclass\n    return isinstance(object, type)\nRecursionError: maximum recursion depth exceeded while calling a Python object'}
ERROR    django.request:log.py:224 Internal Server Error: /api/instance_status/navigation
Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/exception.py", line 47, in inner
    response = get_response(request)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/core/handlers/base.py", line 204, in _get_response
    response = response.render()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/django/template/response.py", line 105, in render
    self.content = self.rendered_content
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/response.py", line 70, in rendered_content
    ret = renderer.render(self.data, accepted_media_type, context)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/renderers.py", line 99, in render
    ret = json.dumps(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/json.py", line 25, in dumps
    return json.dumps(*args, **kwargs)
  File "/usr/local/lib/python3.10/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/usr/local/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/utils/encoders.py", line 53, in default
    return obj.tolist()
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1190, in _execute_mock_call
    return self.return_value
  File "/usr/local/lib/python3.10/unittest/mock.py", line 532, in __get_return_value
    ret = self._get_child_mock(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1040, in _get_child_mock
    return klass(**kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 2111, in __init__
    _safe_super(MagicMixin, self).__init__(*args, **kw)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1096, in __init__
    _safe_super(CallableMixin, self).__init__(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 450, in __init__
    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 505, in _mock_add_spec
    if iscoroutinefunction(getattr(spec, attr, None)):
  File "/usr/local/lib/python3.10/asyncio/coroutines.py", line 166, in iscoroutinefunction
    return (inspect.iscoroutinefunction(func) or
  File "/usr/local/lib/python3.10/inspect.py", line 313, in iscoroutinefunction
    return _has_code_flag(obj, CO_COROUTINE)
  File "/usr/local/lib/python3.10/inspect.py", line 297, in _has_code_flag
    if not (isfunction(f) or _signature_is_functionlike(f)):
  File "/usr/local/lib/python3.10/inspect.py", line 2004, in _signature_is_functionlike
    if not callable(obj) or isclass(obj):
  File "/usr/local/lib/python3.10/inspect.py", line 197, in isclass
    return isinstance(object, type)
RecursionError: maximum recursion depth exceeded while calling a Python object
__ TestOrganizationAPI.test_cant_create_organization_with_custom_plugin_level __

self = <posthog.api.test.test_organization.TestOrganizationAPI testMethod=test_cant_create_organization_with_custom_plugin_level>

    def test_cant_create_organization_with_custom_plugin_level(self):
        with self.is_cloud(True):
            response = self.client.post("/api/organizations/", {"name": "Test", "plugins_access_level": 6})
>           self.assertEqual(response.status_code, status.HTTP_201_CREATED)
E           AssertionError: 403 != 201

posthog/api/test/test_organization.py:57: AssertionError
________________ TestOrganizationDomainsAPI.test_create_domain _________________

self = <posthog.api.test.test_organization_domain.TestOrganizationDomainsAPI testMethod=test_create_domain>

    def test_create_domain(self):
        self.organization_membership.level = OrganizationMembership.Level.ADMIN
        self.organization_membership.save()
    
        with self.is_cloud(True):
            response = self.client.post(
                "/api/organizations/@current/domains/",
                {
                    "domain": "the.posthog.com",
                    "verified_at": "2022-01-01T14:25:25.000Z",  # ignore me
                    "verification_challenge": "123",  # ignore me
                    "jit_provisioning_enabled": True,  # ignore me
                    "sso_enforcement": "saml",  # ignore me
                },
            )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        response_data = response.json()
        self.assertEqual(response_data["domain"], "the.posthog.com")
>       self.assertEqual(response_data["verified_at"], None)
E       AssertionError: '2023-07-10T13:13:06.306969Z' != None

posthog/api/test/test_organization_domain.py:101: AssertionError
_ TestOrganizationDomainsAPI.test_domain_is_not_verified_with_incorrect_challenge _

self = <posthog.api.test.test_organization_domain.TestOrganizationDomainsAPI testMethod=test_domain_is_not_verified_with_incorrect_challenge>
mock_dns_query = <MagicMock name='resolve' id='140140336342560'>

    @patch("posthog.models.organization_domain.dns.resolver.resolve")
    def test_domain_is_not_verified_with_incorrect_challenge(self, mock_dns_query):
        self.organization_membership.level = OrganizationMembership.Level.ADMIN
        self.organization_membership.save()
    
        mock_dns_query.return_value = FakeDNSResponse(
            [dns.rrset.from_text("_posthog-challenge.myposthog.com.", 3600, "IN", "TXT", "incorrect_challenge")]
        )
    
        with freeze_time("2021-10-10T10:10:10Z"):
            with self.is_cloud(True):
                response = self.client.post(f"/api/organizations/@current/domains/{self.domain.id}/verify")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        response_data = response.json()
        self.domain.refresh_from_db()
        self.assertEqual(response_data["domain"], "myposthog.com")
>       self.assertEqual(response_data["verified_at"], None)
E       AssertionError: '2021-10-10T10:10:10Z' != None

posthog/api/test/test_organization_domain.py:262: AssertionError
_ TestOrganizationDomainsAPI.test_domain_is_not_verified_with_missing_challenge _

self = <posthog.api.test.test_organization_domain.TestOrganizationDomainsAPI testMethod=test_domain_is_not_verified_with_missing_challenge>
mock_dns_query = <MagicMock name='resolve' id='140140332778448'>

    @patch("posthog.models.organization_domain.dns.resolver.resolve")
    def test_domain_is_not_verified_with_missing_challenge(self, mock_dns_query):
        self.organization_membership.level = OrganizationMembership.Level.ADMIN
        self.organization_membership.save()
    
        mock_dns_query.side_effect = dns.resolver.NoAnswer()
    
        with freeze_time("2021-10-10T10:10:10Z"):
            with self.is_cloud(True):
                response = self.client.post(f"/api/organizations/@current/domains/{self.domain.id}/verify")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        response_data = response.json()
        self.domain.refresh_from_db()
        self.assertEqual(response_data["domain"], "myposthog.com")
>       self.assertEqual(response_data["verified_at"], None)
E       AssertionError: '2021-10-10T10:10:10Z' != None

posthog/api/test/test_organization_domain.py:220: AssertionError
__ TestOrganizationDomainsAPI.test_domain_is_not_verified_with_missing_domain __

self = <posthog.api.test.test_organization_domain.TestOrganizationDomainsAPI testMethod=test_domain_is_not_verified_with_missing_domain>
mock_dns_query = <MagicMock name='resolve' id='140140336308064'>

    @patch("posthog.models.organization_domain.dns.resolver.resolve")
    def test_domain_is_not_verified_with_missing_domain(self, mock_dns_query):
        self.organization_membership.level = OrganizationMembership.Level.ADMIN
        self.organization_membership.save()
    
        mock_dns_query.side_effect = dns.resolver.NXDOMAIN()
    
        with freeze_time("2021-10-10T10:10:10Z"):
            with self.is_cloud(True):
                response = self.client.post(f"/api/organizations/@current/domains/{self.domain.id}/verify")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        response_data = response.json()
        self.domain.refresh_from_db()
        self.assertEqual(response_data["domain"], "myposthog.com")
>       self.assertEqual(response_data["verified_at"], None)
E       AssertionError: '2021-10-10T10:10:10Z' != None

posthog/api/test/test_organization_domain.py:240: AssertionError
_________ TestPersonTrends.test_trends_people_endpoint_filters_search __________
posthog/test/base.py:806: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.api.test.test_persons_trends.TestPersonTrends testMethod=test_trends_people_endpoint_filters_search>
query = '/* user_id:0 request:_snapshot_ */ \nSELECT\n    person_id AS actor_id,\n    count() AS actor_value\n    , groupUniqA...), \'^"|"$\', \'\')))\n\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m            e."properties" as "properties",[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m            pdi.person_id as person_id,[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:456: AssertionError
_______ TestPersonTrends.test_trends_people_endpoint_includes_recordings _______
posthog/test/base.py:806: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.api.test.test_persons_trends.TestPersonTrends testMethod=test_trends_people_endpoint_includes_recordings>
query = '/* user_id:0 request:_snapshot_ */ \nSELECT\n    person_id AS actor_id,\n    count() AS actor_value\n    , groupUniqA...), \'^"|"$\', \'\')))\n\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m            e."properties" as "properties",[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m            pdi.person_id as person_id,[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:456: AssertionError
_ TestPluginAPI.test_create_plugin_version_range_gt_next_major_ignore_on_cloud _

self = <posthog.api.test.test_plugin.TestPluginAPI testMethod=test_create_plugin_version_range_gt_next_major_ignore_on_cloud>
mock_get = <MagicMock name='get' id='140140315124800'>
mock_reload = <MagicMock name='reload_plugins_on_workers' id='140140315131568'>

    def test_create_plugin_version_range_gt_next_major_ignore_on_cloud(self, mock_get, mock_reload):
        with self.is_cloud(True):
            response = self.client.post(
                "/api/organizations/@current/plugins/",
                {
                    "url": f"https://github.com/posthog-plugin/version-greater-than/commit/{Version(VERSION).next_major()}"
                },
            )
>           self.assertEqual(response.status_code, 201)
E           AssertionError: 400 != 201

posthog/api/test/test_plugin.py:559: AssertionError
____________ TestPreflight.test_cloud_preflight_limited_db_queries _____________

self = <posthog.api.test.test_preflight.TestPreflight testMethod=test_cloud_preflight_limited_db_queries>

    @pytest.mark.ee
    @snapshot_postgres_queries
    def test_cloud_preflight_limited_db_queries(self):
        with self.is_cloud(True):
            # :IMPORTANT: This code is hit _every_ web request on cloud so avoid ever increasing db load.
>           with self.assertNumQueries(4):  # session, user, team and slack instance setting.

posthog/api/test/test_preflight.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 8 != 4 : 8 queries executed, 4 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:14:07.696743+00:00'::timestamptz AND "django_session"."session_key" = 'haowsxxy4wemziesbf1amm426nxgfipv') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 2075 LIMIT 21 /**/
E   3. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" IN ('constance:posthog:SLACK_APP_CLIENT_ID', 'constance:posthog:SLACK_APP_CLIENT_SECRET', 'constance:posthog:SLACK_APP_SIGNING_SECRET') /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
E   4. SELECT COUNT(*) AS "__count" FROM "posthog_user" /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
E   5. SELECT (1) AS "a" FROM "posthog_organization" LIMIT 1 /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
E   6. SELECT (1) AS "a" FROM "posthog_organization" WHERE NOT "posthog_organization"."for_internal_metrics" LIMIT 1 /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
E   7. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
E   8. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_HOST' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.views.preflight_check',route='%5E_preflight/%3F%28%3F%3A%5B%3F%23%5D.%2A%29%3F%24'*/
__________________ TestPreflight.test_cloud_preflight_request __________________

self = <posthog.api.test.test_preflight.TestPreflight testMethod=test_cloud_preflight_request>

    @pytest.mark.ee
    def test_cloud_preflight_request(self):
        with self.is_cloud(True):
            with self.settings(SITE_URL="https://app.posthog.com", OBJECT_STORAGE_ENABLED=False):
                response = self.client.get("/_preflight/")
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                response = response.json()
                available_timezones = cast(dict, response).pop("available_timezones")
    
>               self.assertEqual(
                    response,
                    self.preflight_authenticated_dict(
                        {
                            "can_create_org": True,
                            "cloud": True,
                            "realm": "cloud",
                            "region": "US",
                            "instance_preferences": {"debug_queries": False, "disable_paid_fs": False},
                            "site_url": "https://app.posthog.com",
                            "email_service_available": True,
                            "object_storage": True,
                        }
                    ),
                )
E               AssertionError: {'dja[56 chars]ue, 'clickhouse': True, 'kafka': True, 'db': T[656 chars]: 60} != {'dja[56 chars]ue, 'db': True, 'initiated': True, 'cloud': Tr[640 chars]alse}
E                 {'available_social_auth_providers': {'github': False,
E                                                      'gitlab': False,
E                                                      'google-oauth2': False},
E                  'buffer_conversion_seconds': 60,
E               -  'can_create_org': False,
E               ?                    ^^^^
E               
E               +  'can_create_org': True,
E               ?                    ^^^
E               
E                  'celery': True,
E                  'clickhouse': True,
E               -  'cloud': False,
E               ?           ^^^^
E               
E               +  'cloud': True,
E               ?           ^^^
E               
E                  'db': True,
E                  'demo': False,
E                  'django': True,
E               -  'email_service_available': False,
E               ?                             ^^^^
E               
E               +  'email_service_available': True,
E               ?                             ^^^
E               
E                  'initiated': True,
E                  'instance_preferences': {'debug_queries': False, 'disable_paid_fs': False},
E                  'is_debug': False,
E                  'is_event_property_usage_enabled': True,
E                  'kafka': True,
E                  'licensed_users_available': None,
E               -  'object_storage': False,
E               ?                    ^^^^
E               
E               +  'object_storage': True,
E               ?                    ^^^
E               
E                  'openai_available': False,
E                  'opt_out_capture': False,
E                  'plugins': True,
E                  'posthog_version': '1.43.0',
E               -  'realm': 'hosted-clickhouse',
E               +  'realm': 'cloud',
E                  'redis': True,
E               -  'region': None,
E               ?            ^^^^
E               
E               +  'region': 'US',
E               ?            ^^^^
E               
E                  'site_url': 'https://app.posthog.com',
E                  'slack_service': {'available': False, 'client_id': None}}

posthog/api/test/test_preflight.py:140: AssertionError
__________ TestPreflight.test_cloud_preflight_request_unauthenticated __________

self = <posthog.api.test.test_preflight.TestPreflight testMethod=test_cloud_preflight_request_unauthenticated>

    @pytest.mark.ee
    def test_cloud_preflight_request_unauthenticated(self):
        set_instance_setting("EMAIL_HOST", "localhost")
        set_instance_setting("SLACK_APP_CLIENT_ID", "slack-client-id")
    
        self.client.logout()  # make sure it works anonymously
    
        with self.is_cloud(True):
            with self.settings(OBJECT_STORAGE_ENABLED=False):
                response = self.client.get("/_preflight/")
                self.assertEqual(response.status_code, status.HTTP_200_OK)
    
>               self.assertEqual(
                    response.json(),
                    self.preflight_dict(
                        {
                            "email_service_available": True,
                            "slack_service": {"available": True, "client_id": "slack-client-id"},
                            "can_create_org": True,
                            "cloud": True,
                            "realm": "cloud",
                            "region": "US",
                            "object_storage": True,
                        }
                    ),
                )
E               AssertionError: {'dja[56 chars]ue, 'clickhouse': True, 'kafka': True, 'db': T[343 chars]alse} != {'dja[56 chars]ue, 'db': True, 'initiated': True, 'cloud': Tr[328 chars]True}
E                 {'available_social_auth_providers': {'github': False,
E                                                      'gitlab': False,
E                                                      'google-oauth2': False},
E               -  'can_create_org': False,
E               ?                    ^^^^
E               
E               +  'can_create_org': True,
E               ?                    ^^^
E               
E                  'celery': True,
E                  'clickhouse': True,
E               -  'cloud': False,
E               ?           ^^^^
E               
E               +  'cloud': True,
E               ?           ^^^
E               
E                  'db': True,
E                  'demo': False,
E                  'django': True,
E                  'email_service_available': True,
E                  'initiated': True,
E                  'kafka': True,
E               -  'object_storage': False,
E               ?                    ^^^^
E               
E               +  'object_storage': True,
E               ?                    ^^^
E               
E                  'plugins': True,
E               -  'realm': 'hosted-clickhouse',
E               +  'realm': 'cloud',
E                  'redis': True,
E               -  'region': None,
E               ?            ^^^^
E               
E               +  'region': 'US',
E               ?            ^^^^
E               
E                  'slack_service': {'available': True, 'client_id': 'slack-client-id'}}

posthog/api/test/test_preflight.py:116: AssertionError
____ TestPreflight.test_cloud_preflight_request_with_social_auth_providers _____

self = <posthog.api.test.test_preflight.TestPreflight testMethod=test_cloud_preflight_request_with_social_auth_providers>

    @pytest.mark.ee
    def test_cloud_preflight_request_with_social_auth_providers(self):
        set_instance_setting("EMAIL_HOST", "localhost")
    
        with self.is_cloud(True):
            with self.settings(
                SOCIAL_AUTH_GOOGLE_OAUTH2_KEY="test_key",
                SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET="test_secret",
                INSTANCE_PREFERENCES=self.instance_preferences(disable_paid_fs=True),
                OBJECT_STORAGE_ENABLED=False,
            ):
                response = self.client.get("/_preflight/")
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                response = response.json()
                available_timezones = cast(dict, response).pop("available_timezones")
    
>               self.assertEqual(
                    response,
                    self.preflight_authenticated_dict(
                        {
                            "can_create_org": True,
                            "cloud": True,
                            "realm": "cloud",
                            "region": "US",
                            "instance_preferences": {"debug_queries": False, "disable_paid_fs": True},
                            "site_url": "http://localhost:8000",
                            "available_social_auth_providers": {
                                "google-oauth2": True,
                                "github": False,
                                "gitlab": False,
                            },
                            "email_service_available": True,
                            "object_storage": True,
                        }
                    ),
                )
E               AssertionError: {'dja[56 chars]ue, 'clickhouse': True, 'kafka': True, 'db': T[652 chars]: 60} != {'dja[56 chars]ue, 'db': True, 'initiated': True, 'cloud': Tr[636 chars]alse}
E                 {'available_social_auth_providers': {'github': False,
E                                                      'gitlab': False,
E               -                                      'google-oauth2': False},
E               ?                                                       ^^^^
E               
E               +                                      'google-oauth2': True},
E               ?                                                       ^^^
E               
E                  'buffer_conversion_seconds': 60,
E               -  'can_create_org': False,
E               ?                    ^^^^
E               
E               +  'can_create_org': True,
E               ?                    ^^^
E               
E                  'celery': True,
E                  'clickhouse': True,
E               -  'cloud': False,
E               ?           ^^^^
E               
E               +  'cloud': True,
E               ?           ^^^
E               
E                  'db': True,
E                  'demo': False,
E                  'django': True,
E                  'email_service_available': True,
E                  'initiated': True,
E                  'instance_preferences': {'debug_queries': False, 'disable_paid_fs': True},
E                  'is_debug': False,
E                  'is_event_property_usage_enabled': True,
E                  'kafka': True,
E                  'licensed_users_available': None,
E               -  'object_storage': False,
E               ?                    ^^^^
E               
E               +  'object_storage': True,
E               ?                    ^^^
E               
E                  'openai_available': False,
E                  'opt_out_capture': False,
E                  'plugins': True,
E                  'posthog_version': '1.43.0',
E               -  'realm': 'hosted-clickhouse',
E               +  'realm': 'cloud',
E                  'redis': True,
E               -  'region': None,
E               ?            ^^^^
E               
E               +  'region': 'US',
E               ?            ^^^^
E               
E                  'site_url': 'http://localhost:8000',
E                  'slack_service': {'available': False, 'client_id': None}}

posthog/api/test/test_preflight.py:182: AssertionError
----------------------------- Captured stderr call -----------------------------
2023-07-10T13:14:07.837164Z [warning  ] You have Google login set up, but not the required license! [posthog.utils] host= ip=127.0.0.1 pid=152128 request_id=e104bd0c-7bea-48c8-afad-df44d8d8bf12 team_id=2191 tid=140141459998528 token=token123 x_forwarded_for=
------------------------------ Captured log call -------------------------------
WARNING  posthog.utils:utils.py:917 {'request_id': 'e104bd0c-7bea-48c8-afad-df44d8d8bf12', 'ip': '127.0.0.1', 'event': 'You have Google login set up, but not the required license!', 'token': 'token123', 'host': '', 'x_forwarded_for': '', 'team_id': 2191, 'timestamp': '2023-07-10T13:14:07.837164Z', 'logger': 'posthog.utils', 'level': 'warning', 'pid': 152128, 'tid': 140141459998528}
______________ TestQuery.test_valid_recent_performance_pageviews _______________

self = <posthog.api.test.test_query.TestQuery testMethod=test_valid_recent_performance_pageviews>

    def test_valid_recent_performance_pageviews(self):
        api_response = self.client.post(
            f"/api/projects/{self.team.id}/query/",
            {"query": {"kind": "RecentPerformancePageViewNode", "dateRange": {"date_from": None, "date_to": None}}},
        )
>       assert api_response.status_code == 200
E       AssertionError: assert 400 == 200
E        +  where 400 = <Response status_code=400, "application/json">.status_code

posthog/api/test/test_query.py:489: AssertionError
_ TestQuery.test_valid_recent_performance_pageviews_defaults_to_the_last_hour __
/usr/local/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/usr/local/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/usr/local/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/usr/local/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/usr/local/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/usr/local/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

target = 'ee.api.performance_events'

    def _importer(target):
        components = target.split('.')
        import_path = components.pop(0)
>       thing = __import__(import_path)
E       ModuleNotFoundError: No module named 'ee'

/usr/local/lib/python3.10/unittest/mock.py:1257: ModuleNotFoundError
_______ TestSignupAPI.test_api_cannot_use_whitelist_for_different_domain _______

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_api_cannot_use_whitelist_for_different_domain>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140334634832'>
mock_request = <MagicMock name='request' id='140140334698208'>

    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @pytest.mark.ee
    def test_api_cannot_use_whitelist_for_different_domain(self, mock_sso_providers, mock_request):
        mock_sso_providers.return_value = {"google-oauth2": True}
        new_org = Organization.objects.create(name="Test org")
        OrganizationDomain.objects.create(
            domain="good.com", verified_at=timezone.now(), jit_provisioning_enabled=True, organization=new_org
        )
    
        response = self.client.get(reverse("social:begin", kwargs={"backend": "google-oauth2"}))
>       self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E       AssertionError: 404 != 302

posthog/api/test/test_signup.py:590: AssertionError
________________________ TestSignupAPI.test_api_sign_up ________________________

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_api_sign_up>
mock_capture = <MagicMock name='capture' id='140140326944096'>

    @pytest.mark.skip_on_multitenancy
    @patch("posthoganalytics.capture")
    def test_api_sign_up(self, mock_capture):
        # Ensure the internal system metrics org doesn't prevent org-creation
        Organization.objects.create(name="PostHog Internal Metrics", for_internal_metrics=True)
    
        response = self.client.post(
            "/api/signup/",
            {
                "first_name": "John",
                "email": "hedgehog@posthog.com",
                "password": "notsecure",
                "organization_name": "Hedgehogs United, LLC",
                "role_at_organization": "product",
                "email_opt_in": False,
            },
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
    
        user = cast(User, User.objects.order_by("-pk")[0])
        team = cast(Team, user.team)
        organization = cast(Organization, user.organization)
    
        self.assertEqual(
            response.json(),
            {
                "id": user.pk,
                "uuid": str(user.uuid),
                "distinct_id": user.distinct_id,
                "first_name": "John",
                "email": "hedgehog@posthog.com",
                "redirect_url": "/",
                "is_email_verified": False,
            },
        )
    
        # Assert that the user was properly created
        self.assertEqual(user.first_name, "John")
        self.assertEqual(user.email, "hedgehog@posthog.com")
        self.assertFalse(user.email_opt_in)
>       self.assertTrue(user.is_staff)  # True because this is the first user in the instance
E       AssertionError: False is not true

posthog/api/test/test_signup.py:71: AssertionError
_ TestSignupAPI.test_cannot_social_signup_with_whitelisted_but_jit_provisioning_disabled _

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_cannot_social_signup_with_whitelisted_but_jit_provisioning_disabled>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140324646112'>
mock_request = <MagicMock name='request' id='140140324639776'>

    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @pytest.mark.ee
    def test_cannot_social_signup_with_whitelisted_but_jit_provisioning_disabled(
        self, mock_sso_providers, mock_request
    ):
        mock_sso_providers.return_value = {"google-oauth2": True}
        new_org = Organization.objects.create(name="Test org")
        OrganizationDomain.objects.create(
            domain="posthog.net", verified_at=timezone.now(), jit_provisioning_enabled=False, organization=new_org
        )  # note `jit_provisioning_enabled=False`
    
        response = self.client.get(reverse("social:begin", kwargs={"backend": "google-oauth2"}))
>       self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E       AssertionError: 404 != 302

posthog/api/test/test_signup.py:544: AssertionError
_ TestSignupAPI.test_cannot_social_signup_with_whitelisted_but_unverified_domain _

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_cannot_social_signup_with_whitelisted_but_unverified_domain>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140334634544'>
mock_request = <MagicMock name='request' id='140140329325312'>

    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @pytest.mark.ee
    def test_cannot_social_signup_with_whitelisted_but_unverified_domain(self, mock_sso_providers, mock_request):
        mock_sso_providers.return_value = {"google-oauth2": True}
        new_org = Organization.objects.create(name="Test org")
        OrganizationDomain.objects.create(
            domain="posthog.net", verified_at=None, jit_provisioning_enabled=True, organization=new_org
        )  # note `verified_at=None`
    
        response = self.client.get(reverse("social:begin", kwargs={"backend": "google-oauth2"}))
>       self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E       AssertionError: 404 != 302

posthog/api/test/test_signup.py:567: AssertionError
__________ TestSignupAPI.test_default_dashboard_is_created_on_signup ___________

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_default_dashboard_is_created_on_signup>

    def test_default_dashboard_is_created_on_signup(self):
        """
        Tests that the default web app dashboard is created on signup.
        Note: This feature is currently behind a feature flag.
        """
    
        response = self.client.post(
            "/api/signup/", {"first_name": "Jane", "email": "hedgehog75@posthog.com", "password": "notsecure"}
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
    
>       user: User = User.objects.order_by("-pk").get()

posthog/api/test/test_signup.py:308: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <QuerySet [<User at 0x7f74f6eba980: id=1640, email='a@b.com', first_name='', distinct_id='A1N76PqXUeuAo7dzDqfB2Yu8W3Gr...id=1634, email='random@test.com', first_name='first_name', distinct_id='Dwpj3DDNeqjfty15NNhVVsGYbXsUy2h2JmOSlkc61m4'>]>
args = (), kwargs = {}
clone = <QuerySet [<User at 0x7f74f5cfc340: id=1634, email='random@test.com', first_name='first_name', distinct_id='Dwpj3DDNeq...d=2106, email='hedgehog75@posthog.com', first_name='Jane', distinct_id='QipRNQ2Za4r0vvhzdJgLlC6CwyHuFJUREhanZoaBu0a'>]>
limit = 21, num = 5

    def get(self, *args, **kwargs):
        """
        Perform the query and return a single object matching the given
        keyword arguments.
        """
        if self.query.combinator and (args or kwargs):
            raise NotSupportedError(
                'Calling QuerySet.get(...) with filters after %s() is not '
                'supported.' % self.query.combinator
            )
        clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
        if self.query.can_filter() and not self.query.distinct_fields:
            clone = clone.order_by()
        limit = None
        if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
            limit = MAX_GET_RESULTS
            clone.query.set_limits(high=limit)
        num = len(clone)
        if num == 1:
            return clone._result_cache[0]
        if not num:
            raise self.model.DoesNotExist(
                "%s matching query does not exist." %
                self.model._meta.object_name
            )
>       raise self.model.MultipleObjectsReturned(
            'get() returned more than one %s -- it returned %s!' % (
                self.model._meta.object_name,
                num if not limit or num < limit else 'more than %s' % (limit - 1),
            )
        )
E       posthog.models.user.User.MultipleObjectsReturned: get() returned more than one User -- it returned 5!

env/lib/python3.10/site-packages/django/db/models/query.py:439: MultipleObjectsReturned
___________ TestSignupAPI.test_signup_disallowed_on_email_collision ____________

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_signup_disallowed_on_email_collision>

    @pytest.mark.skip_on_multitenancy
    def test_signup_disallowed_on_email_collision(self):
        # Create a user with the same email
        User.objects.create(email="fake@posthog.com", first_name="Jane")
    
        response = self.client.post(
            "/api/signup/", {"first_name": "John", "email": "fake@posthog.com", "password": "notsecure"}
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        self.assertEqual(
            response.json(),
            self.validation_error_response(
                "There is already an account with this email address.", code="unique", attr="email"
            ),
        )
>       self.assertEqual(User.objects.count(), 1)
E       AssertionError: 5 != 1

posthog/api/test/test_signup.py:116: AssertionError
___________________ TestSignupAPI.test_signup_minimum_attrs ____________________

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_signup_minimum_attrs>
mock_capture = <MagicMock name='capture' id='140140330221184'>

    @pytest.mark.skip_on_multitenancy
    @patch("posthoganalytics.capture")
    def test_signup_minimum_attrs(self, mock_capture):
        response = self.client.post(
            "/api/signup/", {"first_name": "Jane", "email": "hedgehog2@posthog.com", "password": "notsecure"}
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
    
>       user = cast(User, User.objects.order_by("-pk").get())

posthog/api/test/test_signup.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <QuerySet [<User at 0x7f74f68580d0: id=1640, email='a@b.com', first_name='', distinct_id='A1N76PqXUeuAo7dzDqfB2Yu8W3Gr...id=1634, email='random@test.com', first_name='first_name', distinct_id='Dwpj3DDNeqjfty15NNhVVsGYbXsUy2h2JmOSlkc61m4'>]>
args = (), kwargs = {}
clone = <QuerySet [<User at 0x7f74f5dbcc10: id=1634, email='random@test.com', first_name='first_name', distinct_id='Dwpj3DDNeq...id=2110, email='hedgehog2@posthog.com', first_name='Jane', distinct_id='1CcmfKOltnUwTyjLQLfZzCbdsJcF8moTj8IhY3dFkt5'>]>
limit = 21, num = 5

    def get(self, *args, **kwargs):
        """
        Perform the query and return a single object matching the given
        keyword arguments.
        """
        if self.query.combinator and (args or kwargs):
            raise NotSupportedError(
                'Calling QuerySet.get(...) with filters after %s() is not '
                'supported.' % self.query.combinator
            )
        clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
        if self.query.can_filter() and not self.query.distinct_fields:
            clone = clone.order_by()
        limit = None
        if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
            limit = MAX_GET_RESULTS
            clone.query.set_limits(high=limit)
        num = len(clone)
        if num == 1:
            return clone._result_cache[0]
        if not num:
            raise self.model.DoesNotExist(
                "%s matching query does not exist." %
                self.model._meta.object_name
            )
>       raise self.model.MultipleObjectsReturned(
            'get() returned more than one %s -- it returned %s!' % (
                self.model._meta.object_name,
                num if not limit or num < limit else 'more than %s' % (limit - 1),
            )
        )
E       posthog.models.user.User.MultipleObjectsReturned: get() returned more than one User -- it returned 5!

env/lib/python3.10/site-packages/django/db/models/query.py:439: MultipleObjectsReturned
_ TestSignupAPI.test_social_signup_to_existing_org_without_whitelisted_domain_on_cloud _

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_social_signup_to_existing_org_without_whitelisted_domain_on_cloud>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140336178288'>
mock_request = <MagicMock name='request' id='140140315422064'>

    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @pytest.mark.ee
    def test_social_signup_to_existing_org_without_whitelisted_domain_on_cloud(self, mock_sso_providers, mock_request):
        with self.is_cloud(True):
            mock_sso_providers.return_value = {"google-oauth2": True}
            Organization.objects.create(name="Hogflix Movies")
            user_count = User.objects.count()
            org_count = Organization.objects.count()
            response = self.client.get(reverse("social:begin", kwargs={"backend": "google-oauth2"}))
>           self.assertEqual(response.status_code, 302)
E           AssertionError: 404 != 302

posthog/api/test/test_signup.py:615: AssertionError
______ TestSignupAPI.test_social_signup_with_whitelisted_domain_on_cloud _______

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_social_signup_with_whitelisted_domain_on_cloud>
mock_identify = <MagicMock name='identify_task' id='140140310480208'>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140310488896'>
mock_request = <MagicMock name='request' id='140140325428416'>
mock_capture = <MagicMock name='capture' id='140140325426880'>

    @patch("posthoganalytics.capture")
    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @mock.patch("posthog.tasks.user_identify.identify_task")
    @pytest.mark.ee
    def test_social_signup_with_whitelisted_domain_on_cloud(
        self, mock_identify, mock_sso_providers, mock_request, mock_capture
    ):
        with self.is_cloud(True):
>           self.run_test_for_whitelisted_domain(mock_sso_providers, mock_request, mock_capture)

posthog/api/test/test_signup.py:486: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/api/test/test_signup.py:444: in run_test_for_whitelisted_domain
    self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E   AssertionError: 404 != 302
__ TestSignupAPI.test_social_signup_with_whitelisted_domain_on_cloud_reverse ___

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_social_signup_with_whitelisted_domain_on_cloud_reverse>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140334805872'>
mock_request = <MagicMock name='request' id='140140327658416'>

    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @pytest.mark.ee
    def test_social_signup_with_whitelisted_domain_on_cloud_reverse(self, mock_sso_providers, mock_request):
        with self.is_cloud(True):
            # user already exists
            User.objects.create(email="jane@hogflix.posthog.com", distinct_id=str(uuid.uuid4()))
    
            # Make sure Google Auth is valid for this test instance
            mock_sso_providers.return_value = {"google-oauth2": True}
    
            new_org = Organization.objects.create(name="Hogflix Movies")
            OrganizationDomain.objects.create(
                domain="hogflix.posthog.com",
                verified_at=timezone.now(),
                jit_provisioning_enabled=True,
                organization=new_org,
            )
            new_project = Team.objects.create(organization=new_org, name="My First Project")
            user_count = User.objects.count()
            response = self.client.get(reverse("social:begin", kwargs={"backend": "google-oauth2"}))
>           self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E           AssertionError: 404 != 302

posthog/api/test/test_signup.py:509: AssertionError
___ TestSignupAPI.test_social_signup_with_whitelisted_domain_on_self_hosted ____

self = <posthog.api.test.test_signup.TestSignupAPI testMethod=test_social_signup_with_whitelisted_domain_on_self_hosted>
mock_identify = <MagicMock name='identify_task' id='140140331332608'>
mock_sso_providers = <MagicMock name='get_instance_available_sso_providers' id='140140328881360'>
mock_request = <MagicMock name='request' id='140140331008880'>
mock_capture = <MagicMock name='capture' id='140140331011952'>

    @patch("posthoganalytics.capture")
    @mock.patch("social_core.backends.base.BaseAuth.request")
    @mock.patch("posthog.api.authentication.get_instance_available_sso_providers")
    @mock.patch("posthog.tasks.user_identify.identify_task")
    @pytest.mark.ee
    def test_social_signup_with_whitelisted_domain_on_self_hosted(
        self, mock_identify, mock_sso_providers, mock_request, mock_capture
    ):
>       self.run_test_for_whitelisted_domain(mock_sso_providers, mock_request, mock_capture)

posthog/api/test/test_signup.py:475: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/api/test/test_signup.py:444: in run_test_for_whitelisted_domain
    self.assertEqual(response.status_code, status.HTTP_302_FOUND)
E   AssertionError: 404 != 302
_ TestInviteSignupAPI.test_api_invite_sign_up_where_there_are_no_default_non_private_projects _

self = <posthog.api.test.test_signup.TestInviteSignupAPI testMethod=test_api_invite_sign_up_where_there_are_no_default_non_private_projects>

    @pytest.mark.ee
    def test_api_invite_sign_up_where_there_are_no_default_non_private_projects(self):
        self.client.logout()
        invite: OrganizationInvite = OrganizationInvite.objects.create(
            target_email="test+private@posthog.com", organization=self.organization
        )
    
        self.organization.available_features = [AvailableFeature.PROJECT_BASED_PERMISSIONING]
        self.organization.save()
        self.team.access_control = True
        self.team.save()
    
        response = self.client.post(
            f"/api/signup/{invite.id}/", {"first_name": "Alice", "password": "test_password", "email_opt_in": True}
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        user = cast(User, User.objects.order_by("-pk")[0])
        self.assertEqual(user.organization_memberships.count(), 1)
        self.assertEqual(user.organization, self.organization)
        # here
>       self.assertEqual(
            user.current_team, None
        )  # User is not assigned to a project, as there are no non-private projects
E       AssertionError: <Team at 0x7f74f5b52260: id=2253, uuid=UU[84 chars]123'> != None

posthog/api/test/test_signup.py:835: AssertionError
______________________ TestDemoSignupAPI.test_demo_login _______________________

self = <posthog.api.test.test_signup_demo.TestDemoSignupAPI testMethod=test_demo_login>
args = ()
@py_assert1 = <posthog.models.user.UserManager object at 0x7f7508e4af80>
@py_assert3 = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>
@py_assert5 = True

    def test_demo_login(self, *args):
>       assert not User.objects.exists()
E       AssertionError: assert not True
E        +  where True = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>()
E        +    where <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>> = <posthog.models.user.UserManager object at 0x7f7508e4af80>.exists
E        +      where <posthog.models.user.UserManager object at 0x7f7508e4af80> = User.objects

posthog/api/test/test_signup_demo.py:52: AssertionError
______________________ TestDemoSignupAPI.test_demo_signup ______________________

self = <posthog.api.test.test_signup_demo.TestDemoSignupAPI testMethod=test_demo_signup>
args = ()
@py_assert1 = <posthog.models.user.UserManager object at 0x7f7508e4af80>
@py_assert3 = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>
@py_assert5 = True

    def test_demo_signup(self, *args):
>       assert not User.objects.exists()
E       AssertionError: assert not True
E        +  where True = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>()
E        +    where <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>> = <posthog.models.user.UserManager object at 0x7f7508e4af80>.exists
E        +      where <posthog.models.user.UserManager object at 0x7f7508e4af80> = User.objects

posthog/api/test/test_signup_demo.py:21: AssertionError
__________ TestDemoSignupAPI.test_social_login_give_staff_privileges ___________

self = <posthog.api.test.test_signup_demo.TestDemoSignupAPI testMethod=test_social_login_give_staff_privileges>
args = ()
@py_assert1 = <posthog.models.user.UserManager object at 0x7f7508e4af80>
@py_assert3 = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>
@py_assert5 = True

    def test_social_login_give_staff_privileges(self, *args):
>       assert not User.objects.exists()
E       AssertionError: assert not True
E        +  where True = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>()
E        +    where <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>> = <posthog.models.user.UserManager object at 0x7f7508e4af80>.exists
E        +      where <posthog.models.user.UserManager object at 0x7f7508e4af80> = User.objects

posthog/api/test/test_signup_demo.py:118: AssertionError
__________ TestDemoSignupAPI.test_social_signup_give_staff_privileges __________

self = <posthog.api.test.test_signup_demo.TestDemoSignupAPI testMethod=test_social_signup_give_staff_privileges>
args = ()
@py_assert1 = <posthog.models.user.UserManager object at 0x7f7508e4af80>
@py_assert3 = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>
@py_assert5 = True

    def test_social_signup_give_staff_privileges(self, *args):
>       assert not User.objects.exists()
E       AssertionError: assert not True
E        +  where True = <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>>()
E        +    where <bound method BaseManager._get_queryset_methods.<locals>.create_method.<locals>.manager_method of <posthog.models.user.UserManager object at 0x7f7508e4af80>> = <posthog.models.user.UserManager object at 0x7f7508e4af80>.exists
E        +      where <posthog.models.user.UserManager object at 0x7f7508e4af80> = User.objects

posthog/api/test/test_signup_demo.py:80: AssertionError
_______________________ test_can_insert_person_overrides _______________________

query = "\n    CREATE TABLE IF NOT EXISTS `posthog_test`.`kafka_person_overrides`\n    ON CLUSTER 'posthog'\n\n    ENGINE = Ka...n the `person_overrides` table.\n        -- created_at,\n        version\n    FROM `posthog_test`.`person_overrides`\n"
args = None
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba7a0>
query = "\n    CREATE TABLE IF NOT EXISTS `posthog_test`.`kafka_person_overrides`\n    ON CLUSTER 'posthog'\n\n    ENGINE = Ka...verride_person_id,\n        merged_at,\n        oldest_event,\n\nversion\n    FROM `posthog_test`.`person_overrides`\n"
params = None, with_column_types = False, external_tables = None
query_id = 'None_None_6RTTm7GV'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba7a0>
query = "\n    CREATE TABLE IF NOT EXISTS `posthog_test`.`kafka_person_overrides`\n    ON CLUSTER 'posthog'\n\n    ENGINE = Ka...verride_person_id,\n        merged_at,\n        oldest_event,\n\nversion\n    FROM `posthog_test`.`person_overrides`\n"
params = None, with_column_types = False, external_tables = None
query_id = 'None_None_6RTTm7GV', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba7a0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74842b5660>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba7a0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba7a0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 60.
E           DB::Exception: Table posthog_test.person_overrides doesn't exist. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
E           2. DB::DatabaseCatalog::getTableImpl(DB::StorageID const&, std::shared_ptr<DB::Context const>, std::optional<DB::Exception>*) const @ 0x0000000012ffaa1a in /usr/bin/clickhouse
E           3. DB::DatabaseCatalog::getTable(DB::StorageID const&, std::shared_ptr<DB::Context const>) const @ 0x000000001300188a in /usr/bin/clickhouse
E           4. DB::JoinedTables::getLeftTableStorage() @ 0x0000000013b314d4 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a40c41 in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::InterpreterCreateQuery::getTablePropertiesAndNormalizeCreateQuery(DB::ASTCreateQuery&) const @ 0x000000001387a61c in /usr/bin/clickhouse
E           10. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000138814de in /usr/bin/clickhouse
E           11. DB::InterpreterCreateQuery::execute() @ 0x000000001388f8e0 in /usr/bin/clickhouse
E           12. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
E           13. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           14. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           15. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           16. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           17. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           18. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           19. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           20. ? @ 0x00007f2f2fccd609 in ?
E           21. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

    @pytest.mark.django_db
    def test_can_insert_person_overrides():
        # By default the test suite runs with ClickHouse no Kafka or Materialized
        # Views. Updating to include these seems to be a [larger task to
        # fix](https://github.com/PostHog/posthog/pull/13878), so here we create
        # just the missing tables we need to verify functionality.
>       sync_execute(KAFKA_PERSON_OVERRIDES_TABLE_SQL)

posthog/clickhouse/test/test_person_overrides.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = "\n    CREATE TABLE IF NOT EXISTS `posthog_test`.`kafka_person_overrides`\n    ON CLUSTER 'posthog'\n\n    ENGINE = Ka...n the `person_overrides` table.\n        -- created_at,\n        version\n    FROM `posthog_test`.`person_overrides`\n"
args = None
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownTable: Code: 60.
E               DB::Exception: Table posthog_test.person_overrides doesn't exist. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
E               2. DB::DatabaseCatalog::getTableImpl(DB::StorageID const&, std::shared_ptr<DB::Context const>, std::optional<DB::Exception>*) const @ 0x0000000012ffaa1a in /usr/bin/clickhouse
E               3. DB::DatabaseCatalog::getTable(DB::StorageID const&, std::shared_ptr<DB::Context const>) const @ 0x000000001300188a in /usr/bin/clickhouse
E               4. DB::JoinedTables::getLeftTableStorage() @ 0x0000000013b314d4 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a40c41 in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::InterpreterCreateQuery::getTablePropertiesAndNormalizeCreateQuery(DB::ASTCreateQuery&) const @ 0x000000001387a61c in /usr/bin/clickhouse
E               10. DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x00000000138814de in /usr/bin/clickhouse
E               11. DB::InterpreterCreateQuery::execute() @ 0x000000001388f8e0 in /usr/bin/clickhouse
E               12. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
E               13. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               14. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               15. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               16. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               17. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               18. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               19. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               20. ? @ 0x00007f2f2fccd609 in ?
E               21. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownTable
__________ TestQuery.test_join_with_property_materialized_session_id ___________

self = <test_query.TestQuery testMethod=test_join_with_property_materialized_session_id>

    def test_join_with_property_materialized_session_id(self):
        with freeze_time("2020-01-10"):
            _create_person(distinct_ids=["some_id"], team_id=self.team.pk, properties={"$some_prop": "something"})
            _create_event(
                event="$pageview",
                team=self.team,
                distinct_id="some_id",
                properties={"attr": "some_val", "$session_id": "111"},
            )
            _create_event(
                event="$pageview",
                team=self.team,
                distinct_id="some_id",
                properties={"attr": "some_val", "$session_id": "111"},
            )
            create_snapshot(distinct_id="some_id", session_id="111", timestamp=timezone.now(), team_id=self.team.pk)
    
            response = execute_hogql_query(
                "select e.event, s.session_id from events e left join session_recording_events s on s.session_id = e.properties.$session_id where e.properties.$session_id is not null limit 10",
                team=self.team,
            )
>           self.assertEqual(
                response.clickhouse,
                f"SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, nullIf(nullIf(e.`$session_id`, ''), 'null')) WHERE and(equals(s.team_id, {self.team.pk}), equals(e.team_id, {self.team.pk}), isNotNull(nullIf(nullIf(e.`$session_id`, ''), 'null'))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True",
            )
E           AssertionError: 'SELECT e.event, s.session_id FROM events[453 chars]True' != "SELECT e.event, s.session_id FROM events[315 chars]True"
E           - SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(e.properties, %(hogql_val_0)s), ''), 'null'), '^"|"$', '')) WHERE and(equals(s.team_id, 2394), equals(e.team_id, 2394), isNotNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(e.properties, %(hogql_val_1)s), ''), 'null'), '^"|"$', ''))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True
E           + SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, nullIf(nullIf(e.`$session_id`, ''), 'null')) WHERE and(equals(s.team_id, 2394), equals(e.team_id, 2394), isNotNull(nullIf(nullIf(e.`$session_id`, ''), 'null'))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True

posthog/hogql/test/test_query.py:599: AssertionError
_ TestCohortUtils.test_simplified_cohort_filter_properties_non_precalculated_cohort_with_behavioural_filter _

query = '\nINSERT INTO cohortpeople\nSELECT id, %(cohort_id)s as cohort_id, %(team_id)s as team_id, 1 AS sign, %(new_version)s...M cohortpeople\nWHERE team_id = %(team_id)s AND cohort_id = %(cohort_id)s AND version < %(new_version)s AND sign = 1\n'
args = {'cohort_id': 256, 'kperson_filter_pre_256_0_0_0': 'name', 'kpersonquery_person_filter_fin_256_0_0_0': 'name', 'new_version': 0, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...direct,parallel_hash","distributed_replica_max_ignored_errors":1000,"optimize_on_insert":0}}', 'optimize_on_insert': 0}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74842af4f0>
query = '\nINSERT INTO cohortpeople\nSELECT id, 256 as cohort_id, 2446 as team_id, 1 AS sign, 0 AS version\nFROM (\n\n        ...t_id, team_id, -1, version\nFROM cohortpeople\nWHERE team_id = 2446 AND cohort_id = 256 AND version < 0 AND sign = 1\n'
params = None, with_column_types = False, external_tables = None
query_id = 'None_None_nxSX2dY0'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...direct,parallel_hash","distributed_replica_max_ignored_errors":1000,"optimize_on_insert":0}}', 'optimize_on_insert': 0}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74842af4f0>
query = '\nINSERT INTO cohortpeople\nSELECT id, 256 as cohort_id, 2446 as team_id, 1 AS sign, 0 AS version\nFROM (\n\n        ...t_id, team_id, -1, version\nFROM cohortpeople\nWHERE team_id = 2446 AND cohort_id = 256 AND version < 0 AND sign = 1\n'
params = None, with_column_types = False, external_tables = None
query_id = 'None_None_nxSX2dY0', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74842af4f0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74842f7040>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74842af4f0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74842af4f0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 42.
E           DB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
E           2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse
E           3. ? @ 0x000000000885250c in /usr/bin/clickhouse
E           4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse
E           5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse
E           6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse
E           7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse
E           8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse
E           9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse
E           10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse
E           11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse
E           12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse
E           13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse
E           14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse
E           15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse
E           16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse
E           17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse
E           18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse
E           28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
E           29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <test_util.TestCohortUtils testMethod=test_simplified_cohort_filter_properties_non_precalculated_cohort_with_behavioural_filter>

    def test_simplified_cohort_filter_properties_non_precalculated_cohort_with_behavioural_filter(self):
        cohort = Cohort.objects.create(
            team=self.team,
            name="cohortCeption",
            filters={
                "properties": {
                    "type": "AND",
                    "values": [
                        {"key": "name", "value": "test", "type": "person"},
                        {
                            "key": "$pageview",
                            "event_type": "events",
                            "time_interval": "day",
                            "time_value": 8,
                            "seq_time_interval": "day",
                            "seq_time_value": 3,
                            "seq_event": "$pageview",
                            "seq_event_type": "events",
                            "value": "performed_event_sequence",
                            "type": "behavioral",
                        },
                    ],
                }
            },
        )
    
>       cohort.calculate_people_ch(pending_version=0)

posthog/models/cohort/test/test_util.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/cohort/cohort.py:201: in calculate_people_ch
    count = recalculate_cohortpeople(self, pending_version)
posthog/models/cohort/util.py:263: in recalculate_cohortpeople
    sync_execute(
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nINSERT INTO cohortpeople\nSELECT id, %(cohort_id)s as cohort_id, %(team_id)s as team_id, 1 AS sign, %(new_version)s...M cohortpeople\nWHERE team_id = %(team_id)s AND cohort_id = %(cohort_id)s AND version < %(new_version)s AND sign = 1\n'
args = {'cohort_id': 256, 'kperson_filter_pre_256_0_0_0': 'name', 'kpersonquery_person_filter_fin_256_0_0_0': 'name', 'new_version': 0, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"workload"...direct,parallel_hash","distributed_replica_max_ignored_errors":1000,"optimize_on_insert":0}}', 'optimize_on_insert': 0}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorNumberOfArgumentsDoesntMatch: Code: 42.
E               DB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
E               2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse
E               3. ? @ 0x000000000885250c in /usr/bin/clickhouse
E               4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse
E               5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse
E               6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse
E               7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse
E               8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse
E               9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse
E               10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse
E               11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse
E               12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse
E               13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse
E               14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse
E               15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse
E               16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse
E               17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse
E               18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse
E               28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
E               29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse

posthog/clickhouse/client/execute.py:113: CHQueryErrorNumberOfArgumentsDoesntMatch
----------------------------- Captured stderr call -----------------------------
2023-07-10T13:21:33.550829Z [warning  ] cohort_calculation_failed      [posthog.models.cohort.cohort] current_version=None host= id=256 new_version=0 pid=152128 team_id=2290 tid=140141459998528 token=token123 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute
    result = client.execute(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute
    rv = self.process_ordinary_query(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query
    return self.receive_result(with_column_types=with_column_types,
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result
    return result.get_result()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result
    for packet in self.packet_generator:
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator
    packet = self.receive_packet()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet
    raise packet.exception
clickhouse_driver.errors.ServerException: Code: 42.
DB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse
3. ? @ 0x000000000885250c in /usr/bin/clickhouse
4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse
5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse
6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse
7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse
8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse
9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse
10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse
11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse
12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse
13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse
14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse
15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse
16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse
17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse
18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse
28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/posthog/posthog/models/cohort/cohort.py", line 201, in calculate_people_ch
    count = recalculate_cohortpeople(self, pending_version)
  File "/workspaces/posthog/posthog/models/cohort/util.py", line 263, in recalculate_cohortpeople
    sync_execute(
  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner
    return inner._impl(*args, **kwargs)  # type: ignore
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute
    raise err
posthog.errors.CHQueryErrorNumberOfArgumentsDoesntMatch: Code: 42.
DB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008850ead in /usr/bin/clickhouse
2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse
3. ? @ 0x000000000885250c in /usr/bin/clickhouse
4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse
5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse
6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse
7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse
8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse
9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse
10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse
11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse
12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse
13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse
14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse
15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse
16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse
17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse
18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse
28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse
29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse

------------------------------ Captured log call -------------------------------
WARNING  posthog.models.cohort.cohort:cohort.py:208 {'id': 256, 'current_version': None, 'new_version': 0, 'event': 'cohort_calculation_failed', 'token': 'token123', 'host': '', 'x_forwarded_for': '', 'team_id': 2290, 'timestamp': '2023-07-10T13:21:33.550829Z', 'logger': 'posthog.models.cohort.cohort', 'level': 'warning', 'pid': 152128, 'tid': 140141459998528, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute\n    result = client.execute(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute\n    rv = self.process_ordinary_query(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query\n    return self.receive_result(with_column_types=with_column_types,\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result\n    return result.get_result()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result\n    for packet in self.packet_generator:\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator\n    packet = self.receive_packet()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet\n    raise packet.exception\nclickhouse_driver.errors.ServerException: Code: 42.\nDB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008850ead in /usr/bin/clickhouse\n2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse\n3. ? @ 0x000000000885250c in /usr/bin/clickhouse\n4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse\n5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse\n6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse\n7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse\n8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse\n9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse\n10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse\n11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse\n12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse\n13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse\n14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse\n15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse\n16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse\n17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse\n18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse\n28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse\n29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/workspaces/posthog/posthog/models/cohort/cohort.py", line 201, in calculate_people_ch\n    count = recalculate_cohortpeople(self, pending_version)\n  File "/workspaces/posthog/posthog/models/cohort/util.py", line 263, in recalculate_cohortpeople\n    sync_execute(\n  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner\n    return inner._impl(*args, **kwargs)  # type: ignore\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute\n    raise err\nposthog.errors.CHQueryErrorNumberOfArgumentsDoesntMatch: Code: 42.\nDB::Exception: Function tuple requires at least one argument.: While processing (1 = 1) AND tuple(). Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008850ead in /usr/bin/clickhouse\n2. ? @ 0x000000000d322d8e in /usr/bin/clickhouse\n3. ? @ 0x000000000885250c in /usr/bin/clickhouse\n4. DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271db73 in /usr/bin/clickhouse\n5. DB::IFunctionOverloadResolver::getReturnType(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271d779 in /usr/bin/clickhouse\n6. DB::IFunctionOverloadResolver::build(std::vector<DB::ColumnWithTypeAndName, std::allocator<DB::ColumnWithTypeAndName>> const&) const @ 0x000000001271e496 in /usr/bin/clickhouse\n7. DB::ActionsDAG::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<DB::ActionsDAG::Node const*, std::allocator<DB::ActionsDAG::Node const*>>, String) @ 0x0000000012e98e3a in /usr/bin/clickhouse\n8. DB::ScopeStack::addFunction(std::shared_ptr<DB::IFunctionOverloadResolver> const&, std::vector<String, std::allocator<String>> const&, String) @ 0x00000000130a50c0 in /usr/bin/clickhouse\n9. ? @ 0x00000000130b09cf in /usr/bin/clickhouse\n10. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8230 in /usr/bin/clickhouse\n11. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0x00000000130a8cf2 in /usr/bin/clickhouse\n12. ? @ 0x000000001309ca75 in /usr/bin/clickhouse\n13. DB::ExpressionAnalyzer::getRootActions(std::shared_ptr<DB::IAST> const&, bool, std::shared_ptr<DB::ActionsDAG>&, bool) @ 0x000000001307af9b in /usr/bin/clickhouse\n14. DB::SelectQueryExpressionAnalyzer::appendWhere(DB::ExpressionActionsChain&, bool) @ 0x0000000013087435 in /usr/bin/clickhouse\n15. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::shared_ptr<DB::FilterDAGInfo> const&, std::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) @ 0x000000001308e894 in /usr/bin/clickhouse\n16. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0x0000000013a51c0c in /usr/bin/clickhouse\n17. ? @ 0x0000000013a49cba in /usr/bin/clickhouse\n18. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n19. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n20. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n21. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n22. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n23. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n25. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n26. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n27. DB::InterpreterInsertQuery::execute() @ 0x0000000013a1ff91 in /usr/bin/clickhouse\n28. ? @ 0x0000000013e18e53 in /usr/bin/clickhouse\n29. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n30. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n31. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n'}
___ TestOrganization.test_plugins_access_level_is_determined_based_on_realm ____

self = <posthog.models.test.test_organization_model.TestOrganization testMethod=test_plugins_access_level_is_determined_based_on_realm>

    def test_plugins_access_level_is_determined_based_on_realm(self):
        with self.is_cloud(True):
            new_org, _, _ = Organization.objects.bootstrap(self.user)
>           assert new_org.plugins_access_level == Organization.PluginsAccessLevel.CONFIG
E           AssertionError: assert <PluginsAccessLevel.ROOT: 9> == <PluginsAccessLevel.CONFIG: 3>
E            +  where <PluginsAccessLevel.ROOT: 9> = <Organization at 0x7f74a8566ad0: id=UUIDT('01893ff8-bb5c-0000-474d-3eb0a4e29eb4'), name=''>.plugins_access_level
E            +  and   <PluginsAccessLevel.CONFIG: 3> = <enum 'PluginsAccessLevel'>.CONFIG
E            +    where <enum 'PluginsAccessLevel'> = Organization.PluginsAccessLevel

posthog/models/test/test_organization_model.py:58: AssertionError
_________ TestOrganization.test_plugins_are_not_preinstalled_on_cloud __________

self = <posthog.models.test.test_organization_model.TestOrganization testMethod=test_plugins_are_not_preinstalled_on_cloud>
mock_get = <MagicMock name='get' id='140138410632352'>

    @mock.patch("requests.get", side_effect=mocked_plugin_requests_get)
    def test_plugins_are_not_preinstalled_on_cloud(self, mock_get):
        with self.is_cloud(True):
            with self.settings(PLUGINS_PREINSTALLED_URLS=["https://github.com/PostHog/helloworldplugin/"]):
                new_org, _, _ = Organization.objects.bootstrap(
                    self.user, plugins_access_level=Organization.PluginsAccessLevel.INSTALL
                )
    
>       self.assertEqual(Plugin.objects.filter(organization=new_org, is_preinstalled=True).count(), 0)
E       AssertionError: 1 != 0

posthog/models/test/test_organization_model.py:52: AssertionError
________ TestOrganization.test_plugins_are_preinstalled_on_self_hosted _________

__wrapped_mock_method__ = <function NonCallableMock.assert_any_call at 0x7f75377bbeb0>
args = (<MagicMock name='get' id='140139890073872'>, 'https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) c...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140139890073872'>
actual_args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
actual_kwargs = {'headers': {'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140139890073872'>
args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
kwargs = {'headers': {}}
expected = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
cause = None
actual = [call('https://api.github.com/repos/PostHog/helloworldplugin/commits?sha=&path=', headers={'Authorization': 'Bearer gh...d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})]
expected_string = "get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.
    
        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) call not found

/usr/local/lib/python3.10/unittest/mock.py:1000: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.models.test.test_organization_model.TestOrganization testMethod=test_plugins_are_preinstalled_on_self_hosted>
mock_get = <MagicMock name='get' id='140139890073872'>

    @mock.patch("requests.get", side_effect=mocked_plugin_requests_get)
    def test_plugins_are_preinstalled_on_self_hosted(self, mock_get):
        with self.is_cloud(False):
            with self.settings(PLUGINS_PREINSTALLED_URLS=["https://github.com/PostHog/helloworldplugin/"]):
                new_org, _, _ = Organization.objects.bootstrap(
                    self.user, plugins_access_level=Organization.PluginsAccessLevel.INSTALL
                )
    
        self.assertEqual(Plugin.objects.filter(organization=new_org, is_preinstalled=True).count(), 1)
        self.assertEqual(
            Plugin.objects.filter(organization=new_org, is_preinstalled=True).get().name, "helloworldplugin"
        )
        self.assertEqual(mock_get.call_count, 2)
>       mock_get.assert_any_call(
            f"https://github.com/PostHog/helloworldplugin/archive/{HELLO_WORLD_PLUGIN_GITHUB_ZIP[0]}.zip", headers={}
        )
E       AssertionError: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) call not found
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'token ghu                                  
E          -_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj                                  
E          -0xQq4Y',                                                         
E          -  },                                                             
E          -}

posthog/models/test/test_organization_model.py:40: AssertionError
_______________________ TestUser.test_analytics_metadata _______________________

self = <posthog.models.test.test_user_model.TestUser testMethod=test_analytics_metadata>

    def test_analytics_metadata(self):
        # One org, one team, anonymized
        organization, team, user = User.objects.bootstrap(
            organization_name="Test Org", email="test_org@posthog.com", password="12345678", anonymize_data=True
        )
    
        with self.is_cloud(True):
>           self.assertEqual(
                user.get_analytics_metadata(),
                {
                    "realm": "cloud",
                    "email_opt_in": False,
                    "anonymize_data": True,
                    "email": None,
                    "is_signed_up": True,
                    "organization_count": 1,
                    "project_count": 1,
                    "team_member_count_all": 1,
                    "completed_onboarding_once": False,
                    "organization_id": str(organization.id),
                    "project_id": str(team.uuid),
                    "project_setup_complete": False,
                    "has_password_set": True,
                    "joined_at": user.date_joined,
                    "has_social_auth": False,
                    "social_providers": [],
                    "instance_url": "http://localhost:8000",
                    "instance_tag": "none",
                    "is_email_verified": None,
                    "has_seen_product_intro_for": None,
                },
            )
E           AssertionError: {'realm': 'hosted-clickhouse', 'email_opt_in': False[595 chars]None} != {'realm': 'cloud', 'email_opt_in': False, 'anonymize[583 chars]None}
E           Diff is 733 characters long. Set self.maxDiff to None to see it.

posthog/models/test/test_user_model.py:19: AssertionError
_____________ TestPluginsUtils.test_download_plugin_archive_github _____________

__wrapped_mock_method__ = <function NonCallableMock.assert_called_with at 0x7f75377bbd00>
args = (<MagicMock name='get' id='140140316152000'>, 'https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "expected call not found.\nExpected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140140316152000'>
actual_args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
actual_kwargs = {'headers': {'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140140316152000'>
args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
kwargs = {'headers': {}}
expected = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
actual = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})
_error_message = <function NonCallableMock.assert_called_with.<locals>._error_message at 0x7f7484414670>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\nActual: %s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
E           Actual: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})

/usr/local/lib/python3.10/unittest/mock.py:929: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.plugins.test.test_utils.TestPluginsUtils testMethod=test_download_plugin_archive_github>
mock_get = <MagicMock name='get' id='140140316152000'>

    def test_download_plugin_archive_github(self, mock_get):
        plugin_github_zip_1 = download_plugin_archive(
            "https://www.github.com/PostHog/helloworldplugin/commit/82c9218ee40f561b7f37a22d6b6a0ca82887ee3e",
            HELLO_WORLD_PLUGIN_GITHUB_ZIP[0],
        )
        self.assertEqual(plugin_github_zip_1, base64.b64decode(HELLO_WORLD_PLUGIN_GITHUB_ZIP[1]))
        self.assertEqual(mock_get.call_count, 1)
>       mock_get.assert_called_with(
            "https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip",
            headers={},
        )
E       AssertionError: expected call not found.
E       Expected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
E       Actual: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'token ghu                                  
E          -_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj                                  
E          -0xQq4Y',                                                         
E          -  },                                                             
E          -}

posthog/plugins/test/test_utils.py:428: AssertionError
___________________ TestPluginsUtils.test_parse_github_urls ____________________

__wrapped_mock_method__ = <function NonCallableMock.assert_called_with at 0x7f75377bbd00>
args = (<MagicMock name='get' id='140140326651360'>, 'https://api.github.com/repos/PostHog/posthog/commits?sha=&path=')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "expected call not found.\nExpected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={}...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140140326651360'>
actual_args = ('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=',)
actual_kwargs = {'headers': {'Authorization': 'Bearer ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140140326651360'>
args = ('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=',)
kwargs = {'headers': {}}
expected = call('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
actual = call('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})
_error_message = <function NonCallableMock.assert_called_with.<locals>._error_message at 0x7f7484416830>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\nActual: %s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
E           Actual: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})

/usr/local/lib/python3.10/unittest/mock.py:929: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.plugins.test.test_utils.TestPluginsUtils testMethod=test_parse_github_urls>
mock_get = <MagicMock name='get' id='140140326651360'>

    def test_parse_github_urls(self, mock_get):
        parsed_url = parse_url("https://github.com/PostHog/posthog")
        self.assertEqual(parsed_url["type"], "github")
        self.assertEqual(parsed_url["user"], "PostHog")
        self.assertEqual(parsed_url["repo"], "posthog")
        self.assertEqual(parsed_url.get("tag", None), None)
        self.assertEqual(parsed_url.get("path", None), None)
        self.assertEqual(mock_get.call_count, 0)
        mock_get.reset_mock()
    
        parsed_url = parse_url("https://github.com/PostHog/posthog", get_latest_if_none=True)
        self.assertEqual(parsed_url["type"], "github")
        self.assertEqual(parsed_url["user"], "PostHog")
        self.assertEqual(parsed_url["repo"], "posthog")
        self.assertEqual(parsed_url["tag"], "MOCKLATESTCOMMIT")
        self.assertEqual(parsed_url.get("path", None), None)
        self.assertEqual(mock_get.call_count, 1)
>       mock_get.assert_called_with("https://api.github.com/repos/PostHog/posthog/commits?sha=&path=", headers={})
E       AssertionError: expected call not found.
E       Expected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
E       Actual: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rqj0xQq4Y'})
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'Bearer gh                                  
E          -u_sgkrqLLYCR8Y5eK1sdbwKzs8Lm1Rq                                  
E          -j0xQq4Y',                                                        
E          -  },                                                             
E          -}

posthog/plugins/test/test_utils.py:44: AssertionError
___ TestClickhouseSessionRecordingsListFromSessionReplay.test_action_filter ____
posthog/test/base.py:806: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_session_recording_list_from_session_replay.TestClickhouseSessionRecordingsListFromSessionReplay testMethod=test_action_filter>
query = '\n\nWITH\n        events_session_ids AS (\n            SELECT\n                groupUniqArray(event) as event_names,\...RE 1=1\n        GROUP BY session_id\n        HAVING 1=1\n        ORDER BY start_time DESC\n        LIMIT 51 OFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m        ...[0m[0m
E       [0m[2m              AND (has(['Firefox'], replaceRegexpAll(JSONExtractRaw(properties, '$browser'), '^"|"$', ''))[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mD[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mh[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90m[[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mc[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m][0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m)[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mD[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mh[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90m[[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mc[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m][0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mD[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mh[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23m[[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m][0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mD[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mh[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23m[[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m][0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[0m
E       [0m[2m     GROUP BY session_id[0m[0m
E       [0m[2m   ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:456: AssertionError
___________________ TestTrends.test_breakdown_by_group_props ___________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc38d780>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2900_None_yLLOFavG'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc38d780>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2900_None_yLLOFavG', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc38d780>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74f5542740>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc38d780>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc38d780>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2900) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props>

    def test_breakdown_by_group_props(self):
        self._create_groups()
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5"},
                    "group0_properties": {"industry": "finance"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 13),
                    "properties": {"$group_0": "org:6"},
                    "group0_properties": {"industry": "technology"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 15),
                    "properties": {"$group_0": "org:7", "$group_1": "company:10"},
                    "group0_properties": {"industry": "finance"},
                    "group1_properties": {"industry": "finance"},
                },
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
            }
        )
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6420: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2900) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
__________ TestTrends.test_breakdown_by_group_props_person_on_events ___________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140138410112576'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2901_None_3o2mXBlB', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140138410112576'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2901_None_3o2mXBlB', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140138410112576'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2901_None_3o2mXBlB', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = ()
kwargs = {'params': None, 'query_id': '2901_None_3o2mXBlB', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9720>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '2901_None_3o2mXBlB'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9720>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '2901_None_3o2mXBlB', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9720>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74842748e0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9720>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9720>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 2901) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance', 'technology']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_person_on_events>

    @also_test_with_materialized_columns(
        group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_by_group_props_person_on_events(self):
        self._create_groups()
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5"},
                    "group0_properties": {"industry": "finance"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 13),
                    "properties": {"$group_0": "org:6"},
                    "group0_properties": {"industry": "technology"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 15),
                    "properties": {"$group_0": "org:7", "$group_1": "company:10"},
                    "group0_properties": {"industry": "finance"},
                    "group1_properties": {"industry": "finance"},
                },
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01",
                "date_to": "2020-01-12",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6481: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 2901) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance', 'technology']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________ TestTrends.test_breakdown_by_group_props_with_person_filter __________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba560>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2902_None_soVJ8gI4'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba560>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2902_None_soVJ8gI4', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba560>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f7444a79a80>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba560>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba560>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM (SELECT timestamp, event, team_id, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2902 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND ((event = 'sign up') AND (team_id = 2902)))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2902) AND (id IN (SELECT id FROM person WHERE (team_id = 2902) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', ''))) AS person ON `--pdi.person_id` = id WHERE (team_id = 2902) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0' '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_with_person_filter>

    def test_breakdown_by_group_props_with_person_filter(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={"key": "value"},
            group0_properties={"industry": "finance"},
        )
        _create_event(
            event="sign up",
            distinct_id="person2",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={},
            group0_properties={"industry": "technology"},
        )
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [{"key": "key", "value": "value", "type": "person"}],
            }
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM (SELECT timestamp, event, team_id, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2902 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND ((event = 'sign up') AND (team_id = 2902)))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2902) AND (id IN (SELECT id FROM person WHERE (team_id = 2902) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', ''))) AS person ON `--pdi.person_id` = id WHERE (team_id = 2902) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0' '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_ TestTrends.test_breakdown_by_group_props_with_person_filter_person_on_events _

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140324649808'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2903_None_70ipuIAM', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140324649808'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2903_None_70ipuIAM', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140324649808'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '2903_None_70ipuIAM', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = ()
kwargs = {'params': None, 'query_id': '2903_None_70ipuIAM', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba6e0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '2903_None_70ipuIAM'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba6e0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '2903_None_70ipuIAM', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba6e0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74a85062c0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba6e0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318ba6e0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 2903) AND (event = 'sign up') AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', '')) AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'person_properties' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'person_properties', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_with_person_filter_person_on_events>

    @also_test_with_materialized_columns(
        person_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_by_group_props_with_person_filter_person_on_events(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={"key": "value"},
            group0_properties={"industry": "finance"},
        )
        _create_event(
            event="sign up",
            distinct_id="person2",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={},
            group0_properties={"industry": "technology"},
        )
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [{"key": "key", "value": "value", "type": "person"}],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6675: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 2903) AND (event = 'sign up') AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', '')) AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'person_properties' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'person_properties', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________________ TestTrends.test_breakdown_with_filter_groups _________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b89a0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2928_None_sRTzmBXx'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b89a0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2928_None_sRTzmBXx', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b89a0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74dc519840>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b89a0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b89a0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2928) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups>

    def test_breakdown_with_filter_groups(self):
        self._create_groups()
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
        )
    
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2928) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
________ TestTrends.test_breakdown_with_filter_groups_person_on_events _________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137878752272'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2929_None_R5EML8Mf', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137878752272'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2929_None_R5EML8Mf', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137878752272'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2929_None_R5EML8Mf', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '2929_None_R5EML8Mf', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8392aa0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2929_None_R5EML8Mf'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8392aa0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2929_None_R5EML8Mf', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8392aa0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74647b8cd0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8392aa0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8392aa0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2929) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups_person_on_events>

    @also_test_with_materialized_columns(
        event_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_with_filter_groups_person_on_events(self):
        self._create_groups()
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
        )
    
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6301: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 2929) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_______ TestTrends.test_breakdown_with_filter_groups_person_on_events_v2 _______

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...mpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137376903472'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2930_None_o5opOmf3', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137376903472'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2930_None_o5opOmf3', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137376903472'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '2930_None_o5opOmf3', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '2930_None_o5opOmf3', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9a80>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2930_None_o5opOmf3'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9a80>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2930_None_o5opOmf3', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9a80>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f744691c3a0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9a80>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9a80>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e ALL LEFT JOIN (SELECT argMax(override_person_id, version) AS person_id, old_person_id FROM person_overrides WHERE team_id = 2930 GROUP BY old_person_id) AS overrides ON person_id = old_person_id WHERE (team_id = 2930) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND notEmpty(person_id) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'person_id' 'team_id' 'event' 'old_person_id' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'person_id', 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties', joined columns: 'overrides.person_id' 'old_person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups_person_on_events_v2>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=True)
    @snapshot_clickhouse_queries
    def test_breakdown_with_filter_groups_person_on_events_v2(self):
        self._create_groups()
    
        id1 = str(uuid.uuid4())
        id2 = str(uuid.uuid4())
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d2",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
            person_id=id2,
        )
    
        create_person_id_override_by_distinct_id("test_breakdown_d1", "test_breakdown_d2", self.team.pk)
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0, "math": "dau"}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...mpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e ALL LEFT JOIN (SELECT argMax(override_person_id, version) AS person_id, old_person_id FROM person_overrides WHERE team_id = 2930 GROUP BY old_person_id) AS overrides ON person_id = old_person_id WHERE (team_id = 2930) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND notEmpty(person_id) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'person_id' 'team_id' 'event' 'old_person_id' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'person_id', 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties', joined columns: 'overrides.person_id' 'old_person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
________ TestTrends.test_filtering_by_multiple_groups_person_on_events _________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140317303152'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '2952_None_fUDAV5se', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140317303152'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '2952_None_fUDAV5se', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140317303152'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '2952_None_fUDAV5se', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '2952_None_fUDAV5se', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f7484112920>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2952_None_fUDAV5se'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f7484112920>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2952_None_fUDAV5se', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f7484112920>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74f5bb0ca0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f7484112920>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f7484112920>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' 'group_properties_2' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 2952) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['six'], replaceRegexpAll(JSONExtractRaw(group_properties_2, 'name'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'group_properties_2' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f2f2fccd609 in ?
E           30. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_by_multiple_groups_person_on_events>

    @also_test_with_materialized_columns(
        group_properties=[(0, "industry"), (2, "name")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_filtering_by_multiple_groups_person_on_events(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=2)
    
        create_group(team_id=self.team.pk, group_type_index=0, group_key="org:5", properties={"industry": "finance"})
        create_group(team_id=self.team.pk, group_type_index=0, group_key="org:6", properties={"industry": "technology"})
        create_group(team_id=self.team.pk, group_type_index=2, group_key="company:5", properties={"name": "five"})
        create_group(team_id=self.team.pk, group_type_index=2, group_key="company:6", properties={"name": "six"})
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5", "$group_2": "company:6"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12, 30),
                    "properties": {"$group_2": "company:6"},
                },
                {"event": "sign up", "timestamp": datetime(2020, 1, 2, 13), "properties": {"$group_0": "org:6"}},
                {"event": "sign up", "timestamp": datetime(2020, 1, 3, 15), "properties": {"$group_2": "company:5"}},
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12",
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "name", "value": "six", "type": "group", "group_type_index": 2},
                ],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6774: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' 'group_properties_2' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 2952) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['six'], replaceRegexpAll(JSONExtractRaw(group_properties_2, 'name'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'group_properties_2' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f2f2fccd609 in ?
E               30. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
__________________ TestTrends.test_filtering_with_group_props __________________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9840>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2954_None_jyjqpJe2'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9840>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2954_None_jyjqpJe2', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9840>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74f598b700>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9840>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f75318b9840>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2954 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 2954))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2954) AND (id IN (SELECT id FROM person WHERE (team_id = 2954) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 2954) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f2f2fccd609 in ?
E           30. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props>

    def test_filtering_with_group_props(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6581: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2954 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 2954))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2954) AND (id IN (SELECT id FROM person WHERE (team_id = 2954) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 2954) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f2f2fccd609 in ?
E               30. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_____ TestTrends.test_filtering_with_group_props_event_with_no_group_data ______

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc4fd3f0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2955_None_R9IUzFgj'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc4fd3f0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '2955_None_R9IUzFgj', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc4fd3f0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f744624d990>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc4fd3f0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74dc4fd3f0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2955 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 2955))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2955) AND (id IN (SELECT id FROM person WHERE (team_id = 2955) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 2955) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f2f2fccd609 in ?
E           30. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props_event_with_no_group_data>

    def test_filtering_with_group_props_event_with_no_group_data(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {
                        "key": "industry",
                        "operator": "is_not",
                        "value": "textiles",
                        "type": "group",
                        "group_type_index": 0,
                    },
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 2955 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 2955))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 2955) AND (id IN (SELECT id FROM person WHERE (team_id = 2955) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 2955) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f2f2fccd609 in ?
E               30. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________ TestTrends.test_filtering_with_group_props_person_on_events __________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140331135760'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '2956_None_nzyX36iH', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140331135760'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '2956_None_nzyX36iH', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140140331135760'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '2956_None_nzyX36iH', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
args = ()
kwargs = {'params': None, 'query_id': '2956_None_nzyX36iH', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8393130>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2956_None_nzyX36iH'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8393130>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '2956_None_nzyX36iH', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8393130>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f74f6a7c0a0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8393130>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74a8393130>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 2956) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'person_properties' 'group_properties_0' 'person_id', maybe you meant: 'team_id', 'event', 'timestamp', 'person_properties', 'group0_properties' or 'person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f2f2fccd609 in ?
E           30. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props_person_on_events>

    @also_test_with_materialized_columns(
        person_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_filtering_with_group_props_person_on_events(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6726: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 2956) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'person_properties' 'group_properties_0' 'person_id', maybe you meant: 'team_id', 'event', 'timestamp', 'person_properties', 'group0_properties' or 'person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f2f2fccd609 in ?
E               30. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
____________________ TestTrends.test_trends_with_hogql_math ____________________
posthog/test/base.py:806: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_trends_with_hogql_math>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...eTime('2020-01-04 23:59:59', 'UTC')\n\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m             ...[0m[0m
E       [0m[2m                         toStartOfWeek(toDateTime('2019-12-28 00:00:00', 'UTC'))[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mU[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mO[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mS[0m[48;5;225m[38;5;90mE[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90mE[0m[48;5;225m[38;5;90mC[0m[48;5;225m[38;5;90mT[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mp[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mv[0m[48;5;225m[38;5;90mg[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m6[0m[48;5;225m[38;5;90m4[0m[48;5;225m[38;5;90mO[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90me[0m[48;5;90m[38;5;225mv[0m[48;5;225m[38;5;90me[0m[48;5;90m[38;5;225mn[0m[48;5;90m[38;5;225mt[0m[48;5;225m[38;5;90ms[0m[48;5;90m[38;5;225m.[0m[48;5;90m[38;5;225m`[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;90m[38;5;225m`[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m1[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mS[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mU[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mC[0m[48;5;195m[38;5;23mT[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mv[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m6[0m[48;5;195m[38;5;23m4[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mc[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mR[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mg[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mx[0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195mA[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195m([0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23m([0m[48;5;23m[38;5;195mJ[0m[48;5;23m[38;5;195mS[0m[48;5;23m[38;5;195mO[0m[48;5;23m[38;5;195mN[0m[48;5;23m[38;5;195mE[0m[48;5;23m[38;5;195mx[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mc[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mR[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mw[0m[48;5;23m[38;5;195m([0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195mo[0m[48;5;23m[38;5;195mp[0m[48;5;195m[38;5;23me[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m^[0m[48;5;23m[38;5;195m"[0m[48;5;23m[38;5;195m|[0m[48;5;23m[38;5;195m"[0m[48;5;23m[38;5;195m$[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m1[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m                         toStartOfWeek(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date[0m[0m
E       [0m[2m             ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:456: AssertionError
_______________________ TestFormula.test_breakdown_hogql _______________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2019-12-28 00:00:00', 'date_to': '2020-01-04 23:59:59', 'e_0_math_prop': 'session duration', 'event_None': 'session start', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137391519008'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '3123_None_5hJpi84z', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137391519008'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '3123_None_5hJpi84z', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140137391519008'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '3123_None_5hJpi84z', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
args = ()
kwargs = {'params': None, 'query_id': '3123_None_5hJpi84z', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74844258a0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '3123_None_5hJpi84z'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74844258a0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '3123_None_5hJpi84z', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74844258a0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f744770f040>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74844258a0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74844258a0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, '$some_prop'), ''), 'null'), '^"|"$', '')), ''), ' : ', ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'location'), ''), 'null'), '^"|"$', '')), '')) AS value, sum(toFloat64OrNull(replaceRegexpAll(JSONExtractRaw(properties, 'session duration'), '^"|"$', ''))) AS count FROM events AS e WHERE (team_id = 3123) AND (event = 'session start') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2019-12-28 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-04 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'person_props' 'properties', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f2f2fccd609 in ?
E           24. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <test_formula.TestFormula testMethod=test_breakdown_hogql>

    @snapshot_clickhouse_queries
    def test_breakdown_hogql(self):
>       response = self._run(
            {"breakdown": "concat(person.properties.$some_prop, ' : ', properties.location)", "breakdown_type": "hogql"}
        )

posthog/queries/trends/test/test_formula.py:380: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/test/test_formula.py:97: in _run
    action_response = Trends().run(
posthog/queries/trends/trends.py:220: in run
    return handle_compare(filter, self._run_formula_query, team)
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/formula.py:27: in _run_formula_query
    _, sql, entity_params, _ = self._get_sql_for_entity(filter, team, entity)  # type: ignore
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2019-12-28 00:00:00', 'date_to': '2020-01-04 23:59:59', 'e_0_math_prop': 'session duration', 'event_None': 'session start', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, '$some_prop'), ''), 'null'), '^"|"$', '')), ''), ' : ', ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'location'), ''), 'null'), '^"|"$', '')), '')) AS value, sum(toFloat64OrNull(replaceRegexpAll(JSONExtractRaw(properties, 'session duration'), '^"|"$', ''))) AS count FROM events AS e WHERE (team_id = 3123) AND (event = 'session start') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2019-12-28 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-04 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'person_props' 'properties', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f2f2fccd609 in ?
E               24. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
____________ TestPerson.test_group_query_includes_recording_events _____________

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session...RDER BY actor_value DESC, actor_id DESC /* Also sorting by ID for determinism */\nLIMIT %(limit)s\nOFFSET %(offset)s\n'
args = {'date_from': '2021-01-21 00:00:00', 'date_to': '2021-01-21 23:59:59', 'event_0': 'pageview', 'limit': 100, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140139889272352'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '3143_None_C2BHhMNb', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140139889272352'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '3143_None_C2BHhMNb', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140139889272352'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '3143_None_C2BHhMNb', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
args = ()
kwargs = {'params': None, 'query_id': '3143_None_C2BHhMNb', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:677: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74f5d24b80>
query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, with_column_types = False, external_tables = None
query_id = '3143_None_C2BHhMNb'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74f5d24b80>
query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, with_column_types = False, external_tables = None
query_id = '3143_None_C2BHhMNb', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74f5d24b80>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7f7444a27d60>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74f5d24b80>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7f74f5d24b80>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: '$group_0' while processing query: 'SELECT `$group_0` AS actor_id, count() AS actor_value, groupUniqArray(100)((timestamp, uuid, `$session_id`, `$window_id`)) AS matching_events FROM (SELECT e.timestamp AS timestamp, replaceRegexpAll(JSONExtractRaw(e.properties, '$window_id'), '^"|"$', '') AS `$window_id`, replaceRegexpAll(JSONExtractRaw(e.properties, '$session_id'), '^"|"$', '') AS `$session_id`, e.uuid AS uuid FROM events AS e WHERE (team_id = 3143) AND (event = 'pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2021-01-21 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2021-01-21 23:59:59', 'UTC')) AND (`$group_0` != '')) GROUP BY actor_id ORDER BY actor_value DESC, actor_id DESC LIMIT 0, 100', required columns: '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id' '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           9. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           10. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           11. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           12. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           13. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           14. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           15. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           16. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           17. ? @ 0x00007f2f2fccd609 in ?
E           18. __clone @ 0x00007f2f2fbf2133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <test_person.TestPerson testMethod=test_group_query_includes_recording_events>

    @snapshot_clickhouse_queries
    @freeze_time("2021-01-21T20:00:00.000Z")
    def test_group_query_includes_recording_events(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        create_group(team_id=self.team.pk, group_type_index=0, group_key="bla", properties={})
        create_session_recording_events(self.team.pk, timezone.now(), "u1", "s1")
    
        _create_event(
            event="pageview", distinct_id="u1", team=self.team, timestamp=timezone.now(), properties={"$group_0": "bla"}
        )
        _create_event(
            event="pageview",
            distinct_id="u1",
            team=self.team,
            timestamp=timezone.now() + relativedelta(hours=2),
            properties={"$session_id": "s1", "$window_id": "w1", "$group_0": "bla"},
            event_uuid="b06e5a5e-e001-4293-af81-ac73e194569d",
        )
    
        event = {
            "id": "pageview",
            "name": "pageview",
            "type": "events",
            "order": 0,
            "math": "unique_group",
            "math_group_type_index": 0,
        }
    
        filter = Filter(
            data={
                "date_from": "2021-01-21T00:00:00Z",
                "date_to": "2021-01-21T23:59:59Z",
                "events": [event],
                "include_recordings": "true",
            }
        )
        entity = Entity(event)
    
>       _, serialized_actors, _ = TrendsActors(self.team, entity, filter).get_actors()

posthog/queries/trends/test/test_person.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/actor_base_query.py:102: in get_actors
    raw_result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session...RDER BY actor_value DESC, actor_id DESC /* Also sorting by ID for determinism */\nLIMIT %(limit)s\nOFFSET %(offset)s\n'
args = {'date_from': '2021-01-21 00:00:00', 'date_to': '2021-01-21 23:59:59', 'event_0': 'pageview', 'limit': 100, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"team_id":...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: '$group_0' while processing query: 'SELECT `$group_0` AS actor_id, count() AS actor_value, groupUniqArray(100)((timestamp, uuid, `$session_id`, `$window_id`)) AS matching_events FROM (SELECT e.timestamp AS timestamp, replaceRegexpAll(JSONExtractRaw(e.properties, '$window_id'), '^"|"$', '') AS `$window_id`, replaceRegexpAll(JSONExtractRaw(e.properties, '$session_id'), '^"|"$', '') AS `$session_id`, e.uuid AS uuid FROM events AS e WHERE (team_id = 3143) AND (event = 'pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2021-01-21 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2021-01-21 23:59:59', 'UTC')) AND (`$group_0` != '')) GROUP BY actor_id ORDER BY actor_value DESC, actor_id DESC LIMIT 0, 100', required columns: '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id' '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               9. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               10. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               11. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               12. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               13. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               14. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               15. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               16. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               17. ? @ 0x00007f2f2fccd609 in ?
E               18. __clone @ 0x00007f2f2fbf2133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
________________ TestExporterTask.test_exporter_setsup_selenium ________________

self = <posthog.tasks.test.test_exporter.TestExporterTask testMethod=test_exporter_setsup_selenium>
mock_uuid = <MagicMock name='uuid' id='140137385376736'>

    def test_exporter_setsup_selenium(self, mock_uuid: MagicMock) -> None:
>       driver = get_driver()

posthog/tasks/test/test_exporter.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/tasks/exports/image_exporter.py:66: in get_driver
    return webdriver.Chrome(
env/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:70: in __init__
    super(WebDriver, self).__init__(DesiredCapabilities.CHROME['browserName'], "goog",
env/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py:92: in __init__
    RemoteWebDriver.__init__(
env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:275: in __init__
    self.start_session(capabilities, browser_profile)
env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:365: in start_session
    response = self.execute(Command.NEW_SESSION, parameters)
env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:430: in execute
    self.error_handler.check_response(response)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <selenium.webdriver.remote.errorhandler.ErrorHandler object at 0x7f7444add210>
response = {'status': 500, 'value': '{"value":{"error":"unknown error","message":"unknown error: cannot find Chrome binary","stac...wn>\\n#15 0x560d0ebf3847 \\u003Cunknown>\\n#16 0x560d0ec03243 \\u003Cunknown>\\n#17 0x7f7c86d38ea7 start_thread\\n"}}'}

    def check_response(self, response: Dict[str, Any]) -> None:
        """
        Checks that a JSON response from the WebDriver does not have an error.
    
        :Args:
         - response - The JSON response from the WebDriver server as a dictionary
           object.
    
        :Raises: If the response contains an error message.
        """
        status = response.get('status', None)
        if not status or status == ErrorCode.SUCCESS:
            return
        value = None
        message = response.get("message", "")
        screen: str = response.get("screen", "")
        stacktrace = None
        if isinstance(status, int):
            value_json = response.get('value', None)
            if value_json and isinstance(value_json, str):
                import json
                try:
                    value = json.loads(value_json)
                    if len(value.keys()) == 1:
                        value = value['value']
                    status = value.get('error', None)
                    if not status:
                        status = value.get("status", ErrorCode.UNKNOWN_ERROR)
                        message = value.get("value") or value.get("message")
                        if not isinstance(message, str):
                            value = message
                            message = message.get('message')
                    else:
                        message = value.get('message', None)
                except ValueError:
                    pass
    
        exception_class: Type[WebDriverException]
        if status in ErrorCode.NO_SUCH_ELEMENT:
            exception_class = NoSuchElementException
        elif status in ErrorCode.NO_SUCH_FRAME:
            exception_class = NoSuchFrameException
        elif status in ErrorCode.NO_SUCH_SHADOW_ROOT:
            exception_class = NoSuchShadowRootException
        elif status in ErrorCode.NO_SUCH_WINDOW:
            exception_class = NoSuchWindowException
        elif status in ErrorCode.STALE_ELEMENT_REFERENCE:
            exception_class = StaleElementReferenceException
        elif status in ErrorCode.ELEMENT_NOT_VISIBLE:
            exception_class = ElementNotVisibleException
        elif status in ErrorCode.INVALID_ELEMENT_STATE:
            exception_class = InvalidElementStateException
        elif status in ErrorCode.INVALID_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR \
                or status in ErrorCode.INVALID_XPATH_SELECTOR_RETURN_TYPER:
            exception_class = InvalidSelectorException
        elif status in ErrorCode.ELEMENT_IS_NOT_SELECTABLE:
            exception_class = ElementNotSelectableException
        elif status in ErrorCode.ELEMENT_NOT_INTERACTABLE:
            exception_class = ElementNotInteractableException
        elif status in ErrorCode.INVALID_COOKIE_DOMAIN:
            exception_class = InvalidCookieDomainException
        elif status in ErrorCode.UNABLE_TO_SET_COOKIE:
            exception_class = UnableToSetCookieException
        elif status in ErrorCode.TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.SCRIPT_TIMEOUT:
            exception_class = TimeoutException
        elif status in ErrorCode.UNKNOWN_ERROR:
            exception_class = WebDriverException
        elif status in ErrorCode.UNEXPECTED_ALERT_OPEN:
            exception_class = UnexpectedAlertPresentException
        elif status in ErrorCode.NO_ALERT_OPEN:
            exception_class = NoAlertPresentException
        elif status in ErrorCode.IME_NOT_AVAILABLE:
            exception_class = ImeNotAvailableException
        elif status in ErrorCode.IME_ENGINE_ACTIVATION_FAILED:
            exception_class = ImeActivationFailedException
        elif status in ErrorCode.MOVE_TARGET_OUT_OF_BOUNDS:
            exception_class = MoveTargetOutOfBoundsException
        elif status in ErrorCode.JAVASCRIPT_ERROR:
            exception_class = JavascriptException
        elif status in ErrorCode.SESSION_NOT_CREATED:
            exception_class = SessionNotCreatedException
        elif status in ErrorCode.INVALID_ARGUMENT:
            exception_class = InvalidArgumentException
        elif status in ErrorCode.NO_SUCH_COOKIE:
            exception_class = NoSuchCookieException
        elif status in ErrorCode.UNABLE_TO_CAPTURE_SCREEN:
            exception_class = ScreenshotException
        elif status in ErrorCode.ELEMENT_CLICK_INTERCEPTED:
            exception_class = ElementClickInterceptedException
        elif status in ErrorCode.INSECURE_CERTIFICATE:
            exception_class = InsecureCertificateException
        elif status in ErrorCode.INVALID_COORDINATES:
            exception_class = InvalidCoordinatesException
        elif status in ErrorCode.INVALID_SESSION_ID:
            exception_class = InvalidSessionIdException
        elif status in ErrorCode.UNKNOWN_METHOD:
            exception_class = UnknownMethodException
        else:
            exception_class = WebDriverException
        if not value:
            value = response['value']
        if isinstance(value, str):
            raise exception_class(value)
        if message == "" and 'message' in value:
            message = value['message']
    
        screen = None  # type: ignore[assignment]
        if 'screen' in value:
            screen = value['screen']
    
        stacktrace = None
        st_value = value.get('stackTrace') or value.get('stacktrace')
        if st_value:
            if isinstance(st_value, str):
                stacktrace = st_value.split('\n')
            else:
                stacktrace = []
                try:
                    for frame in st_value:
                        line = self._value_or_default(frame, 'lineNumber', '')
                        file = self._value_or_default(frame, 'fileName', '<anonymous>')
                        if line:
                            file = "%s:%s" % (file, line)
                        meth = self._value_or_default(frame, 'methodName', '<anonymous>')
                        if 'className' in frame:
                            meth = "%s.%s" % (frame['className'], meth)
                        msg = "    at %s (%s)"
                        msg = msg % (meth, file)
                        stacktrace.append(msg)
                except TypeError:
                    pass
        if exception_class == UnexpectedAlertPresentException:
            alert_text = None
            if 'data' in value:
                alert_text = value['data'].get('text')
            elif 'alert' in value:
                alert_text = value['alert'].get('text')
            raise exception_class(message, screen, stacktrace, alert_text)  # type: ignore[call-arg]  # mypy is not smart enough here
>       raise exception_class(message, screen, stacktrace)
E       selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary
E       Stacktrace:
E       #0 0x560d0ec0a4e3 <unknown>
E       #1 0x560d0e939c76 <unknown>
E       #2 0x560d0e960757 <unknown>
E       #3 0x560d0e95f029 <unknown>
E       #4 0x560d0e99dccc <unknown>
E       #5 0x560d0e99d47f <unknown>
E       #6 0x560d0e994de3 <unknown>
E       #7 0x560d0e96a2dd <unknown>
E       #8 0x560d0e96b34e <unknown>
E       #9 0x560d0ebca3e4 <unknown>
E       #10 0x560d0ebce3d7 <unknown>
E       #11 0x560d0ebd8b20 <unknown>
E       #12 0x560d0ebcf023 <unknown>
E       #13 0x560d0eb9d1aa <unknown>
E       #14 0x560d0ebf36b8 <unknown>
E       #15 0x560d0ebf3847 <unknown>
E       #16 0x560d0ec03243 <unknown>
E       #17 0x7f7c86d38ea7 start_thread

env/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py:247: WebDriverException
___________________________ test_prepare_dictionary ____________________________

query_inputs = QueryInputs(partition_ids=[], team_ids=[], person_overrides_to_delete=[], dictionary_name='fancy_dictionary', dry_run=False, _latest_created_at=None)
activity_environment = <temporalio.testing._activity.ActivityEnvironment object at 0x7f7444aa9e10>
person_overrides_data = {1: {PersonOverrideTuple(old_person_id=UUID('190835f2-b5eb-42ea-9e70-6d7420918f13'), override_person_id=UUID('e649c22c...son_id=UUID('eac0edf4-ae10-4eb6-8e86-f2b79c36ada5'), override_person_id=UUID('2d69aa2f-a1d8-4df1-a7c7-352aa3adc3c3'))}}

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_prepare_dictionary(query_inputs, activity_environment, person_overrides_data):
        """Test a DICTIONARY is created by the prepare_dictionary activity."""
        query_inputs.dictionary_name = "fancy_dictionary"
        query_inputs.dry_run = False
    
        latest_merge_at = await activity_environment.run(prepare_dictionary, query_inputs)
    
>       assert latest_merge_at == OVERRIDES_CREATED_AT.isoformat()
E       AssertionError: assert equals failed
E          -'2023-07-10T13:33:17+00:00'       +'2020-01-02T00:00:00.123123+00: 
E                                           +00'

posthog/temporal/tests/test_squash_person_overrides_workflow.py:136: AssertionError
____________________ test_squash_person_overrides_workflow _____________________

query_inputs = QueryInputs(partition_ids=[], team_ids=[], person_overrides_to_delete=[], dictionary_name='person_overrides_join_dict', dry_run=True, _latest_created_at=None)
events_to_override = [{'event': 'test-event', 'person_id': UUID('ac120bdd-edd3-405a-8180-1d469175f918'), 'team_id': 1, 'timestamp': datetim...'), 'team_id': 2, 'timestamp': datetime.datetime(2020, 1, 1, 0, 0, 0, 123123, tzinfo=datetime.timezone.utc), ...}, ...]
person_overrides_data = {1: {PersonOverrideTuple(old_person_id=UUID('47c4c3ff-52f6-4c66-91e9-d97c995ebbaf'), override_person_id=UUID('50334112...son_id=UUID('cf7d6291-e59b-45e4-a46b-8f6dc3c6d202'), override_person_id=UUID('e5f56d7e-cd3e-4e93-9d89-dad9c6e52b7e'))}}
person_overrides = PersonOverrideTuple(old_person_id=UUID('157e6322-259b-46a4-81c8-91713defaf86'), override_person_id=UUID('628d5828-d6cf-40ad-ad4b-82e89329b6bf'))
person_overrides_table = None

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_squash_person_overrides_workflow(
        query_inputs,
        events_to_override,
        person_overrides_data,
        person_overrides,
        person_overrides_table,
    ):
        """Test the squash_person_overrides workflow end-to-end."""
>       client = await Client.connect(
            f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
            namespace=settings.TEMPORAL_NAMESPACE,
        )

posthog/temporal/tests/test_squash_person_overrides_workflow.py:1178: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________ test_squash_person_overrides_workflow_with_newer_overrides __________

query_inputs = QueryInputs(partition_ids=[], team_ids=[], person_overrides_to_delete=[], dictionary_name='person_overrides_join_dict', dry_run=True, _latest_created_at=None)
events_to_override = [{'event': 'test-event', 'person_id': UUID('7863b639-5027-4727-be26-9f73ed1e752c'), 'team_id': 1, 'timestamp': datetim...'), 'team_id': 2, 'timestamp': datetime.datetime(2020, 1, 1, 0, 0, 0, 123123, tzinfo=datetime.timezone.utc), ...}, ...]
person_overrides_data = {1: {PersonOverrideTuple(old_person_id=UUID('0c353436-e788-4d7c-a1f0-8aa0924b9a23'), override_person_id=UUID('c9398b0c...son_id=UUID('e7242b21-62e2-4946-8aad-71a0a970e2b8'), override_person_id=UUID('1314bb6d-3b3f-438d-8dbb-55a7254abb25'))}}
person_overrides = PersonOverrideTuple(old_person_id=UUID('19e6ce21-d491-4df4-a594-6ed4d742212c'), override_person_id=UUID('db193c9f-51f1-49ac-bb67-7f68d62b8f17'))
newer_overrides = defaultdict(<class 'set'>, {1: {PersonOverrideTuple(old_person_id=UUID('730d70b9-5e7a-4660-8291-4aeebcf05f9b'), overri...on_id=UUID('e7242b21-62e2-4946-8aad-71a0a970e2b8'), override_person_id=UUID('66576d0f-bf18-435c-ba38-e1f2eeb4d423'))}})

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_squash_person_overrides_workflow_with_newer_overrides(
        query_inputs, events_to_override, person_overrides_data, person_overrides, newer_overrides
    ):
        """Test the squash_person_overrides workflow end-to-end with newer overrides."""
>       client = await Client.connect(
            f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
            namespace=settings.TEMPORAL_NAMESPACE,
        )

posthog/temporal/tests/test_squash_person_overrides_workflow.py:1223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_________ test_squash_person_overrides_workflow_with_limited_team_ids __________

query_inputs = QueryInputs(partition_ids=[], team_ids=[], person_overrides_to_delete=[], dictionary_name='person_overrides_join_dict', dry_run=True, _latest_created_at=None)
events_to_override = [{'event': 'test-event', 'person_id': UUID('9d8e2889-b0ab-4f21-853c-010860bf50f2'), 'team_id': 1, 'timestamp': datetim...'), 'team_id': 2, 'timestamp': datetime.datetime(2020, 1, 1, 0, 0, 0, 123123, tzinfo=datetime.timezone.utc), ...}, ...]
person_overrides_data = {1: {PersonOverrideTuple(old_person_id=UUID('50a24de5-c01c-4b19-b56f-8f341bd8a161'), override_person_id=UUID('f10a098d...son_id=UUID('3b498cfb-0819-4010-a44c-2adf75274e00'), override_person_id=UUID('c2583164-be52-443a-86fb-8f8e4ecd4d54'))}}
person_overrides = PersonOverrideTuple(old_person_id=UUID('5849672c-bc59-47ff-bea3-13d2c9116f46'), override_person_id=UUID('9ca8f4cb-2610-479b-83ce-06e82a6a3273'))

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_squash_person_overrides_workflow_with_limited_team_ids(
        query_inputs, events_to_override, person_overrides_data, person_overrides
    ):
        """Test the squash_person_overrides workflow end-to-end."""
>       client = await Client.connect(
            f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
            namespace=settings.TEMPORAL_NAMESPACE,
        )

posthog/temporal/tests/test_squash_person_overrides_workflow.py:1265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
__________________ test_s3_export_workflow_with_minio_bucket ___________________

client = <django.test.client.Client object at 0x7f7444aca380>
s3_client = <botocore.client.S3 object at 0x7f7446c59180>

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_s3_export_workflow_with_minio_bucket(client: HttpClient, s3_client):
        """
        Test that the whole workflow not just the activity works. It should update
        the batch export run status to completed, as well as updating the record
        count.
        """
        ch_client = ChClient(
            url=settings.CLICKHOUSE_HTTP_URL,
            user=settings.CLICKHOUSE_USER,
            password=settings.CLICKHOUSE_PASSWORD,
            database=settings.CLICKHOUSE_DATABASE,
        )
    
        destination_data = {
            "type": "S3",
            "config": {
                "bucket_name": "my-production-s3-bucket",
                "region": "us-east-1",
                "prefix": "posthog-events/",
                "batch_window_size": 3600,
                "aws_access_key_id": "abc123",
                "aws_secret_access_key": "secret",
            },
        }
    
        batch_export_data = {
            "name": "my-production-s3-bucket-destination",
            "destination": destination_data,
            "interval": "hour",
        }
    
        organization = await acreate_organization("test")
        team = await acreate_team(organization=organization)
>       batch_export = await acreate_batch_export(
            team_id=team.pk,
            name=batch_export_data["name"],
            destination_data=batch_export_data["destination"],
            interval=batch_export_data["interval"],
        )

posthog/temporal/tests/batch_exports/test_s3_batch_export_workflow.py:333: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/batch_exports/service.py:326: in acreate_batch_export
    return await sync_to_async(create_batch_export)(team_id, interval, name, destination_data)  # type: ignore
env/lib/python3.10/site-packages/asgiref/sync.py:404: in __call__
    ret = await asyncio.wait_for(future, timeout=None)
/usr/local/lib/python3.10/asyncio/tasks.py:408: in wait_for
    return await fut
/usr/local/lib/python3.10/concurrent/futures/thread.py:58: in run
    result = self.fn(*self.args, **self.kwargs)
env/lib/python3.10/site-packages/asgiref/sync.py:443: in thread_handler
    return func(*args, **kwargs)
posthog/batch_exports/service.py:289: in create_batch_export
    temporal = sync_connect()
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_ test_snowflake_export_workflow_exports_events_in_the_last_hour_for_the_right_team _

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_snowflake_export_workflow_exports_events_in_the_last_hour_for_the_right_team():
        """Test that the whole workflow not just the activity works.
    
        It should update the batch export run status to completed, as well as updating the record
        count.
        """
        ch_client = ChClient(
            url=settings.CLICKHOUSE_HTTP_URL,
            user=settings.CLICKHOUSE_USER,
            password=settings.CLICKHOUSE_PASSWORD,
            database=settings.CLICKHOUSE_DATABASE,
        )
    
        destination_data = {
            "type": "Snowflake",
            "config": {
                "user": "hazzadous",
                "password": "password",
                "account": "account",
                "database": "PostHog",
                "schema": "test",
                "warehouse": "COMPUTE_WH",
                "table_name": "events",
            },
        }
    
        batch_export_data = {
            "name": "my-production-snowflake-bucket-destination",
            "destination": destination_data,
            "interval": "hour",
        }
    
        organization = await acreate_organization("test")
        team = await acreate_team(organization=organization)
>       batch_export = await acreate_batch_export(
            team_id=team.pk,
            name=batch_export_data["name"],
            destination_data=batch_export_data["destination"],
            interval=batch_export_data["interval"],
        )

posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py:262: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/batch_exports/service.py:326: in acreate_batch_export
    return await sync_to_async(create_batch_export)(team_id, interval, name, destination_data)  # type: ignore
env/lib/python3.10/site-packages/asgiref/sync.py:404: in __call__
    ret = await asyncio.wait_for(future, timeout=None)
/usr/local/lib/python3.10/asyncio/tasks.py:408: in wait_for
    return await fut
/usr/local/lib/python3.10/concurrent/futures/thread.py:58: in run
    result = self.fn(*self.args, **self.kwargs)
env/lib/python3.10/site-packages/asgiref/sync.py:443: in thread_handler
    return func(*args, **kwargs)
posthog/batch_exports/service.py:289: in create_batch_export
    temporal = sync_connect()
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___________ test_snowflake_export_workflow_raises_error_on_put_fail ____________

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_snowflake_export_workflow_raises_error_on_put_fail():
        destination_data = {
            "type": "Snowflake",
            "config": {
                "user": "hazzadous",
                "password": "password",
                "account": "account",
                "database": "PostHog",
                "schema": "test",
                "warehouse": "COMPUTE_WH",
                "table_name": "events",
            },
        }
    
        batch_export_data = {
            "name": "my-production-snowflake-bucket-destination",
            "destination": destination_data,
            "interval": "hour",
        }
    
        organization = await acreate_organization("test")
        team = await acreate_team(organization=organization)
>       batch_export = await acreate_batch_export(
            team_id=team.pk,
            name=batch_export_data["name"],
            destination_data=batch_export_data["destination"],
            interval=batch_export_data["interval"],
        )

posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py:484: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/batch_exports/service.py:326: in acreate_batch_export
    return await sync_to_async(create_batch_export)(team_id, interval, name, destination_data)  # type: ignore
env/lib/python3.10/site-packages/asgiref/sync.py:404: in __call__
    ret = await asyncio.wait_for(future, timeout=None)
/usr/local/lib/python3.10/asyncio/tasks.py:408: in wait_for
    return await fut
/usr/local/lib/python3.10/concurrent/futures/thread.py:58: in run
    result = self.fn(*self.args, **self.kwargs)
env/lib/python3.10/site-packages/asgiref/sync.py:443: in thread_handler
    return func(*args, **kwargs)
posthog/batch_exports/service.py:289: in create_batch_export
    temporal = sync_connect()
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
___________ test_snowflake_export_workflow_raises_error_on_copy_fail ___________

    @pytest.mark.django_db
    @pytest.mark.asyncio
    async def test_snowflake_export_workflow_raises_error_on_copy_fail():
        destination_data = {
            "type": "Snowflake",
            "config": {
                "user": "hazzadous",
                "password": "password",
                "account": "account",
                "database": "PostHog",
                "schema": "test",
                "warehouse": "COMPUTE_WH",
                "table_name": "events",
            },
        }
    
        batch_export_data = {
            "name": "my-production-snowflake-bucket-destination",
            "destination": destination_data,
            "interval": "hour",
        }
    
        organization = await acreate_organization("test")
        team = await acreate_team(organization=organization)
>       batch_export = await acreate_batch_export(
            team_id=team.pk,
            name=batch_export_data["name"],
            destination_data=batch_export_data["destination"],
            interval=batch_export_data["interval"],
        )

posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py:578: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/batch_exports/service.py:326: in acreate_batch_export
    return await sync_to_async(create_batch_export)(team_id, interval, name, destination_data)  # type: ignore
env/lib/python3.10/site-packages/asgiref/sync.py:404: in __call__
    ret = await asyncio.wait_for(future, timeout=None)
/usr/local/lib/python3.10/asyncio/tasks.py:408: in wait_for
    return await fut
/usr/local/lib/python3.10/concurrent/futures/thread.py:58: in run
    result = self.fn(*self.args, **self.kwargs)
env/lib/python3.10/site-packages/asgiref/sync.py:443: in thread_handler
    return func(*args, **kwargs)
posthog/batch_exports/service.py:289: in create_batch_export
    temporal = sync_connect()
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
env/lib/python3.10/site-packages/asgiref/sync.py:268: in main_wrap
    result = await self.awaitable(*args, **kwargs)
posthog/temporal/client.py:33: in sync_connect
    client = await connect(
posthog/temporal/client.py:19: in connect
    client = await Client.connect(
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
_ TestAutoProjectMiddleware.test_project_switched_when_accessing_dashboard_of_another_accessible_team _

self = <posthog.test.test_middleware.TestAutoProjectMiddleware testMethod=test_project_switched_when_accessing_dashboard_of_another_accessible_team>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_project_switched_when_accessing_dashboard_of_another_accessible_team(self):
        dashboard = Dashboard.objects.create(team=self.second_team)
>       with self.assertNumQueries(self.base_app_num_queries + 4):  # AutoProjectMiddleware adds 4 queries

posthog/test/test_middleware.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 41 != 45 : 41 queries executed, 45 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:04.455923+00:00'::timestamptz AND "django_session"."session_key" = 'jxfukbjrk15r4a377y7dz3vqn3996ouh') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /**/
E   3. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /**/
E   4. SELECT (1) AS "a" FROM "posthog_dashboard" WHERE (NOT ("posthog_dashboard"."deleted") AND NOT "posthog_dashboard"."deleted" AND "posthog_dashboard"."id" = 649 AND "posthog_dashboard"."team_id" = 3220) LIMIT 1 /**/
E   5. SELECT "posthog_dashboard"."id", "posthog_dashboard"."team_id", "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_dashboard" INNER JOIN "posthog_team" ON ("posthog_dashboard"."team_id" = "posthog_team"."id") WHERE (NOT ("posthog_dashboard"."deleted") AND NOT "posthog_dashboard"."deleted" AND "posthog_dashboard"."id" = 649) ORDER BY "posthog_dashboard"."id" ASC LIMIT 1 /**/
E   6. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /**/
E   7. UPDATE "posthog_user" SET "password" = 'pbkdf2_sha256$260000$IYummxPmNXPiUItPDlMbZd$yKDjn09S3O4wukm/g5wyLQ69gVIIqsLIQfEAtHEPyas=', "last_login" = '2023-07-10T13:36:04.448716+00:00'::timestamptz, "first_name" = '', "last_name" = '', "is_staff" = false, "is_active" = true, "date_joined" = '2023-07-10T13:36:04.000035+00:00'::timestamptz, "uuid" = '01894004-31a0-0000-b29c-c9f4926f9990'::uuid, "current_organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid, "current_team_id" = 3221, "email" = 'user1@posthog.com', "pending_email" = NULL, "temporary_token" = NULL, "distinct_id" = 'zhEJlgQajferJDutmaS0iE4uQtCBNF6TzyNkOCHUG16', "is_email_verified" = NULL, "has_seen_product_intro_for" = NULL, "email_opt_in" = false, "partial_notification_settings" = NULL, "anonymize_data" = false, "toolbar_mode" = 'toolbar', "events_column_config" = '{"active": "DEFAULT"}' WHERE "posthog_user"."id" = 3000 /**/
E   8. SELECT (1) AS "a" FROM "posthog_user" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   9. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:04.467531+00:00'::timestamptz AND "django_session"."session_key" = 'jxfukbjrk15r4a377y7dz3vqn3996ouh') LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   10. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" IN ('constance:posthog:SLACK_APP_CLIENT_ID', 'constance:posthog:SLACK_APP_CLIENT_SECRET', 'constance:posthog:SLACK_APP_SIGNING_SECRET') /*controller='posthog.urls.home',route='%5E.%2A'*/
E   11. SELECT COUNT(*) AS "__count" FROM "posthog_user" /*controller='posthog.urls.home',route='%5E.%2A'*/
E   12. SELECT (1) AS "a" FROM "posthog_organization" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   13. SELECT (1) AS "a" FROM "posthog_organization" WHERE NOT "posthog_organization"."for_internal_metrics" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   14. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   15. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_HOST' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   16. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$pageview' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   17. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$screen' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   18. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   19. SELECT COUNT(*) FROM (SELECT DISTINCT "posthog_organizationmembership"."user_id" AS Col1 FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."organization_id" IN (SELECT U0."id" FROM "posthog_organization" U0 INNER JOIN "posthog_organizationmembership" U1 ON (U0."id" = U1."organization_id") WHERE U1."user_id" = 3000)) subquery /*controller='posthog.urls.home',route='%5E.%2A'*/
E   20. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3221 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   21. SELECT COUNT(*) AS "__count" FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   22. SELECT (1) AS "a" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organization"."available_features" @> ARRAY['project_based_permissioning']::varchar(64)[] AND "posthog_organizationmembership"."user_id" = 3000) LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   23. SELECT COUNT(*) AS "__count" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   24. SELECT (1) AS "a" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organizationmembership"."user_id" = 3000 AND "posthog_team"."completed_snippet_onboarding" AND "posthog_team"."ingested_event") LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   25. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   26. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   27. SELECT "social_auth_usersocialauth"."provider" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   28. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   29. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   30. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid /*controller='posthog.urls.home',route='%5E.%2A'*/
E   31. SELECT "posthog_team"."id", "posthog_team"."organization_id", "posthog_team"."access_control" FROM "posthog_team" WHERE "posthog_team"."organization_id" IN ('01894004-3199-0000-b6c7-2c09ea655467'::uuid) /*controller='posthog.urls.home',route='%5E.%2A'*/
E   32. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   33. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at" FROM "posthog_organizationmembership" WHERE ("posthog_organizationmembership"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid AND "posthog_organizationmembership"."user_id" = 3000) ORDER BY "posthog_organizationmembership"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   34. SELECT "otp_static_staticdevice"."id", "otp_static_staticdevice"."user_id", "otp_static_staticdevice"."name", "otp_static_staticdevice"."confirmed", "otp_static_staticdevice"."throttling_failure_timestamp", "otp_static_staticdevice"."throttling_failure_count" FROM "otp_static_staticdevice" WHERE ("otp_static_staticdevice"."user_id" = 3000 AND "otp_static_staticdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   35. SELECT "otp_totp_totpdevice"."id", "otp_totp_totpdevice"."user_id", "otp_totp_totpdevice"."name", "otp_totp_totpdevice"."confirmed", "otp_totp_totpdevice"."throttling_failure_timestamp", "otp_totp_totpdevice"."throttling_failure_count", "otp_totp_totpdevice"."key", "otp_totp_totpdevice"."step", "otp_totp_totpdevice"."t0", "otp_totp_totpdevice"."digits", "otp_totp_totpdevice"."tolerance", "otp_totp_totpdevice"."drift", "otp_totp_totpdevice"."last_t" FROM "otp_totp_totpdevice" WHERE ("otp_totp_totpdevice"."user_id" = 3000 AND "otp_totp_totpdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   36. SELECT "two_factor_phonedevice"."id", "two_factor_phonedevice"."user_id", "two_factor_phonedevice"."name", "two_factor_phonedevice"."confirmed", "two_factor_phonedevice"."throttling_failure_timestamp", "two_factor_phonedevice"."throttling_failure_count", "two_factor_phonedevice"."number", "two_factor_phonedevice"."key", "two_factor_phonedevice"."method" FROM "two_factor_phonedevice" WHERE ("two_factor_phonedevice"."user_id" = 3000 AND "two_factor_phonedevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   37. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   38. SELECT (1) AS "a" FROM "posthog_grouptypemapping" WHERE "posthog_grouptypemapping"."team_id" = 3220 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   39. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   40. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:GROUPS_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   41. SELECT "posthog_pluginconfig"."id", "posthog_pluginconfig"."config", "posthog_plugin"."config_schema", "posthog_plugin"."id", "posthog_plugin"."plugin_type", "posthog_plugin"."name" FROM "posthog_plugin" INNER JOIN "posthog_pluginconfig" ON ("posthog_plugin"."id" = "posthog_pluginconfig"."plugin_id") INNER JOIN "posthog_pluginsourcefile" ON ("posthog_plugin"."id" = "posthog_pluginsourcefile"."plugin_id") WHERE ("posthog_pluginconfig"."enabled" AND "posthog_pluginconfig"."team_id" = 3220 AND "posthog_pluginsourcefile"."filename" = 'frontend.tsx' AND "posthog_pluginsourcefile"."status" = 'TRANSPILED') /*controller='posthog.urls.home',route='%5E.%2A'*/
_ TestAutoProjectMiddleware.test_project_switched_when_accessing_feature_flag_of_another_accessible_team _

self = <posthog.test.test_middleware.TestAutoProjectMiddleware testMethod=test_project_switched_when_accessing_feature_flag_of_another_accessible_team>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_project_switched_when_accessing_feature_flag_of_another_accessible_team(self):
        feature_flag = FeatureFlag.objects.create(team=self.second_team, created_by=self.user)
    
>       with self.assertNumQueries(self.base_app_num_queries + 4):

posthog/test/test_middleware.py:226: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 41 != 45 : 41 queries executed, 45 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:04.723291+00:00'::timestamptz AND "django_session"."session_key" = 'rddfyd321t9t4yso5m1pcbythck1fvvg') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /**/
E   3. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /**/
E   4. SELECT (1) AS "a" FROM "posthog_featureflag" WHERE (NOT "posthog_featureflag"."deleted" AND "posthog_featureflag"."id" = 546 AND "posthog_featureflag"."team_id" = 3220) LIMIT 1 /**/
E   5. SELECT "posthog_featureflag"."id", "posthog_featureflag"."team_id", "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_featureflag" INNER JOIN "posthog_team" ON ("posthog_featureflag"."team_id" = "posthog_team"."id") WHERE (NOT "posthog_featureflag"."deleted" AND "posthog_featureflag"."id" = 546) ORDER BY "posthog_featureflag"."id" ASC LIMIT 1 /**/
E   6. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /**/
E   7. UPDATE "posthog_user" SET "password" = 'pbkdf2_sha256$260000$IYummxPmNXPiUItPDlMbZd$yKDjn09S3O4wukm/g5wyLQ69gVIIqsLIQfEAtHEPyas=', "last_login" = '2023-07-10T13:36:04.715788+00:00'::timestamptz, "first_name" = '', "last_name" = '', "is_staff" = false, "is_active" = true, "date_joined" = '2023-07-10T13:36:04.000035+00:00'::timestamptz, "uuid" = '01894004-31a0-0000-b29c-c9f4926f9990'::uuid, "current_organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid, "current_team_id" = 3221, "email" = 'user1@posthog.com', "pending_email" = NULL, "temporary_token" = NULL, "distinct_id" = 'zhEJlgQajferJDutmaS0iE4uQtCBNF6TzyNkOCHUG16', "is_email_verified" = NULL, "has_seen_product_intro_for" = NULL, "email_opt_in" = false, "partial_notification_settings" = NULL, "anonymize_data" = false, "toolbar_mode" = 'toolbar', "events_column_config" = '{"active": "DEFAULT"}' WHERE "posthog_user"."id" = 3000 /**/
E   8. SELECT (1) AS "a" FROM "posthog_user" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   9. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:04.735828+00:00'::timestamptz AND "django_session"."session_key" = 'rddfyd321t9t4yso5m1pcbythck1fvvg') LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   10. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" IN ('constance:posthog:SLACK_APP_CLIENT_ID', 'constance:posthog:SLACK_APP_CLIENT_SECRET', 'constance:posthog:SLACK_APP_SIGNING_SECRET') /*controller='posthog.urls.home',route='%5E.%2A'*/
E   11. SELECT COUNT(*) AS "__count" FROM "posthog_user" /*controller='posthog.urls.home',route='%5E.%2A'*/
E   12. SELECT (1) AS "a" FROM "posthog_organization" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   13. SELECT (1) AS "a" FROM "posthog_organization" WHERE NOT "posthog_organization"."for_internal_metrics" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   14. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   15. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_HOST' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   16. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$pageview' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   17. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$screen' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   18. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   19. SELECT COUNT(*) FROM (SELECT DISTINCT "posthog_organizationmembership"."user_id" AS Col1 FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."organization_id" IN (SELECT U0."id" FROM "posthog_organization" U0 INNER JOIN "posthog_organizationmembership" U1 ON (U0."id" = U1."organization_id") WHERE U1."user_id" = 3000)) subquery /*controller='posthog.urls.home',route='%5E.%2A'*/
E   20. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3221 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   21. SELECT COUNT(*) AS "__count" FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   22. SELECT (1) AS "a" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organization"."available_features" @> ARRAY['project_based_permissioning']::varchar(64)[] AND "posthog_organizationmembership"."user_id" = 3000) LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   23. SELECT COUNT(*) AS "__count" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   24. SELECT (1) AS "a" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organizationmembership"."user_id" = 3000 AND "posthog_team"."completed_snippet_onboarding" AND "posthog_team"."ingested_event") LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   25. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   26. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   27. SELECT "social_auth_usersocialauth"."provider" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   28. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   29. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   30. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid /*controller='posthog.urls.home',route='%5E.%2A'*/
E   31. SELECT "posthog_team"."id", "posthog_team"."organization_id", "posthog_team"."access_control" FROM "posthog_team" WHERE "posthog_team"."organization_id" IN ('01894004-3199-0000-b6c7-2c09ea655467'::uuid) /*controller='posthog.urls.home',route='%5E.%2A'*/
E   32. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   33. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at" FROM "posthog_organizationmembership" WHERE ("posthog_organizationmembership"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid AND "posthog_organizationmembership"."user_id" = 3000) ORDER BY "posthog_organizationmembership"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   34. SELECT "otp_static_staticdevice"."id", "otp_static_staticdevice"."user_id", "otp_static_staticdevice"."name", "otp_static_staticdevice"."confirmed", "otp_static_staticdevice"."throttling_failure_timestamp", "otp_static_staticdevice"."throttling_failure_count" FROM "otp_static_staticdevice" WHERE ("otp_static_staticdevice"."user_id" = 3000 AND "otp_static_staticdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   35. SELECT "otp_totp_totpdevice"."id", "otp_totp_totpdevice"."user_id", "otp_totp_totpdevice"."name", "otp_totp_totpdevice"."confirmed", "otp_totp_totpdevice"."throttling_failure_timestamp", "otp_totp_totpdevice"."throttling_failure_count", "otp_totp_totpdevice"."key", "otp_totp_totpdevice"."step", "otp_totp_totpdevice"."t0", "otp_totp_totpdevice"."digits", "otp_totp_totpdevice"."tolerance", "otp_totp_totpdevice"."drift", "otp_totp_totpdevice"."last_t" FROM "otp_totp_totpdevice" WHERE ("otp_totp_totpdevice"."user_id" = 3000 AND "otp_totp_totpdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   36. SELECT "two_factor_phonedevice"."id", "two_factor_phonedevice"."user_id", "two_factor_phonedevice"."name", "two_factor_phonedevice"."confirmed", "two_factor_phonedevice"."throttling_failure_timestamp", "two_factor_phonedevice"."throttling_failure_count", "two_factor_phonedevice"."number", "two_factor_phonedevice"."key", "two_factor_phonedevice"."method" FROM "two_factor_phonedevice" WHERE ("two_factor_phonedevice"."user_id" = 3000 AND "two_factor_phonedevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   37. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   38. SELECT (1) AS "a" FROM "posthog_grouptypemapping" WHERE "posthog_grouptypemapping"."team_id" = 3220 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   39. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   40. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:GROUPS_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   41. SELECT "posthog_pluginconfig"."id", "posthog_pluginconfig"."config", "posthog_plugin"."config_schema", "posthog_plugin"."id", "posthog_plugin"."plugin_type", "posthog_plugin"."name" FROM "posthog_plugin" INNER JOIN "posthog_pluginconfig" ON ("posthog_plugin"."id" = "posthog_pluginconfig"."plugin_id") INNER JOIN "posthog_pluginsourcefile" ON ("posthog_plugin"."id" = "posthog_pluginsourcefile"."plugin_id") WHERE ("posthog_pluginconfig"."enabled" AND "posthog_pluginconfig"."team_id" = 3220 AND "posthog_pluginsourcefile"."filename" = 'frontend.tsx' AND "posthog_pluginsourcefile"."status" = 'TRANSPILED') /*controller='posthog.urls.home',route='%5E.%2A'*/
_ TestAutoProjectMiddleware.test_project_unchanged_when_accessing_dashboards_list _

self = <posthog.test.test_middleware.TestAutoProjectMiddleware testMethod=test_project_unchanged_when_accessing_dashboards_list>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_project_unchanged_when_accessing_dashboards_list(self):
>       with self.assertNumQueries(self.base_app_num_queries):  # No AutoProjectMiddleware queries

posthog/test/test_middleware.py:156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 37 != 41 : 37 queries executed, 41 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:05.289632+00:00'::timestamptz AND "django_session"."session_key" = '2rjs9oh28re2xlhhct3vnq8wxtooglaw') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /**/
E   3. SELECT (1) AS "a" FROM "posthog_user" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   4. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:05.293222+00:00'::timestamptz AND "django_session"."session_key" = '2rjs9oh28re2xlhhct3vnq8wxtooglaw') LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   5. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   6. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" IN ('constance:posthog:SLACK_APP_CLIENT_ID', 'constance:posthog:SLACK_APP_CLIENT_SECRET', 'constance:posthog:SLACK_APP_SIGNING_SECRET') /*controller='posthog.urls.home',route='%5E.%2A'*/
E   7. SELECT COUNT(*) AS "__count" FROM "posthog_user" /*controller='posthog.urls.home',route='%5E.%2A'*/
E   8. SELECT (1) AS "a" FROM "posthog_organization" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   9. SELECT (1) AS "a" FROM "posthog_organization" WHERE NOT "posthog_organization"."for_internal_metrics" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   10. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   11. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_HOST' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   12. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$pageview' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   13. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$screen' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   14. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   15. SELECT COUNT(*) FROM (SELECT DISTINCT "posthog_organizationmembership"."user_id" AS Col1 FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."organization_id" IN (SELECT U0."id" FROM "posthog_organization" U0 INNER JOIN "posthog_organizationmembership" U1 ON (U0."id" = U1."organization_id") WHERE U1."user_id" = 3000)) subquery /*controller='posthog.urls.home',route='%5E.%2A'*/
E   16. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   17. SELECT COUNT(*) AS "__count" FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   18. SELECT (1) AS "a" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organization"."available_features" @> ARRAY['project_based_permissioning']::varchar(64)[] AND "posthog_organizationmembership"."user_id" = 3000) LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   19. SELECT COUNT(*) AS "__count" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   20. SELECT (1) AS "a" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organizationmembership"."user_id" = 3000 AND "posthog_team"."completed_snippet_onboarding" AND "posthog_team"."ingested_event") LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   21. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   22. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   23. SELECT "social_auth_usersocialauth"."provider" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   24. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   25. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   26. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid /*controller='posthog.urls.home',route='%5E.%2A'*/
E   27. SELECT "posthog_team"."id", "posthog_team"."organization_id", "posthog_team"."access_control" FROM "posthog_team" WHERE "posthog_team"."organization_id" IN ('01894004-3199-0000-b6c7-2c09ea655467'::uuid) /*controller='posthog.urls.home',route='%5E.%2A'*/
E   28. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   29. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at" FROM "posthog_organizationmembership" WHERE ("posthog_organizationmembership"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid AND "posthog_organizationmembership"."user_id" = 3000) ORDER BY "posthog_organizationmembership"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   30. SELECT "otp_static_staticdevice"."id", "otp_static_staticdevice"."user_id", "otp_static_staticdevice"."name", "otp_static_staticdevice"."confirmed", "otp_static_staticdevice"."throttling_failure_timestamp", "otp_static_staticdevice"."throttling_failure_count" FROM "otp_static_staticdevice" WHERE ("otp_static_staticdevice"."user_id" = 3000 AND "otp_static_staticdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   31. SELECT "otp_totp_totpdevice"."id", "otp_totp_totpdevice"."user_id", "otp_totp_totpdevice"."name", "otp_totp_totpdevice"."confirmed", "otp_totp_totpdevice"."throttling_failure_timestamp", "otp_totp_totpdevice"."throttling_failure_count", "otp_totp_totpdevice"."key", "otp_totp_totpdevice"."step", "otp_totp_totpdevice"."t0", "otp_totp_totpdevice"."digits", "otp_totp_totpdevice"."tolerance", "otp_totp_totpdevice"."drift", "otp_totp_totpdevice"."last_t" FROM "otp_totp_totpdevice" WHERE ("otp_totp_totpdevice"."user_id" = 3000 AND "otp_totp_totpdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   32. SELECT "two_factor_phonedevice"."id", "two_factor_phonedevice"."user_id", "two_factor_phonedevice"."name", "two_factor_phonedevice"."confirmed", "two_factor_phonedevice"."throttling_failure_timestamp", "two_factor_phonedevice"."throttling_failure_count", "two_factor_phonedevice"."number", "two_factor_phonedevice"."key", "two_factor_phonedevice"."method" FROM "two_factor_phonedevice" WHERE ("two_factor_phonedevice"."user_id" = 3000 AND "two_factor_phonedevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   33. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   34. SELECT (1) AS "a" FROM "posthog_grouptypemapping" WHERE "posthog_grouptypemapping"."team_id" = 3220 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   35. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   36. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:GROUPS_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   37. SELECT "posthog_pluginconfig"."id", "posthog_pluginconfig"."config", "posthog_plugin"."config_schema", "posthog_plugin"."id", "posthog_plugin"."plugin_type", "posthog_plugin"."name" FROM "posthog_plugin" INNER JOIN "posthog_pluginconfig" ON ("posthog_plugin"."id" = "posthog_pluginconfig"."plugin_id") INNER JOIN "posthog_pluginsourcefile" ON ("posthog_plugin"."id" = "posthog_pluginsourcefile"."plugin_id") WHERE ("posthog_pluginconfig"."enabled" AND "posthog_pluginconfig"."team_id" = 3220 AND "posthog_pluginsourcefile"."filename" = 'frontend.tsx' AND "posthog_pluginsourcefile"."status" = 'TRANSPILED') /*controller='posthog.urls.home',route='%5E.%2A'*/
_ TestAutoProjectMiddleware.test_project_unchanged_when_creating_feature_flag __

self = <posthog.test.test_middleware.TestAutoProjectMiddleware testMethod=test_project_unchanged_when_creating_feature_flag>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_project_unchanged_when_creating_feature_flag(self):
>       with self.assertNumQueries(self.base_app_num_queries):

posthog/test/test_middleware.py:240: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 37 != 41 : 37 queries executed, 41 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:05.394446+00:00'::timestamptz AND "django_session"."session_key" = 'j85h7ifnia1gtgefcji43ypjsg698tho') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /**/
E   3. SELECT (1) AS "a" FROM "posthog_user" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   4. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-10T13:36:05.398106+00:00'::timestamptz AND "django_session"."session_key" = 'j85h7ifnia1gtgefcji43ypjsg698tho') LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   5. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   6. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" IN ('constance:posthog:SLACK_APP_CLIENT_ID', 'constance:posthog:SLACK_APP_CLIENT_SECRET', 'constance:posthog:SLACK_APP_SIGNING_SECRET') /*controller='posthog.urls.home',route='%5E.%2A'*/
E   7. SELECT COUNT(*) AS "__count" FROM "posthog_user" /*controller='posthog.urls.home',route='%5E.%2A'*/
E   8. SELECT (1) AS "a" FROM "posthog_organization" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   9. SELECT (1) AS "a" FROM "posthog_organization" WHERE NOT "posthog_organization"."for_internal_metrics" LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   10. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   11. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:EMAIL_HOST' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   12. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$pageview' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   13. SELECT (1) AS "a" FROM "posthog_eventdefinition" WHERE "posthog_eventdefinition"."name" = '$screen' LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   14. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 3000 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   15. SELECT COUNT(*) FROM (SELECT DISTINCT "posthog_organizationmembership"."user_id" AS Col1 FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."organization_id" IN (SELECT U0."id" FROM "posthog_organization" U0 INNER JOIN "posthog_organizationmembership" U1 ON (U0."id" = U1."organization_id") WHERE U1."user_id" = 3000)) subquery /*controller='posthog.urls.home',route='%5E.%2A'*/
E   16. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days", "posthog_team"."plugins_opt_in", "posthog_team"."opt_out_capture", "posthog_team"."event_names", "posthog_team"."event_names_with_usage", "posthog_team"."event_properties", "posthog_team"."event_properties_with_usage", "posthog_team"."event_properties_numerical" FROM "posthog_team" WHERE "posthog_team"."id" = 3220 LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   17. SELECT COUNT(*) AS "__count" FROM "posthog_organizationmembership" WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   18. SELECT (1) AS "a" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organization"."available_features" @> ARRAY['project_based_permissioning']::varchar(64)[] AND "posthog_organizationmembership"."user_id" = 3000) LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   19. SELECT COUNT(*) AS "__count" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   20. SELECT (1) AS "a" FROM "posthog_team" INNER JOIN "posthog_organization" ON ("posthog_team"."organization_id" = "posthog_organization"."id") INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE ("posthog_organizationmembership"."user_id" = 3000 AND "posthog_team"."completed_snippet_onboarding" AND "posthog_team"."ingested_event") LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   21. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   22. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   23. SELECT "social_auth_usersocialauth"."provider" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   24. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" WHERE "posthog_organization"."id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid LIMIT 21 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   25. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   26. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid /*controller='posthog.urls.home',route='%5E.%2A'*/
E   27. SELECT "posthog_team"."id", "posthog_team"."organization_id", "posthog_team"."access_control" FROM "posthog_team" WHERE "posthog_team"."organization_id" IN ('01894004-3199-0000-b6c7-2c09ea655467'::uuid) /*controller='posthog.urls.home',route='%5E.%2A'*/
E   28. SELECT "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organization" INNER JOIN "posthog_organizationmembership" ON ("posthog_organization"."id" = "posthog_organizationmembership"."organization_id") WHERE "posthog_organizationmembership"."user_id" = 3000 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   29. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at" FROM "posthog_organizationmembership" WHERE ("posthog_organizationmembership"."organization_id" = '01894004-3199-0000-b6c7-2c09ea655467'::uuid AND "posthog_organizationmembership"."user_id" = 3000) ORDER BY "posthog_organizationmembership"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   30. SELECT "otp_static_staticdevice"."id", "otp_static_staticdevice"."user_id", "otp_static_staticdevice"."name", "otp_static_staticdevice"."confirmed", "otp_static_staticdevice"."throttling_failure_timestamp", "otp_static_staticdevice"."throttling_failure_count" FROM "otp_static_staticdevice" WHERE ("otp_static_staticdevice"."user_id" = 3000 AND "otp_static_staticdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   31. SELECT "otp_totp_totpdevice"."id", "otp_totp_totpdevice"."user_id", "otp_totp_totpdevice"."name", "otp_totp_totpdevice"."confirmed", "otp_totp_totpdevice"."throttling_failure_timestamp", "otp_totp_totpdevice"."throttling_failure_count", "otp_totp_totpdevice"."key", "otp_totp_totpdevice"."step", "otp_totp_totpdevice"."t0", "otp_totp_totpdevice"."digits", "otp_totp_totpdevice"."tolerance", "otp_totp_totpdevice"."drift", "otp_totp_totpdevice"."last_t" FROM "otp_totp_totpdevice" WHERE ("otp_totp_totpdevice"."user_id" = 3000 AND "otp_totp_totpdevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   32. SELECT "two_factor_phonedevice"."id", "two_factor_phonedevice"."user_id", "two_factor_phonedevice"."name", "two_factor_phonedevice"."confirmed", "two_factor_phonedevice"."throttling_failure_timestamp", "two_factor_phonedevice"."throttling_failure_count", "two_factor_phonedevice"."number", "two_factor_phonedevice"."key", "two_factor_phonedevice"."method" FROM "two_factor_phonedevice" WHERE ("two_factor_phonedevice"."user_id" = 3000 AND "two_factor_phonedevice"."confirmed") /*controller='posthog.urls.home',route='%5E.%2A'*/
E   33. SELECT (1) AS "a" FROM "social_auth_usersocialauth" WHERE "social_auth_usersocialauth"."user_id" = 3000 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   34. SELECT (1) AS "a" FROM "posthog_grouptypemapping" WHERE "posthog_grouptypemapping"."team_id" = 3220 LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   35. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   36. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:GROUPS_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='posthog.urls.home',route='%5E.%2A'*/
E   37. SELECT "posthog_pluginconfig"."id", "posthog_pluginconfig"."config", "posthog_plugin"."config_schema", "posthog_plugin"."id", "posthog_plugin"."plugin_type", "posthog_plugin"."name" FROM "posthog_plugin" INNER JOIN "posthog_pluginconfig" ON ("posthog_plugin"."id" = "posthog_pluginconfig"."plugin_id") INNER JOIN "posthog_pluginsourcefile" ON ("posthog_plugin"."id" = "posthog_pluginsourcefile"."plugin_id") WHERE ("posthog_pluginconfig"."enabled" AND "posthog_pluginconfig"."team_id" = 3220 AND "posthog_pluginsourcefile"."filename" = 'frontend.tsx' AND "posthog_pluginsourcefile"."status" = 'TRANSPILED') /*controller='posthog.urls.home',route='%5E.%2A'*/
____________ TestPostHogTokenCookieMiddleware.test_logged_in_client ____________

self = <posthog.test.test_middleware.TestPostHogTokenCookieMiddleware testMethod=test_logged_in_client>

    def test_logged_in_client(self):
        self.client.force_login(self.user)
        response = self.client.get("/")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
>       ph_project_token_cookie = response.cookies["ph_current_project_token"]
E       KeyError: 'ph_current_project_token'

posthog/test/test_middleware.py:265: KeyError
_________________ TestPostHogTokenCookieMiddleware.test_logout _________________

self = <posthog.test.test_middleware.TestPostHogTokenCookieMiddleware testMethod=test_logout>

    def test_logout(self):
        self.client.force_login(self.user)
        response = self.client.get("/")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
>       self.assertEqual(response.cookies["ph_current_project_token"].key, "ph_current_project_token")
E       KeyError: 'ph_current_project_token'

posthog/test/test_middleware.py:350: KeyError
_ TestTeam.test_team_on_cloud_uses_feature_flag_to_determine_person_on_events __

self = <posthog.test.test_team.TestTeam testMethod=test_team_on_cloud_uses_feature_flag_to_determine_person_on_events>
mock_feature_enabled = <MagicMock name='feature_enabled' id='140137373674288'>

    @mock.patch("posthoganalytics.feature_enabled", return_value=True)
    def test_team_on_cloud_uses_feature_flag_to_determine_person_on_events(self, mock_feature_enabled):
        with self.is_cloud(True):
            with override_instance_config("PERSON_ON_EVENTS_ENABLED", False):
                team = Team.objects.create_with_data(organization=self.organization)
>               self.assertEqual(team.person_on_events_mode, PersonOnEventsMode.V2_ENABLED)
E               AssertionError: <PersonOnEventsMode.DISABLED: 'disabled'> != <PersonOnEventsMode.V2_ENABLED: 'v2_enabled'>

posthog/test/test_team.py:124: AssertionError
_________ TestResiliency.test_feature_flags_v3_with_a_working_slow_db __________

self = <django.db.backends.utils.CursorWrapper object at 0x7f74468f0310>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$Vfh9OQuNDXOKZ0zEODE0TL$bhUpnjy3i0N3invTazRCuWtK6CSeV/sQjcm4fLjxMJ8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74468f0310>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_a_working_slow_db>
mock_postgres_check = <MagicMock name='is_connected' id='140137379997120'>

    
    def test_feature_flags_v3_with_a_working_slow_db(self, mock_postgres_check):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:3048: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74468f0310>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$Vfh9OQuNDXOKZ0zEODE0TL$bhUpnjy3i0N3invTazRCuWtK6CSeV/sQjcm4fLjxMJ8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74468f0310>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestResiliency.test_feature_flags_v3_with_experience_continuity_working_slow_db _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447074700>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$bINc8lddrA1m7oCFianWqq$X1/doBXxChh/bJ1D3ZwzQPKGLPJbS5uQM/M2ZkI1NHU=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447074700>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random12@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_experience_continuity_working_slow_db>
mock_counter = <MagicMock name='FLAG_EVALUATION_ERROR_COUNTER' id='140137389256384'>
args = (<MagicMock name='is_connected' id='140137389247456'>,)

    
    @patch("posthog.models.feature_flag.flag_matching.FLAG_EVALUATION_ERROR_COUNTER")
    def test_feature_flags_v3_with_experience_continuity_working_slow_db(self, mock_counter, *args):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:3321: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447074700>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$bINc8lddrA1m7oCFianWqq$X1/doBXxChh/bJ1D3ZwzQPKGLPJbS5uQM/M2ZkI1NHU=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447074700>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random12@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
__________ TestResiliency.test_feature_flags_v3_with_group_properties __________

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2800>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$PPR8xPL2GWRYpd3Z3yE3cM$6NRi3b/MfYooJM2nX4K5tANI+of4sbjEjhCsjRf5gV8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74470c2800>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_group_properties>
args = (<MagicMock name='is_connected' id='140137384913520'>,)

    
    def test_feature_flags_v3_with_group_properties(self, *args):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:2867: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74470c2800>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$PPR8xPL2GWRYpd3Z3yE3cM$6NRi3b/MfYooJM2nX4K5tANI+of4sbjEjhCsjRf5gV8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74470c2800>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
____ TestResiliency.test_feature_flags_v3_with_group_properties_and_slow_db ____

self = <django.db.backends.utils.CursorWrapper object at 0x7f74466d1510>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$rQVOpAp0hYZDix4eoXyAtv$hfjo7YN/m8Ag0/GEAS3vDAiN3JqlHhde4NdzHkSA9I8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74466d1510>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(randomXYZ@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_group_properties_and_slow_db>
mock_counter = <MagicMock name='FLAG_EVALUATION_ERROR_COUNTER' id='140139015637648'>
args = (<MagicMock name='is_connected' id='140137374496704'>,)

    
    @patch("posthog.models.feature_flag.flag_matching.FLAG_EVALUATION_ERROR_COUNTER")
    def test_feature_flags_v3_with_group_properties_and_slow_db(self, mock_counter, *args):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:3214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74466d1510>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$rQVOpAp0hYZDix4eoXyAtv$hfjo7YN/m8Ag0/GEAS3vDAiN3JqlHhde4NdzHkSA9I8=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74466d1510>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(randomXYZ@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_________ TestResiliency.test_feature_flags_v3_with_person_properties __________

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e68c0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$tvi8MU7m6GJYGSM449k6LO$jU53vULAIkFZ8swI4m2dU57aZGfU3t9ZbR/MMkQSN0k=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844e68c0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_person_properties>
mock_counter = <MagicMock name='FLAG_EVALUATION_ERROR_COUNTER' id='140137377083216'>
args = (<MagicMock name='is_connected' id='140137377086720'>,)

    
    @patch("posthog.models.feature_flag.flag_matching.FLAG_EVALUATION_ERROR_COUNTER")
    def test_feature_flags_v3_with_person_properties(self, mock_counter, *args):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:2963: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844e68c0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$tvi8MU7m6GJYGSM449k6LO$jU53vULAIkFZ8swI4m2dU57aZGfU3t9ZbR/MMkQSN0k=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844e68c0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestResiliency.test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6da8160>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$k0m4tI64hkgI5CUkfxIbed$A4ABwX6iZFaK4gwMYdVZjLv0cl9hhYmvvaqEvn8tV5k=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6da8160>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.api.test.test_feature_flag.TestResiliency testMethod=test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again>
mock_counter = <MagicMock name='FLAG_EVALUATION_ERROR_COUNTER' id='140137380447088'>
args = (<MagicMock name='is_connected' id='140137383892528'>,)

    
    @patch("posthog.models.feature_flag.flag_matching.FLAG_EVALUATION_ERROR_COUNTER")
    def test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again(self, mock_counter, *args):
        self.organization = Organization.objects.create(name="test")
>       self.team = Team.objects.create(organization=self.organization)

posthog/api/test/test_feature_flag.py:3131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f6da8160>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$k0m4tI64hkgI5CUkfxIbed$A4ABwX6iZFaK4gwMYdVZjLv0cl9hhYmvvaqEvn8tV5k=', None, 'first_name', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f6da8160>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(random@test.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestPersonOverride.test_person_override_allows_duplicate_override_person_id __

self = <unittest.case._Outcome object at 0x7f75085bf850>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_duplicate_override_person_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_duplicate_override_person_id>
method = <bound method TestPersonOverride.test_person_override_allows_duplicate_override_person_id of <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_duplicate_override_person_id>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_allows_duplicate_override_person_id() missing 1 required positional argument: 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverride.test_person_override_allows_override_person_id_as_old_person_id_in_different_teams _

self = <unittest.case._Outcome object at 0x7f7444a392d0>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_override_person_id_as_old_person_id_in_different_teams>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_allows_override_person_id_as_old_person_id_in_different_teams>
method = <bound method TestPersonOverride.test_person_override_allows_override_person_id_as_old_person_id_in_different_teams of...odel.TestPersonOverride testMethod=test_person_override_allows_override_person_id_as_old_person_id_in_different_teams>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_allows_override_person_id_as_old_person_id_in_different_teams() missing 2 required positional arguments: 'organization' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverride.test_person_override_disallows_old_person_id_as_override_person_id _

self = <unittest.case._Outcome object at 0x7f75085bf100>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_old_person_id_as_override_person_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_old_person_id_as_override_person_id>
method = <bound method TestPersonOverride.test_person_override_disallows_old_person_id_as_override_person_id of <posthog.models...erson_override_model.TestPersonOverride testMethod=test_person_override_disallows_old_person_id_as_override_person_id>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_disallows_old_person_id_as_override_person_id() missing 1 required positional argument: 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverride.test_person_override_disallows_override_person_id_as_old_person_id _

self = <unittest.case._Outcome object at 0x7f74f6d87e20>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_override_person_id_as_old_person_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_override_person_id_as_old_person_id>
method = <bound method TestPersonOverride.test_person_override_disallows_override_person_id_as_old_person_id of <posthog.models...erson_override_model.TestPersonOverride testMethod=test_person_override_disallows_override_person_id_as_old_person_id>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_disallows_override_person_id_as_old_person_id() missing 1 required positional argument: 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_____ TestPersonOverride.test_person_override_disallows_same_old_person_id _____

self = <unittest.case._Outcome object at 0x7f74462e0d30>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_same_old_person_id>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_same_old_person_id>
method = <bound method TestPersonOverride.test_person_override_disallows_same_old_person_id of <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_disallows_same_old_person_id>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_disallows_same_old_person_id() missing 1 required positional argument: 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverride.test_person_override_old_person_id_as_override_person_id_in_different_teams _

self = <unittest.case._Outcome object at 0x7f75084e1a50>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_old_person_id_as_override_person_id_in_different_teams>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_old_person_id_as_override_person_id_in_different_teams>
method = <bound method TestPersonOverride.test_person_override_old_person_id_as_override_person_id_in_different_teams of <posth...rride_model.TestPersonOverride testMethod=test_person_override_old_person_id_as_override_person_id_in_different_teams>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_old_person_id_as_override_person_id_in_different_teams() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverride.test_person_override_same_old_person_id_in_different_teams _

self = <unittest.case._Outcome object at 0x7f7444aced70>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_same_old_person_id_in_different_teams>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_same_old_person_id_in_different_teams>
method = <bound method TestPersonOverride.test_person_override_same_old_person_id_in_different_teams of <posthog.models.test.test_person_override_model.TestPersonOverride testMethod=test_person_override_same_old_person_id_in_different_teams>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverride.test_person_override_same_old_person_id_in_different_teams() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverrideConcurrency.test_person_override_allow_consecutive_merges __

self = <unittest.case._Outcome object at 0x7f74f6d86c50>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_allow_consecutive_merges>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_allow_consecutive_merges>
method = <bound method TestPersonOverrideConcurrency.test_person_override_allow_consecutive_merges of <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_allow_consecutive_merges>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverrideConcurrency.test_person_override_allow_consecutive_merges() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge _

self = <unittest.case._Outcome object at 0x7f750889b910>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge>
method = <bound method TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge of <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
_ TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge_different_order _

self = <unittest.case._Outcome object at 0x7f7446b94670>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge_different_order>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge_different_order>
method = <bound method TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge_different_order of <postho...erride_model.TestPersonOverrideConcurrency testMethod=test_person_override_disallows_concurrent_merge_different_order>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverrideConcurrency.test_person_override_disallows_concurrent_merge_different_order() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
___________ TestPersonOverrideConcurrency.test_person_override_merge ___________

self = <unittest.case._Outcome object at 0x7f74f6d2a7a0>
test_case = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_merge>
isTest = True

    @contextlib.contextmanager
    def testPartExecutor(self, test_case, isTest=False):
        old_success = self.success
        self.success = True
        try:
>           yield

/usr/local/lib/python3.10/unittest/case.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/local/lib/python3.10/unittest/case.py:591: in run
    self._callTestMethod(testMethod)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_merge>
method = <bound method TestPersonOverrideConcurrency.test_person_override_merge of <posthog.models.test.test_person_override_model.TestPersonOverrideConcurrency testMethod=test_person_override_merge>>

    def _callTestMethod(self, method):
>       method()
E       TypeError: TestPersonOverrideConcurrency.test_person_override_merge() missing 2 required positional arguments: 'team' and 'oldest_event'

/usr/local/lib/python3.10/unittest/case.py:549: TypeError
____________________ RunUpdatesTest.test_create_export_run _____________________
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <asgiref.sync.AsyncToSync object at 0x7f7508654850>, args = ()
kwargs = {}
call_result = <Future at 0x7f7508657730 state=finished raised TypeError>
source_thread = <_MainThread(MainThread, started 140141459998528)>
exc_info = (None, None, None)
context = [<_contextvars.Context object at 0x7f75086213c0>]

    async def main_wrap(
        self, args, kwargs, call_result, source_thread, exc_info, context
    ):
        """
        Wraps the awaitable with something that puts the result into the
        result/exception future.
        """
        if context is not None:
            _restore_context(context[0])
    
        current_task = SyncToAsync.get_current_task()
        self.launch_map[current_task] = source_thread
        try:
            # If we have an exception, run the function inside the except block
            # after raising it so exc_info is correctly populated.
            if exc_info[1]:
                try:
                    raise exc_info[1]
                except BaseException:
                    result = await self.awaitable(*args, **kwargs)
            else:
>               result = await self.awaitable(*args, **kwargs)
E               TypeError: RunUpdatesTest.test_create_export_run() missing 2 required positional arguments: 'team' and 'batch_export'

env/lib/python3.10/site-packages/asgiref/sync.py:268: TypeError
_________________ RunUpdatesTest.test_update_export_run_status _________________
env/lib/python3.10/site-packages/asgiref/sync.py:203: in __call__
    return call_result.result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:451: in result
    return self.__get_result()
/usr/local/lib/python3.10/concurrent/futures/_base.py:403: in __get_result
    raise self._exception
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <asgiref.sync.AsyncToSync object at 0x7f7444a25900>, args = ()
kwargs = {}
call_result = <Future at 0x7f7444a24370 state=finished raised TypeError>
source_thread = <_MainThread(MainThread, started 140141459998528)>
exc_info = (None, None, None)
context = [<_contextvars.Context object at 0x7f7508a9b0c0>]

    async def main_wrap(
        self, args, kwargs, call_result, source_thread, exc_info, context
    ):
        """
        Wraps the awaitable with something that puts the result into the
        result/exception future.
        """
        if context is not None:
            _restore_context(context[0])
    
        current_task = SyncToAsync.get_current_task()
        self.launch_map[current_task] = source_thread
        try:
            # If we have an exception, run the function inside the except block
            # after raising it so exc_info is correctly populated.
            if exc_info[1]:
                try:
                    raise exc_info[1]
                except BaseException:
                    result = await self.awaitable(*args, **kwargs)
            else:
>               result = await self.awaitable(*args, **kwargs)
E               TypeError: RunUpdatesTest.test_update_export_run_status() missing 2 required positional arguments: 'team' and 'batch_export'

env/lib/python3.10/site-packages/asgiref/sync.py:268: TypeError
_ TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_race_conditions _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844d54b0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$ypHRrgEIy0GjQF7710nvbT$fZFQZsHVEm75L35EREweaINGbZiTHliy0VabGRQjI/U=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844d54b0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_race_conditions>

    def test_hash_key_overrides_with_race_conditions(self):
        org = Organization.objects.create(name="test")
>       user = User.objects.create_and_join(org, "a@b.com", "kkk")

posthog/test/test_feature_flag.py:2797: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74844d54b0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$ypHRrgEIy0GjQF7710nvbT$fZFQZsHVEm75L35EREweaINGbZiTHliy0VabGRQjI/U=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74844d54b0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447021de0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$VNqxBLAmCbdBOphjEmYaUI$pGvJPh1q9Y2hGHw2Ha7/o4ZWr2unDrTvKGdUlr5KKho=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447021de0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion>

    def test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion(self):
        org = Organization.objects.create(name="test")
>       user = User.objects.create_and_join(org, "a@b.com", "kkk")

posthog/test/test_feature_flag.py:3006: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447021de0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$VNqxBLAmCbdBOphjEmYaUI$pGvJPh1q9Y2hGHw2Ha7/o4ZWr2unDrTvKGdUlr5KKho=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447021de0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447769ed0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$4GZgMfLwAlpQHDkARTqulB$mpGQPxrTRxHTZ93lPhaZy2PpvbmVaUbsvHpi3RT/8Bc=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447769ed0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging>

    def test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging(self):
        def insert_fail(execute, sql, *args, **kwargs):
            if "statement_timeout" in sql:
                return execute(sql, *args, **kwargs)
            if "insert" in sql.lower():
                # run the sql so it shows up in snapshots
                execute(sql, *args, **kwargs)
    
                raise IntegrityError(
                    """
                    insert or update on table "posthog_featureflaghashkeyoverride" violates foreign key constraint "posthog_featureflagh_person_id_7e517f7c_fk_posthog_p"
                    DETAIL:  Key (person_id)=(1487010281) is not present in table "posthog_person".
                """
                )
            return execute(sql, *args, **kwargs)
    
        org = Organization.objects.create(name="test")
>       user = User.objects.create_and_join(org, "a@b.com", "kkk")

posthog/test/test_feature_flag.py:2870: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7447769ed0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$4GZgMfLwAlpQHDkARTqulB$mpGQPxrTRxHTZ93lPhaZy2PpvbmVaUbsvHpi3RT/8Bc=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7447769ed0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ TestHashKeyOverridesRaceConditions.test_hash_key_overrides_with_simulated_race_conditions_on_person_merging _

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f692d6c0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$VzvJnEXQqqp9e1e5ZGvM1y$MDYVslFthJgMq8WGDrHDmp9GwI6Di8zSC/NPOadY85M=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f692d6c0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: UniqueViolation

The above exception was the direct cause of the following exception:

self = <posthog.test.test_feature_flag.TestHashKeyOverridesRaceConditions testMethod=test_hash_key_overrides_with_simulated_race_conditions_on_person_merging>

    def test_hash_key_overrides_with_simulated_race_conditions_on_person_merging(self):
        class InsertFailOnce:
            def __init__(self):
                self.has_failed = False
    
            def __call__(self, execute, sql, *args, **kwargs):
                if "statement_timeout" in sql:
                    return execute(sql, *args, **kwargs)
                if "insert" in sql.lower() and not self.has_failed:
                    self.has_failed = True
                    # run the sql so it shows up in snapshots
                    execute(sql, *args, **kwargs)
                    # then raise an error
                    raise IntegrityError(
                        """
                        insert or update on table "posthog_featureflaghashkeyoverride" violates foreign key constraint "posthog_featureflagh_person_id_7e517f7c_fk_posthog_p"
                        DETAIL:  Key (person_id)=(1487010281) is not present in table "posthog_person".
                    """
                    )
                return execute(sql, *args, **kwargs)
    
        org = Organization.objects.create(name="test")
>       user = User.objects.create_and_join(org, "a@b.com", "kkk")

posthog/test/test_feature_flag.py:2947: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/models/user.py:99: in create_and_join
    user = self.create_user(email=email, password=password, first_name=first_name, **extra_fields)
posthog/models/user.py:59: in create_user
    user.save()
env/lib/python3.10/site-packages/django/contrib/auth/base_user.py:67: in save
    super().save(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/base.py:739: in save
    self.save_base(using=using, force_insert=force_insert,
env/lib/python3.10/site-packages/django/db/models/base.py:776: in save_base
    updated = self._save_table(
env/lib/python3.10/site-packages/django/db/models/base.py:881: in _save_table
    results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
env/lib/python3.10/site-packages/django/db/models/base.py:919: in _do_insert
    return manager._insert(
env/lib/python3.10/site-packages/django/db/models/manager.py:85: in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
env/lib/python3.10/site-packages/django/db/models/query.py:1270: in _insert
    return query.get_compiler(using=using).execute_sql(returning_fields)
env/lib/python3.10/site-packages/django/db/models/sql/compiler.py:1416: in execute_sql
    cursor.execute(sql, params)
env/lib/python3.10/site-packages/django/db/backends/utils.py:66: in execute
    return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)
env/lib/python3.10/site-packages/django/db/backends/utils.py:75: in _execute_with_wrappers
    return executor(sql, params, many, context)
env/lib/python3.10/site-packages/django/db/backends/utils.py:79: in _execute
    with self.db.wrap_database_errors:
env/lib/python3.10/site-packages/django/db/utils.py:90: in __exit__
    raise dj_exc_value.with_traceback(traceback) from exc_value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f74f692d6c0>
sql = 'INSERT INTO "posthog_user" ("password", "last_login", "first_name", "last_name", "is_staff", "is_active", "date_joine...(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING "posthog_user"."id"'
params = ('pbkdf2_sha256$260000$VzvJnEXQqqp9e1e5ZGvM1y$MDYVslFthJgMq8WGDrHDmp9GwI6Di8zSC/NPOadY85M=', None, '', '', False, True, ...)
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f74f692d6c0>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
                return self.cursor.execute(sql)
            else:
>               return self.cursor.execute(sql, params)
E               django.db.utils.IntegrityError: duplicate key value violates unique constraint "posthog_user_email_af269794_uniq"
E               DETAIL:  Key (email)=(a@b.com) already exists.

env/lib/python3.10/site-packages/django/db/backends/utils.py:84: IntegrityError
_ CreatingSessionRecordingModelMigrationTestCase.test_migrate_to_create_session_recordings _

self = <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               psycopg2.errors.FeatureNotSupported: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: FeatureNotSupported

The above exception was the direct cause of the following exception:

self = <django.core.management.commands.flush.Command object at 0x7f74a8180070>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
>                   connection.ops.execute_sql_flush(sql_list)

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.postgresql.operations.DatabaseOperations object at 0x7f75355e5270>
sql_list = ['TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinv...loadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";']

    def execute_sql_flush(self, sql_list):
        """Execute a list of SQL statements to flush the database."""
        with transaction.atomic(
            using=self.connection.alias,
            savepoint=self.connection.features.can_rollback_ddl,
        ):
            with self.connection.cursor() as cursor:
                for sql in sql_list:
>                   cursor.execute(sql)

env/lib/python3.10/site-packages/django/db/backends/base/operations.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None

    def execute(self, sql, params=None):
>       return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)

env/lib/python3.10/site-packages/django/db/backends/utils.py:66: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None, many = False
executor = <bound method CursorWrapper._execute of <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>>

    def _execute_with_wrappers(self, sql, params, many, executor):
        context = {'connection': self.db, 'cursor': self}
        for wrapper in reversed(self.db.execute_wrappers):
            executor = functools.partial(wrapper, executor)
>       return executor(sql, params, many, context)

env/lib/python3.10/site-packages/django/db/backends/utils.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
>       with self.db.wrap_database_errors:

env/lib/python3.10/site-packages/django/db/backends/utils.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.utils.DatabaseErrorWrapper object at 0x7f7508fa48b0>
exc_type = <class 'psycopg2.errors.FeatureNotSupported'>
exc_value = FeatureNotSupported('cannot truncate a table referenced in a foreign key constraint\nDETAIL:  Table "ee_explicitteamme...ationmembership".\nHINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.\n')
traceback = <traceback object at 0x7f740f4aef00>

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type is None:
            return
        for dj_exc_type in (
                DataError,
                OperationalError,
                IntegrityError,
                InternalError,
                ProgrammingError,
                NotSupportedError,
                DatabaseError,
                InterfaceError,
                Error,
        ):
            db_exc_type = getattr(self.wrapper.Database, dj_exc_type.__name__)
            if issubclass(exc_type, db_exc_type):
                dj_exc_value = dj_exc_type(*exc_value.args)
                # Only set the 'errors_occurred' flag for errors that may make
                # the connection unusable.
                if dj_exc_type not in (DataError, IntegrityError):
                    self.wrapper.errors_occurred = True
>               raise dj_exc_value.with_traceback(traceback) from exc_value

env/lib/python3.10/site-packages/django/db/utils.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>
sql = 'TRUNCATE "posthog_personalapikey", "posthog_sessionrecordingplaylistitem", "posthog_event", "posthog_organizationinvi...ploadedmedia", "posthog_promptsequence_must_have_completed", "posthog_sessionrecordingevent", "posthog_asyncdeletion";'
params = None
ignored_wrapper_args = (False, {'connection': <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>, 'cursor': <django.db.backends.utils.CursorWrapper object at 0x7f7484617f70>})

    def _execute(self, sql, params, *ignored_wrapper_args):
        self.db.validate_no_broken_transaction()
        with self.db.wrap_database_errors:
            if params is None:
                # params default might be backend specific.
>               return self.cursor.execute(sql)
E               django.db.utils.NotSupportedError: cannot truncate a table referenced in a foreign key constraint
E               DETAIL:  Table "ee_explicitteammembership" references "posthog_organizationmembership".
E               HINT:  Truncate table "ee_explicitteammembership" at the same time, or use TRUNCATE ... CASCADE.

env/lib/python3.10/site-packages/django/db/backends/utils.py:82: NotSupportedError

The above exception was the direct cause of the following exception:

self = <posthog.test.test_migration_0287.CreatingSessionRecordingModelMigrationTestCase testMethod=test_migrate_to_create_session_recordings>
result = <TestCaseFunction test_migrate_to_create_session_recordings>
debug = False

    def _setup_and_call(self, result, debug=False):
        """
        Perform the following in order: pre-setup, run test, post-teardown,
        skipping pre/post hooks if test is set to be skipped.
    
        If debug=True, reraise any errors in setup and use super().debug()
        instead of __call__() to run the test.
        """
        testMethod = getattr(self, self._testMethodName)
        skipped = (
            getattr(self.__class__, "__unittest_skip__", False) or
            getattr(testMethod, "__unittest_skip__", False)
        )
    
        # Convert async test methods.
        if asyncio.iscoroutinefunction(testMethod):
            setattr(self, self._testMethodName, async_to_sync(testMethod))
    
        if not skipped:
            try:
                self._pre_setup()
            except Exception:
                if debug:
                    raise
                result.addError(self, sys.exc_info())
                return
        if debug:
            super().debug()
        else:
            super().__call__(result)
        if not skipped:
            try:
>               self._post_teardown()

env/lib/python3.10/site-packages/django/test/testcases.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:1006: in _post_teardown
    self._fixture_teardown()
env/lib/python3.10/site-packages/django/test/testcases.py:1038: in _fixture_teardown
    call_command('flush', verbosity=0, interactive=False,
env/lib/python3.10/site-packages/django/core/management/__init__.py:181: in call_command
    return command.execute(*args, **defaults)
env/lib/python3.10/site-packages/django/core/management/base.py:398: in execute
    output = self.handle(*args, **options)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <django.core.management.commands.flush.Command object at 0x7f74a8180070>
options = {'allow_cascade': False, 'database': 'default', 'force_color': False, 'inhibit_post_migrate': False, ...}
database = 'default'
connection = <django.db.backends.postgresql.base.DatabaseWrapper object at 0x7f75357340a0>
verbosity = 0, interactive = False, reset_sequences = False
allow_cascade = False, inhibit_post_migrate = False

        def handle(self, **options):
            database = options['database']
            connection = connections[database]
            verbosity = options['verbosity']
            interactive = options['interactive']
            # The following are stealth options used by Django's internals.
            reset_sequences = options.get('reset_sequences', True)
            allow_cascade = options.get('allow_cascade', False)
            inhibit_post_migrate = options.get('inhibit_post_migrate', False)
    
            self.style = no_style()
    
            # Import the 'management' module within each installed app, to register
            # dispatcher events.
            for app_config in apps.get_app_configs():
                try:
                    import_module('.management', app_config.name)
                except ImportError:
                    pass
    
            sql_list = sql_flush(self.style, connection,
                                 reset_sequences=reset_sequences,
                                 allow_cascade=allow_cascade)
    
            if interactive:
                confirm = input("""You have requested a flush of the database.
    This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
    and return each table to an empty state.
    Are you sure you want to do this?
    
        Type 'yes' to continue, or 'no' to cancel: """ % connection.settings_dict['NAME'])
            else:
                confirm = 'yes'
    
            if confirm == 'yes':
                try:
                    connection.ops.execute_sql_flush(sql_list)
                except Exception as exc:
>                   raise CommandError(
                        "Database %s couldn't be flushed. Possible reasons:\n"
                        "  * The database isn't running or isn't configured correctly.\n"
                        "  * At least one of the expected database tables doesn't exist.\n"
                        "  * The SQL was invalid.\n"
                        "Hint: Look at the output of 'django-admin sqlflush'. "
                        "That's the SQL this command wasn't able to run." % (
                            connection.settings_dict['NAME'],
                        )
                    ) from exc
E                   django.core.management.base.CommandError: Database test_posthog couldn't be flushed. Possible reasons:
E                     * The database isn't running or isn't configured correctly.
E                     * At least one of the expected database tables doesn't exist.
E                     * The SQL was invalid.
E                   Hint: Look at the output of 'django-admin sqlflush'. That's the SQL this command wasn't able to run.

env/lib/python3.10/site-packages/django/core/management/commands/flush.py:65: CommandError
_________________________ test_payloads_are_encrypted __________________________

    @pytest.mark.asyncio
    async def test_payloads_are_encrypted():
        """Test the payloads of a Workflow are encrypted when running with EncryptionCodec."""
        codec = EncryptionCodec(settings=settings)
>       client = await Client.connect(
            f"{settings.TEMPORAL_HOST}:{settings.TEMPORAL_PORT}",
            namespace=settings.TEMPORAL_NAMESPACE,
            data_converter=dataclasses.replace(temporalio.converter.default(), payload_codec=codec),
        )

posthog/temporal/tests/test_encryption_codec.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/temporalio/client.py:147: in connect
    await temporalio.service.ServiceClient.connect(connect_config),
env/lib/python3.10/site-packages/temporalio/service.py:158: in connect
    return await _BridgeServiceClient.connect(config)
env/lib/python3.10/site-packages/temporalio/service.py:675: in connect
    await client._connected_client()
env/lib/python3.10/site-packages/temporalio/service.py:688: in _connected_client
    self._bridge_client = await temporalio.bridge.client.Client.connect(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

runtime = <temporalio.bridge.runtime.Runtime object at 0x7f74f55e82e0>
config = ClientConfig(target_url='http://127.0.0.1:7233', metadata={}, identity='152128@codespaces-db4367', tls_config=None, retry_config=None, client_name='temporal-python', client_version='1.1.0')

    @staticmethod
    async def connect(
        runtime: temporalio.bridge.runtime.Runtime, config: ClientConfig
    ) -> Client:
        """Establish connection with server."""
        return Client(
            runtime,
>           await temporalio.bridge.temporal_sdk_bridge.connect_client(
                runtime._ref, config
            ),
        )
E       RuntimeError: Failed client connect: Server connection error: tonic::transport::Error(Transport, hyper::Error(Connect, ConnectError("tcp connect error", Os { code: 111, kind: ConnectionRefused, message: "Connection refused" })))

env/lib/python3.10/site-packages/temporalio/bridge/client.py:78: RuntimeError
________ TestLatestMigrations.test_ee_migrations_is_in_sync_with_latest ________

self = <posthog.test.test_latest_migrations.TestLatestMigrations testMethod=test_ee_migrations_is_in_sync_with_latest>

    def test_ee_migrations_is_in_sync_with_latest(self):
        latest_manifest_migration = self._get_latest_migration_from_manifest("ee")
>       latest_migration_file = self._get_newest_migration_file(f"{pathlib.Path().resolve()}/ee/migrations/*")

posthog/test/test_latest_migrations.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path = '/workspaces/posthog/ee/migrations/*'

    @staticmethod
    def _get_newest_migration_file(path: str) -> str:
        migrations = [file for file in glob.glob(path) if file.endswith(".py") and not file.endswith("__init__.py")]
>       latest_file = max(sorted(migrations))
E       ValueError: max() arg is an empty sequence

posthog/test/test_latest_migrations.py:29: ValueError
--------------------------- snapshot report summary ----------------------------
4 snapshots failed. 1222 snapshots passed. 245 snapshots unused.

Re-run pytest with --snapshot-update to delete unused snapshots.
=========================== short test summary info ============================
FAILED posthog/api/test/test_capture.py::TestCapture::test_quota_limits - Mod...
FAILED posthog/api/test/test_capture.py::TestCapture::test_quota_limits_ignored_if_disabled
FAILED posthog/api/test/test_event.py::TestEvents::test_filter_events_by_properties
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_all_selected
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_incorrect_group_type
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_multiple_queries
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_zero_selected
FAILED posthog/api/test/test_insight.py::TestInsight::test_insight_funnels_hogql_breakdown
FAILED posthog/api/test/test_insight.py::TestInsight::test_insight_funnels_hogql_breakdown_single
FAILED posthog/api/test/test_instance_status.py::TestInstanceStatus::test_navigation_on_cloud
FAILED posthog/api/test/test_organization.py::TestOrganizationAPI::test_cant_create_organization_with_custom_plugin_level
FAILED posthog/api/test/test_organization_domain.py::TestOrganizationDomainsAPI::test_create_domain
FAILED posthog/api/test/test_organization_domain.py::TestOrganizationDomainsAPI::test_domain_is_not_verified_with_incorrect_challenge
FAILED posthog/api/test/test_organization_domain.py::TestOrganizationDomainsAPI::test_domain_is_not_verified_with_missing_challenge
FAILED posthog/api/test/test_organization_domain.py::TestOrganizationDomainsAPI::test_domain_is_not_verified_with_missing_domain
FAILED posthog/api/test/test_persons_trends.py::TestPersonTrends::test_trends_people_endpoint_filters_search
FAILED posthog/api/test/test_persons_trends.py::TestPersonTrends::test_trends_people_endpoint_includes_recordings
FAILED posthog/api/test/test_plugin.py::TestPluginAPI::test_create_plugin_version_range_gt_next_major_ignore_on_cloud
FAILED posthog/api/test/test_preflight.py::TestPreflight::test_cloud_preflight_limited_db_queries
FAILED posthog/api/test/test_preflight.py::TestPreflight::test_cloud_preflight_request
FAILED posthog/api/test/test_preflight.py::TestPreflight::test_cloud_preflight_request_unauthenticated
FAILED posthog/api/test/test_preflight.py::TestPreflight::test_cloud_preflight_request_with_social_auth_providers
FAILED posthog/api/test/test_query.py::TestQuery::test_valid_recent_performance_pageviews
FAILED posthog/api/test/test_query.py::TestQuery::test_valid_recent_performance_pageviews_defaults_to_the_last_hour
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_api_cannot_use_whitelist_for_different_domain
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_api_sign_up - Ass...
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_cannot_social_signup_with_whitelisted_but_jit_provisioning_disabled
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_cannot_social_signup_with_whitelisted_but_unverified_domain
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_default_dashboard_is_created_on_signup
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_signup_disallowed_on_email_collision
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_signup_minimum_attrs
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_social_signup_to_existing_org_without_whitelisted_domain_on_cloud
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_social_signup_with_whitelisted_domain_on_cloud
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_social_signup_with_whitelisted_domain_on_cloud_reverse
FAILED posthog/api/test/test_signup.py::TestSignupAPI::test_social_signup_with_whitelisted_domain_on_self_hosted
FAILED posthog/api/test/test_signup.py::TestInviteSignupAPI::test_api_invite_sign_up_where_there_are_no_default_non_private_projects
FAILED posthog/api/test/test_signup_demo.py::TestDemoSignupAPI::test_demo_login
FAILED posthog/api/test/test_signup_demo.py::TestDemoSignupAPI::test_demo_signup
FAILED posthog/api/test/test_signup_demo.py::TestDemoSignupAPI::test_social_login_give_staff_privileges
FAILED posthog/api/test/test_signup_demo.py::TestDemoSignupAPI::test_social_signup_give_staff_privileges
FAILED posthog/clickhouse/test/test_person_overrides.py::test_can_insert_person_overrides
FAILED posthog/hogql/test/test_query.py::TestQuery::test_join_with_property_materialized_session_id
FAILED posthog/models/cohort/test/test_util.py::TestCohortUtils::test_simplified_cohort_filter_properties_non_precalculated_cohort_with_behavioural_filter
FAILED posthog/models/test/test_organization_model.py::TestOrganization::test_plugins_access_level_is_determined_based_on_realm
FAILED posthog/models/test/test_organization_model.py::TestOrganization::test_plugins_are_not_preinstalled_on_cloud
FAILED posthog/models/test/test_organization_model.py::TestOrganization::test_plugins_are_preinstalled_on_self_hosted
FAILED posthog/models/test/test_user_model.py::TestUser::test_analytics_metadata
FAILED posthog/plugins/test/test_utils.py::TestPluginsUtils::test_download_plugin_archive_github
FAILED posthog/plugins/test/test_utils.py::TestPluginsUtils::test_parse_github_urls
FAILED posthog/queries/session_recordings/test/test_session_recording_list_from_session_replay.py::TestClickhouseSessionRecordingsListFromSessionReplay::test_action_filter
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_with_person_filter
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_with_person_filter_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups_person_on_events_v2
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_by_multiple_groups_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props_event_with_no_group_data
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_trends_with_hogql_math
FAILED posthog/queries/trends/test/test_formula.py::TestFormula::test_breakdown_hogql
FAILED posthog/queries/trends/test/test_person.py::TestPerson::test_group_query_includes_recording_events
FAILED posthog/tasks/test/test_exporter.py::TestExporterTask::test_exporter_setsup_selenium
FAILED posthog/temporal/tests/test_squash_person_overrides_workflow.py::test_prepare_dictionary
FAILED posthog/temporal/tests/test_squash_person_overrides_workflow.py::test_squash_person_overrides_workflow
FAILED posthog/temporal/tests/test_squash_person_overrides_workflow.py::test_squash_person_overrides_workflow_with_newer_overrides
FAILED posthog/temporal/tests/test_squash_person_overrides_workflow.py::test_squash_person_overrides_workflow_with_limited_team_ids
FAILED posthog/temporal/tests/batch_exports/test_s3_batch_export_workflow.py::test_s3_export_workflow_with_minio_bucket
FAILED posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py::test_snowflake_export_workflow_exports_events_in_the_last_hour_for_the_right_team
FAILED posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py::test_snowflake_export_workflow_raises_error_on_put_fail
FAILED posthog/temporal/tests/batch_exports/test_snowflake_batch_export_workflow.py::test_snowflake_export_workflow_raises_error_on_copy_fail
FAILED posthog/test/test_middleware.py::TestAutoProjectMiddleware::test_project_switched_when_accessing_dashboard_of_another_accessible_team
FAILED posthog/test/test_middleware.py::TestAutoProjectMiddleware::test_project_switched_when_accessing_feature_flag_of_another_accessible_team
FAILED posthog/test/test_middleware.py::TestAutoProjectMiddleware::test_project_unchanged_when_accessing_dashboards_list
FAILED posthog/test/test_middleware.py::TestAutoProjectMiddleware::test_project_unchanged_when_creating_feature_flag
FAILED posthog/test/test_middleware.py::TestPostHogTokenCookieMiddleware::test_logged_in_client
FAILED posthog/test/test_middleware.py::TestPostHogTokenCookieMiddleware::test_logout
FAILED posthog/test/test_team.py::TestTeam::test_team_on_cloud_uses_feature_flag_to_determine_person_on_events
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_a_working_slow_db
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_experience_continuity_working_slow_db
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_group_properties
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_group_properties_and_slow_db
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_person_properties
FAILED posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_allows_duplicate_override_person_id
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_allows_override_person_id_as_old_person_id_in_different_teams
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_old_person_id_as_override_person_id
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_override_person_id_as_old_person_id
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_same_old_person_id
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_old_person_id_as_override_person_id_in_different_teams
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_same_old_person_id_in_different_teams
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_allow_consecutive_merges
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_disallows_concurrent_merge
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_disallows_concurrent_merge_different_order
FAILED posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_merge
FAILED posthog/temporal/tests/batch_exports/test_run_updates.py::RunUpdatesTest::test_create_export_run
FAILED posthog/temporal/tests/batch_exports/test_run_updates.py::RunUpdatesTest::test_update_export_run_status
FAILED posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_race_conditions
FAILED posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion
FAILED posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging
FAILED posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_simulated_race_conditions_on_person_merging
FAILED posthog/test/test_migration_0287.py::CreatingSessionRecordingModelMigrationTestCase::test_migrate_to_create_session_recordings
FAILED posthog/temporal/tests/test_encryption_codec.py::test_payloads_are_encrypted
FAILED posthog/test/test_latest_migrations.py::TestLatestMigrations::test_ee_migrations_is_in_sync_with_latest
ERROR posthog/api/test/batch_exports/test_backfill.py::test_batch_export_backfill
ERROR posthog/api/test/batch_exports/test_backfill.py::test_batch_export_backfill_with_non_isoformatted_dates
ERROR posthog/api/test/batch_exports/test_backfill.py::test_batch_export_backfill_with_start_at_after_end_at
ERROR posthog/api/test/batch_exports/test_backfill.py::test_cannot_trigger_backfill_for_another_organization
ERROR posthog/api/test/batch_exports/test_backfill.py::test_backfill_is_partitioned_by_team_id
ERROR posthog/api/test/batch_exports/test_create.py::test_create_batch_export_with_interval_schedule
ERROR posthog/api/test/batch_exports/test_create.py::test_cannot_create_a_batch_export_for_another_organization
ERROR posthog/api/test/batch_exports/test_delete.py::test_delete_batch_export
ERROR posthog/api/test/batch_exports/test_delete.py::test_cannot_delete_export_of_other_organizations
ERROR posthog/api/test/batch_exports/test_delete.py::test_deletes_are_partitioned_by_team_id
ERROR posthog/api/test/batch_exports/test_get.py::test_can_get_exports_for_your_organizations
ERROR posthog/api/test/batch_exports/test_get.py::test_cannot_get_exports_for_other_organizations
ERROR posthog/api/test/batch_exports/test_get.py::test_batch_exports_are_partitioned_by_team
ERROR posthog/api/test/batch_exports/test_list.py::test_list_batch_exports - ...
ERROR posthog/api/test/batch_exports/test_list.py::test_cannot_list_batch_exports_for_other_organizations
ERROR posthog/api/test/batch_exports/test_list.py::test_list_is_partitioned_by_team
ERROR posthog/api/test/batch_exports/test_pause.py::test_pause_and_unpause_batch_export
ERROR posthog/api/test/batch_exports/test_pause.py::test_connot_pause_and_unpause_batch_exports_of_other_organizations
ERROR posthog/api/test/batch_exports/test_pause.py::test_pause_and_unpause_are_partitioned_by_team_id
ERROR posthog/api/test/batch_exports/test_pause.py::test_pause_batch_export_that_is_already_paused
ERROR posthog/api/test/batch_exports/test_pause.py::test_unpause_batch_export_that_is_already_unpaused
ERROR posthog/api/test/batch_exports/test_pause.py::test_pause_non_existent_batch_export
ERROR posthog/api/test/batch_exports/test_pause.py::test_unpause_can_trigger_a_backfill
ERROR posthog/api/test/batch_exports/test_runs.py::test_can_get_export_runs_for_your_organizations
ERROR posthog/api/test/batch_exports/test_runs.py::test_cannot_get_exports_for_other_organizations
ERROR posthog/api/test/batch_exports/test_runs.py::test_batch_exports_are_partitioned_by_team
ERROR posthog/api/test/batch_exports/test_update.py::test_can_put_config - Ru...
ERROR posthog/api/test/batch_exports/test_update.py::test_can_patch_config - ...
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_a_working_slow_db
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_experience_continuity_working_slow_db
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_group_properties
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_group_properties_and_slow_db
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_person_properties
ERROR posthog/api/test/test_feature_flag.py::TestResiliency::test_feature_flags_v3_with_slow_db_doesnt_try_to_compute_conditions_again
ERROR posthog/api/test/batch_exports/test_reset.py::test_can_reset_export_run
ERROR posthog/api/test/batch_exports/test_reset.py::test_can_reset_export_run
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_allows_duplicate_override_person_id
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_allows_override_person_id_as_old_person_id_in_different_teams
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_old_person_id_as_override_person_id
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_override_person_id_as_old_person_id
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_disallows_same_old_person_id
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_old_person_id_as_override_person_id_in_different_teams
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverride::test_person_override_same_old_person_id_in_different_teams
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_allow_consecutive_merges
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_disallows_concurrent_merge
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_disallows_concurrent_merge_different_order
ERROR posthog/models/test/test_person_override_model.py::TestPersonOverrideConcurrency::test_person_override_merge
ERROR posthog/temporal/tests/batch_exports/test_run_updates.py::RunUpdatesTest::test_create_export_run
ERROR posthog/temporal/tests/batch_exports/test_run_updates.py::RunUpdatesTest::test_update_export_run_status
ERROR posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_race_conditions
ERROR posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_race_conditions_on_person_creation_and_deletion
ERROR posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_simulated_error_race_conditions_on_person_merging
ERROR posthog/test/test_feature_flag.py::TestHashKeyOverridesRaceConditions::test_hash_key_overrides_with_simulated_race_conditions_on_person_merging
===== 107 failed, 2490 passed, 23 skipped, 53 errors in 1697.47s (0:28:17) =====
