Number of tests to run: 36
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-6.2.5, py-1.10.0, pluggy-0.13.1
django: settings: posthog.settings (from ini)
rootdir: /workspaces/posthog, configfile: pytest.ini
plugins: celery-4.4.7, flaky-3.7.0, timeout-2.1.0, env-0.6.2, cov-2.12.1, icdiff-0.5, syrupy-1.4.6, asyncio-0.20.3, Faker-17.5.0, mock-3.5.1, django-4.1.0, split-0.6.0
asyncio: mode=strict
collected 2643 items / 2601 deselected / 42 selected

posthog/api/test/test_event.py F                                         [  2%]
posthog/api/test/test_feature_flag.py FFFFF                              [ 14%]
posthog/api/test/test_insight.py FF                                      [ 19%]
posthog/api/test/test_persons_trends.py FF                               [ 23%]
posthog/api/test/test_plugin.py F                                        [ 26%]
posthog/api/test/test_query.py Fs                                        [ 30%]
posthog/api/test/test_signup.py F                                        [ 33%]
posthog/hogql/test/test_query.py F                                       [ 35%]
posthog/models/test/test_organization_model.py FF                        [ 40%]
posthog/models/test/test_user_model.py F                                 [ 42%]
posthog/plugins/test/test_utils.py FF                                    [ 47%]
posthog/queries/session_recordings/test/test_session_recording_list.py . [ 50%]
.                                                                        [ 52%]
posthog/queries/session_recordings/test/test_session_recording_list_from_session_replay.py F [ 54%]
                                                                         [ 54%]
posthog/queries/test/test_trends.py ...FFFFFFFFFFFF                      [ 90%]
posthog/queries/trends/test/test_formula.py F                            [ 92%]
posthog/queries/trends/test/test_person.py F                             [ 95%]
posthog/test/test_team.py F                                              [ 97%]
posthog/api/test/batch_exports/test_reset.py F                           [100%]

=================================== FAILURES ===================================
_________________ TestEvents.test_filter_events_by_properties __________________

self = <posthog.api.test.test_event.TestEvents testMethod=test_filter_events_by_properties>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=False)
    def test_filter_events_by_properties(self):
        _create_person(properties={"email": "tim@posthog.com"}, team=self.team, distinct_ids=["2", "some-random-uid"])
        _create_event(event="event_name", team=self.team, distinct_id="2", properties={"$browser": "Chrome"})
        event2_uuid = _create_event(
            event="event_name", team=self.team, distinct_id="2", properties={"$browser": "Safari"}
        )
        flush_persons_and_events()
    
        # Django session, PostHog user, PostHog team, PostHog org membership,
        # look up if rate limit is enabled (cached after first lookup), 5x non-cached instance
        # setting (poe, rate limit), person and distinct id
        expected_queries = 12
    
>       with self.assertNumQueries(expected_queries):

posthog/api/test/test_event.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
env/lib/python3.10/site-packages/django/test/testcases.py:84: in __exit__
    self.test_case.assertEqual(
E   AssertionError: 10 != 12 : 10 queries executed, 12 expected
E   Captured queries were:
E   1. SELECT "django_session"."session_key", "django_session"."session_data", "django_session"."expire_date" FROM "django_session" WHERE ("django_session"."expire_date" > '2023-07-11T11:34:33.398265+00:00'::timestamptz AND "django_session"."session_key" = 'pmyh2ja933hhpx95gvekjqtg4hb7jaz0') LIMIT 21 /**/
E   2. SELECT "posthog_user"."id", "posthog_user"."password", "posthog_user"."last_login", "posthog_user"."first_name", "posthog_user"."last_name", "posthog_user"."is_staff", "posthog_user"."is_active", "posthog_user"."date_joined", "posthog_user"."uuid", "posthog_user"."current_organization_id", "posthog_user"."current_team_id", "posthog_user"."email", "posthog_user"."pending_email", "posthog_user"."temporary_token", "posthog_user"."distinct_id", "posthog_user"."is_email_verified", "posthog_user"."has_seen_product_intro_for", "posthog_user"."email_opt_in", "posthog_user"."partial_notification_settings", "posthog_user"."anonymize_data", "posthog_user"."toolbar_mode", "posthog_user"."events_column_config" FROM "posthog_user" WHERE "posthog_user"."id" = 567 LIMIT 21 /**/
E   3. SELECT "posthog_team"."id", "posthog_team"."uuid", "posthog_team"."organization_id", "posthog_team"."api_token", "posthog_team"."app_urls", "posthog_team"."name", "posthog_team"."slack_incoming_webhook", "posthog_team"."created_at", "posthog_team"."updated_at", "posthog_team"."anonymize_ips", "posthog_team"."completed_snippet_onboarding", "posthog_team"."ingested_event", "posthog_team"."autocapture_opt_out", "posthog_team"."autocapture_exceptions_opt_in", "posthog_team"."autocapture_exceptions_errors_to_ignore", "posthog_team"."session_recording_opt_in", "posthog_team"."capture_console_log_opt_in", "posthog_team"."capture_performance_opt_in", "posthog_team"."session_recording_version", "posthog_team"."signup_token", "posthog_team"."is_demo", "posthog_team"."access_control", "posthog_team"."inject_web_apps", "posthog_team"."test_account_filters", "posthog_team"."test_account_filters_default_checked", "posthog_team"."path_cleaning_filters", "posthog_team"."timezone", "posthog_team"."data_attributes", "posthog_team"."person_display_name_properties", "posthog_team"."live_events_columns", "posthog_team"."recording_domains", "posthog_team"."primary_dashboard_id", "posthog_team"."extra_settings", "posthog_team"."correlation_config", "posthog_team"."session_recording_retention_period_days" FROM "posthog_team" WHERE "posthog_team"."id" = 802 LIMIT 21 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   4. SELECT "posthog_organizationmembership"."id", "posthog_organizationmembership"."organization_id", "posthog_organizationmembership"."user_id", "posthog_organizationmembership"."level", "posthog_organizationmembership"."joined_at", "posthog_organizationmembership"."updated_at", "posthog_organization"."id", "posthog_organization"."name", "posthog_organization"."slug", "posthog_organization"."created_at", "posthog_organization"."updated_at", "posthog_organization"."plugins_access_level", "posthog_organization"."for_internal_metrics", "posthog_organization"."is_member_join_email_enabled", "posthog_organization"."enforce_2fa", "posthog_organization"."customer_id", "posthog_organization"."available_features", "posthog_organization"."available_product_features", "posthog_organization"."usage", "posthog_organization"."setup_section_2_completed", "posthog_organization"."personalization", "posthog_organization"."domain_whitelist" FROM "posthog_organizationmembership" INNER JOIN "posthog_organization" ON ("posthog_organizationmembership"."organization_id" = "posthog_organization"."id") WHERE "posthog_organizationmembership"."user_id" = 567 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   5. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:RATE_LIMIT_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   6. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   7. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   8. SELECT "posthog_instancesetting"."id", "posthog_instancesetting"."key", "posthog_instancesetting"."raw_value" FROM "posthog_instancesetting" WHERE "posthog_instancesetting"."key" = 'constance:posthog:PERSON_ON_EVENTS_ENABLED' ORDER BY "posthog_instancesetting"."id" ASC LIMIT 1 /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   9. SELECT "posthog_person"."id", "posthog_person"."created_at", "posthog_person"."properties_last_updated_at", "posthog_person"."properties_last_operation", "posthog_person"."team_id", "posthog_person"."properties", "posthog_person"."is_user_id", "posthog_person"."is_identified", "posthog_person"."uuid", "posthog_person"."version" FROM "posthog_person" INNER JOIN "posthog_persondistinctid" ON ("posthog_person"."id" = "posthog_persondistinctid"."person_id") WHERE ("posthog_persondistinctid"."distinct_id" IN ('2') AND "posthog_persondistinctid"."team_id" = 802 AND "posthog_person"."team_id" = 802) /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
E   10. SELECT "posthog_persondistinctid"."id", "posthog_persondistinctid"."team_id", "posthog_persondistinctid"."person_id", "posthog_persondistinctid"."distinct_id", "posthog_persondistinctid"."version" FROM "posthog_persondistinctid" WHERE "posthog_persondistinctid"."person_id" IN (659) /*controller='project_events-list',route='api/projects/%28%3FP%3Cparent_lookup_team_id%3E%5B%5E/.%5D%2B%29/events/%3F%24'*/
______________ TestBlastRadius.test_user_blast_radius_with_groups ______________

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups>

    @snapshot_clickhouse_queries
    def test_user_blast_radius_with_groups(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3],
                            "operator": "exact",
                            "group_type_index": 0,
                        }
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 4, "total_users": 10}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 4, actual: 0,'total_users', expected: 10, actual: 0

posthog/api/test/test_feature_flag.py:2637: AssertionError
_______ TestBlastRadius.test_user_blast_radius_with_groups_all_selected ________

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_all_selected>

    def test_user_blast_radius_with_groups_all_selected(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(5):
            create_group(
                team_id=self.team.pk, group_type_index=1, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [],
                    "rollout_percentage": 25,
                },
                "group_type_index": 1,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 5, "total_users": 5}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 5, actual: 0,'total_users', expected: 5, actual: 0

posthog/api/test/test_feature_flag.py:2688: AssertionError
___ TestBlastRadius.test_user_blast_radius_with_groups_incorrect_group_type ____

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_incorrect_group_type>

    def test_user_blast_radius_with_groups_incorrect_group_type(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3, 4],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [2, 3, 4, 5, 6],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 1,
            },
        )
    
>       self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
E       AssertionError: 200 != 400

posthog/api/test/test_feature_flag.py:2766: AssertionError
_____ TestBlastRadius.test_user_blast_radius_with_groups_multiple_queries ______

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_multiple_queries>

    @snapshot_clickhouse_queries
    def test_user_blast_radius_with_groups_multiple_queries(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=1)
    
        for i in range(10):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [0, 1, 2, 3, 4],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                        {
                            "key": "industry",
                            "type": "group",
                            "value": [2, 3, 4, 5, 6],
                            "operator": "exact",
                            "group_type_index": 0,
                        },
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 3, "total_users": 10}, response_json)
E       AssertionError: Mismatched values: 'users_affected', expected: 3, actual: 0,'total_users', expected: 10, actual: 0

posthog/api/test/test_feature_flag.py:2729: AssertionError
_______ TestBlastRadius.test_user_blast_radius_with_groups_zero_selected _______

self = <posthog.api.test.test_feature_flag.TestBlastRadius testMethod=test_user_blast_radius_with_groups_zero_selected>

    def test_user_blast_radius_with_groups_zero_selected(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
    
        for i in range(5):
            create_group(
                team_id=self.team.pk, group_type_index=0, group_key=f"org:{i}", properties={"industry": f"{i}"}
            )
    
        response = self.client.post(
            f"/api/projects/{self.team.id}/feature_flags/user_blast_radius",
            {
                "condition": {
                    "properties": [
                        {"key": "industry", "type": "group", "value": [8], "operator": "exact", "group_type_index": 0}
                    ],
                    "rollout_percentage": 25,
                },
                "group_type_index": 0,
            },
        )
    
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
        response_json = response.json()
>       self.assertDictContainsSubset({"users_affected": 0, "total_users": 5}, response_json)
E       AssertionError: Mismatched values: 'total_users', expected: 5, actual: 0

posthog/api/test/test_feature_flag.py:2663: AssertionError
_______________ TestInsight.test_insight_funnels_hogql_breakdown _______________

self = <posthog.api.test.test_insight.TestInsight testMethod=test_insight_funnels_hogql_breakdown>

    @snapshot_clickhouse_queries
    @also_test_with_materialized_columns(event_properties=["int_value"], person_properties=["fish"])
    def test_insight_funnels_hogql_breakdown(self) -> None:
        with freeze_time("2012-01-15T04:01:34.000Z"):
            _create_person(team=self.team, distinct_ids=["1"], properties={"fish": "there is no fish"})
            _create_event(team=self.team, event="user signed up", distinct_id="1", properties={"int_value": 1})
            _create_event(team=self.team, event="user did things", distinct_id="1", properties={"int_value": 20})
            response = self.client.post(
                f"/api/projects/{self.team.id}/insights/funnel/",
                {
                    "breakdown_type": "hogql",
                    "breakdowns": [{"property": "person.properties.fish", "type": "hogql"}],
                    "events": [
                        {"id": "user signed up", "type": "events", "order": 0},
                        {"id": "user did things", "type": "events", "order": 1},
                    ],
                    "properties": json.dumps(
                        [
                            {"key": "toInt(properties.int_value) < 10 and 'bla' != 'a%sd'", "type": "hogql"},
                        ]
                    ),
                    "funnel_window_days": 14,
                },
            )
>           self.assertEqual(response.status_code, status.HTTP_200_OK)
E           AssertionError: 500 != 200

posthog/api/test/test_insight.py:2445: AssertionError
----------------------------- Captured stderr call -----------------------------
2023-07-11T11:34:37.651114Z [error    ] Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?
 [posthog.exceptions] host= ip=127.0.0.1 path=/api/projects/808/insights/funnel/ pid=608019 request_id=dabb0efe-032e-4a0d-a9c2-50e8604132e2 team_id=808 tid=140683017934656 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute
    result = client.execute(
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/unittest/mock.py", line 1188, in _execute_mock_call
    return self._mock_wraps(*args, **kwargs)
  File "/workspaces/posthog/posthog/test/base.py", line 673, in execute_wrapper
    return original_client_execute(query, *args, **kwargs)
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute
    rv = self.process_ordinary_query(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query
    return self.receive_result(with_column_types=with_column_types,
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result
    return result.get_result()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result
    for packet in self.packet_generator:
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator
    packet = self.receive_packet()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet
    raise packet.exception
clickhouse_driver.errors.ServerException: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel
    funnel = self.calculate_funnel(request)
  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper
    fresh_result_package = cast(T, f(self, request))
  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel
    "result": funnel_order_class(team=team, filter=filter).run(),
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run
    results = self._exec_query()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query
    query = self.get_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query
    {self.get_step_counts_query()}
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query
    steps_per_person_query = self.get_step_counts_without_aggregation_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query
    formatted_query = self.build_step_subquery(2, max_steps)
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery
    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query
    values = self._get_breakdown_conditions()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions
    return get_breakdown_prop_values(
  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values
    return insight_sync_execute(
  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner
    return inner._impl(*args, **kwargs)  # type: ignore
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute
    raise err
posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?

2023-07-11T11:34:37.654797Z [error    ] Internal Server Error: /api/projects/808/insights/funnel/ [django.request] host= pid=608019 team_id=808 tid=140683017934656 x_forwarded_for=
2023-07-11T11:34:37.655011Z [error    ] Internal Server Error: /api/projects/808/insights/funnel/ [django.request] host= pid=608019 team_id=808 tid=140683017934656 x_forwarded_for=
------------------------------ Captured log call -------------------------------
ERROR    posthog.exceptions:exceptions.py:56 {'request_id': 'dabb0efe-032e-4a0d-a9c2-50e8604132e2', 'ip': '127.0.0.1', 'path': '/api/projects/808/insights/funnel/', 'event': CHQueryErrorUnknownIdentifier('DB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n'), 'x_forwarded_for': '', 'team_id': 808, 'host': '', 'timestamp': '2023-07-11T11:34:37.651114Z', 'logger': 'posthog.exceptions', 'level': 'error', 'pid': 608019, 'tid': 140683017934656, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute\n    result = client.execute(\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1114, in __call__\n    return self._mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1118, in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n  File "/usr/local/lib/python3.10/unittest/mock.py", line 1188, in _execute_mock_call\n    return self._mock_wraps(*args, **kwargs)\n  File "/workspaces/posthog/posthog/test/base.py", line 673, in execute_wrapper\n    return original_client_execute(query, *args, **kwargs)\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute\n    rv = self.process_ordinary_query(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query\n    return self.receive_result(with_column_types=with_column_types,\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result\n    return result.get_result()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result\n    for packet in self.packet_generator:\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator\n    packet = self.receive_packet()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet\n    raise packet.exception\nclickhouse_driver.errors.ServerException: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch\n    response = handler(request, *args, **kwargs)\n  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel\n    funnel = self.calculate_funnel(request)\n  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper\n    fresh_result_package = cast(T, f(self, request))\n  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel\n    "result": funnel_order_class(team=team, filter=filter).run(),\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run\n    results = self._exec_query()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query\n    query = self.get_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query\n    {self.get_step_counts_query()}\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query\n    steps_per_person_query = self.get_step_counts_without_aggregation_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query\n    formatted_query = self.build_step_subquery(2, max_steps)\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery\n    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query\n    values = self._get_breakdown_conditions()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions\n    return get_breakdown_prop_values(\n  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values\n    return insight_sync_execute(\n  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute\n    return sync_execute(query, args=args, team_id=team_id, **kwargs)\n  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner\n    return inner._impl(*args, **kwargs)  # type: ignore\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute\n    raise err\nposthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 808) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n'}
ERROR    django.request:log.py:224 Internal Server Error: /api/projects/808/insights/funnel/
___________ TestInsight.test_insight_funnels_hogql_breakdown_single ____________

self = <posthog.api.test.test_insight.TestInsight testMethod=test_insight_funnels_hogql_breakdown_single>

    @also_test_with_materialized_columns(event_properties=["int_value"], person_properties=["fish"])
    def test_insight_funnels_hogql_breakdown_single(self) -> None:
        with freeze_time("2012-01-15T04:01:34.000Z"):
            _create_person(team=self.team, distinct_ids=["1"], properties={"fish": "there is no fish"})
            _create_event(team=self.team, event="user signed up", distinct_id="1", properties={"int_value": 1})
            _create_event(team=self.team, event="user did things", distinct_id="1", properties={"int_value": 20})
            response = self.client.post(
                f"/api/projects/{self.team.id}/insights/funnel/",
                {
                    "breakdown_type": "hogql",
                    "breakdown": "person.properties.fish",
                    "events": [
                        {"id": "user signed up", "type": "events", "order": 0},
                        {"id": "user did things", "type": "events", "order": 1},
                    ],
                    "properties": json.dumps(
                        [
                            {"key": "toInt(properties.int_value) < 10 and 'bla' != 'a%sd'", "type": "hogql"},
                        ]
                    ),
                    "funnel_window_days": 14,
                },
            )
>           self.assertEqual(response.status_code, status.HTTP_200_OK)
E           AssertionError: 500 != 200

posthog/api/test/test_insight.py:2483: AssertionError
----------------------------- Captured stderr call -----------------------------
2023-07-11T11:34:38.177383Z [error    ] Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?
 [posthog.exceptions] host= ip=127.0.0.1 path=/api/projects/809/insights/funnel/ pid=608019 request_id=2c7c7568-e4d2-43e8-ab0b-48cf3f257f41 team_id=809 tid=140683017934656 x_forwarded_for=
Traceback (most recent call last):
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute
    result = client.execute(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute
    rv = self.process_ordinary_query(
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query
    return self.receive_result(with_column_types=with_column_types,
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result
    return result.get_result()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result
    for packet in self.packet_generator:
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator
    packet = self.receive_packet()
  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet
    raise packet.exception
clickhouse_driver.errors.ServerException: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel
    funnel = self.calculate_funnel(request)
  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper
    fresh_result_package = cast(T, f(self, request))
  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel
    "result": funnel_order_class(team=team, filter=filter).run(),
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run
    results = self._exec_query()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query
    query = self.get_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query
    {self.get_step_counts_query()}
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query
    steps_per_person_query = self.get_step_counts_without_aggregation_query()
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query
    formatted_query = self.build_step_subquery(2, max_steps)
  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery
    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query
    values = self._get_breakdown_conditions()
  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions
    return get_breakdown_prop_values(
  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values
    return insight_sync_execute(
  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner
    return inner._impl(*args, **kwargs)  # type: ignore
  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute
    raise err
posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, 'fish'), ''), 'null'), '^"|"$', '')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN ['user did things', 'user signed up']) AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2012-01-08 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2012-01-15 23:59:59', 'UTC')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'int_value'), ''), 'null'), '^"|"$', '')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'properties' 'person_props', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:

0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
23. ? @ 0x00007f134ac45609 in ?
24. __clone @ 0x00007f134ab6a133 in ?

2023-07-11T11:34:38.179939Z [error    ] Internal Server Error: /api/projects/809/insights/funnel/ [django.request] host= pid=608019 team_id=809 tid=140683017934656 x_forwarded_for=
2023-07-11T11:34:38.180158Z [error    ] Internal Server Error: /api/projects/809/insights/funnel/ [django.request] host= pid=608019 team_id=809 tid=140683017934656 x_forwarded_for=
------------------------------ Captured log call -------------------------------
ERROR    posthog.exceptions:exceptions.py:56 {'request_id': '2c7c7568-e4d2-43e8-ab0b-48cf3f257f41', 'ip': '127.0.0.1', 'path': '/api/projects/809/insights/funnel/', 'event': CHQueryErrorUnknownIdentifier('DB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n'), 'x_forwarded_for': '', 'team_id': 809, 'host': '', 'timestamp': '2023-07-11T11:34:38.177383Z', 'logger': 'posthog.exceptions', 'level': 'error', 'pid': 608019, 'tid': 140683017934656, 'exception': 'Traceback (most recent call last):\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 102, in sync_execute\n    result = client.execute(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 304, in execute\n    rv = self.process_ordinary_query(\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 491, in process_ordinary_query\n    return self.receive_result(with_column_types=with_column_types,\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 151, in receive_result\n    return result.get_result()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/result.py", line 50, in get_result\n    for packet in self.packet_generator:\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 167, in packet_generator\n    packet = self.receive_packet()\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/clickhouse_driver/client.py", line 184, in receive_packet\n    raise packet.exception\nclickhouse_driver.errors.ServerException: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "/workspaces/posthog/env/lib/python3.10/site-packages/rest_framework/views.py", line 506, in dispatch\n    response = handler(request, *args, **kwargs)\n  File "/workspaces/posthog/posthog/api/insight.py", line 876, in funnel\n    funnel = self.calculate_funnel(request)\n  File "/workspaces/posthog/posthog/decorators.py", line 74, in wrapper\n    fresh_result_package = cast(T, f(self, request))\n  File "/workspaces/posthog/posthog/api/insight.py", line 902, in calculate_funnel\n    "result": funnel_order_class(team=team, filter=filter).run(),\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 103, in run\n    results = self._exec_query()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 269, in _exec_query\n    query = self.get_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 38, in get_query\n    {self.get_step_counts_query()}\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 43, in get_step_counts_query\n    steps_per_person_query = self.get_step_counts_without_aggregation_query()\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 61, in get_step_counts_without_aggregation_query\n    formatted_query = self.build_step_subquery(2, max_steps)\n  File "/workspaces/posthog/posthog/queries/funnels/funnel.py", line 123, in build_step_subquery\n    FROM ({self._get_inner_event_query(entity_name=event_names_alias)})\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 461, in _get_inner_event_query\n    values = self._get_breakdown_conditions()\n  File "/workspaces/posthog/posthog/queries/funnels/base.py", line 846, in _get_breakdown_conditions\n    return get_breakdown_prop_values(\n  File "/workspaces/posthog/posthog/queries/breakdown_props.py", line 201, in get_breakdown_prop_values\n    return insight_sync_execute(\n  File "/workspaces/posthog/posthog/queries/insight.py", line 15, in insight_sync_execute\n    return sync_execute(query, args=args, team_id=team_id, **kwargs)\n  File "/workspaces/posthog/posthog/utils.py", line 1263, in inner\n    return inner._impl(*args, **kwargs)  # type: ignore\n  File "/workspaces/posthog/posthog/clickhouse/client/execute.py", line 113, in sync_execute\n    raise err\nposthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.\nDB::Exception: Missing columns: \'person_props\' while processing query: \'SELECT [replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, \'fish\'), \'\'), \'null\'), \'^"|"$\', \'\')] AS value, count() AS count FROM events AS e WHERE (team_id = 809) AND (event IN [\'user did things\', \'user signed up\']) AND (toTimeZone(timestamp, \'UTC\') >= toDateTime(\'2012-01-08 00:00:00\', \'UTC\')) AND (toTimeZone(timestamp, \'UTC\') <= toDateTime(\'2012-01-15 23:59:59\', \'UTC\')) AND ((toInt64OrNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, \'int_value\'), \'\'), \'null\'), \'^"|"$\', \'\')) < 10) AND 1) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25\', required columns: \'team_id\' \'event\' \'timestamp\' \'properties\' \'person_props\', maybe you meant: \'team_id\', \'event\', \'timestamp\' or \'properties\'. Stack trace:\n\n0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse\n1. ? @ 0x0000000008877911 in /usr/bin/clickhouse\n2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse\n3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse\n4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse\n5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse\n6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse\n9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse\n10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse\n11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse\n12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse\n13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse\n14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse\n15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse\n16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse\n17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse\n18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse\n19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse\n20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse\n21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse\n23. ? @ 0x00007f134ac45609 in ?\n24. __clone @ 0x00007f134ab6a133 in ?\n'}
ERROR    django.request:log.py:224 Internal Server Error: /api/projects/809/insights/funnel/
_________ TestPersonTrends.test_trends_people_endpoint_filters_search __________
posthog/test/base.py:802: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.api.test.test_persons_trends.TestPersonTrends testMethod=test_trends_people_endpoint_filters_search>
query = '/* user_id:0 request:_snapshot_ */ \nSELECT\n    person_id AS actor_id,\n    count() AS actor_value\n    , groupUniqA...), \'^"|"$\', \'\')))\n\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m            e."properties" as "properties",[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m            pdi.person_id as person_id,[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:452: AssertionError
_______ TestPersonTrends.test_trends_people_endpoint_includes_recordings _______
posthog/test/base.py:802: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.api.test.test_persons_trends.TestPersonTrends testMethod=test_trends_people_endpoint_includes_recordings>
query = '/* user_id:0 request:_snapshot_ */ \nSELECT\n    person_id AS actor_id,\n    count() AS actor_value\n    , groupUniqA...), \'^"|"$\', \'\')))\n\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m            e."properties" as "properties",[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m.[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m.[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m            pdi.person_id as person_id,[0m[0m
E       [0m[2m       ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:452: AssertionError
_ TestPluginAPI.test_create_plugin_version_range_gt_next_major_ignore_on_cloud _

self = <posthog.api.test.test_plugin.TestPluginAPI testMethod=test_create_plugin_version_range_gt_next_major_ignore_on_cloud>
mock_get = <MagicMock name='get' id='140682184019104'>
mock_reload = <MagicMock name='reload_plugins_on_workers' id='140682184022176'>

    def test_create_plugin_version_range_gt_next_major_ignore_on_cloud(self, mock_get, mock_reload):
        with self.is_cloud(True):
            response = self.client.post(
                "/api/organizations/@current/plugins/",
                {
                    "url": f"https://github.com/posthog-plugin/version-greater-than/commit/{Version(VERSION).next_major()}"
                },
            )
>           self.assertEqual(response.status_code, 201)
E           AssertionError: 400 != 201

posthog/api/test/test_plugin.py:559: AssertionError
______________ TestQuery.test_valid_recent_performance_pageviews _______________

self = <posthog.api.test.test_query.TestQuery testMethod=test_valid_recent_performance_pageviews>

    def test_valid_recent_performance_pageviews(self):
        api_response = self.client.post(
            f"/api/projects/{self.team.id}/query/",
            {"query": {"kind": "RecentPerformancePageViewNode", "dateRange": {"date_from": None, "date_to": None}}},
        )
>       assert api_response.status_code == 200
E       AssertionError: assert 400 == 200
E        +  where 400 = <Response status_code=400, "application/json">.status_code

posthog/api/test/test_query.py:493: AssertionError
_ TestInviteSignupAPI.test_api_invite_sign_up_where_there_are_no_default_non_private_projects _

self = <posthog.api.test.test_signup.TestInviteSignupAPI testMethod=test_api_invite_sign_up_where_there_are_no_default_non_private_projects>

    @pytest.mark.ee
    def test_api_invite_sign_up_where_there_are_no_default_non_private_projects(self):
        self.client.logout()
        invite: OrganizationInvite = OrganizationInvite.objects.create(
            target_email="test+private@posthog.com", organization=self.organization
        )
    
        self.organization.available_features = [AvailableFeature.PROJECT_BASED_PERMISSIONING]
        self.organization.save()
        self.team.access_control = True
        self.team.save()
    
        response = self.client.post(
            f"/api/signup/{invite.id}/", {"first_name": "Alice", "password": "test_password", "email_opt_in": True}
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        user = cast(User, User.objects.order_by("-pk")[0])
        self.assertEqual(user.organization_memberships.count(), 1)
        self.assertEqual(user.organization, self.organization)
        # here
>       self.assertEqual(
            user.current_team, None
        )  # User is not assigned to a project, as there are no non-private projects
E       AssertionError: <Team at 0x7ff31f608370: id=814, uuid=UUI[83 chars]123'> != None

posthog/api/test/test_signup.py:835: AssertionError
__________ TestQuery.test_join_with_property_materialized_session_id ___________

self = <test_query.TestQuery testMethod=test_join_with_property_materialized_session_id>

    def test_join_with_property_materialized_session_id(self):
        with freeze_time("2020-01-10"):
            _create_person(distinct_ids=["some_id"], team_id=self.team.pk, properties={"$some_prop": "something"})
            _create_event(
                event="$pageview",
                team=self.team,
                distinct_id="some_id",
                properties={"attr": "some_val", "$session_id": "111"},
            )
            _create_event(
                event="$pageview",
                team=self.team,
                distinct_id="some_id",
                properties={"attr": "some_val", "$session_id": "111"},
            )
            create_snapshot(distinct_id="some_id", session_id="111", timestamp=timezone.now(), team_id=self.team.pk)
    
            response = execute_hogql_query(
                "select e.event, s.session_id from events e left join session_recording_events s on s.session_id = e.properties.$session_id where e.properties.$session_id is not null limit 10",
                team=self.team,
            )
>           self.assertEqual(
                response.clickhouse,
                f"SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, nullIf(nullIf(e.`$session_id`, ''), 'null')) WHERE and(equals(s.team_id, {self.team.pk}), equals(e.team_id, {self.team.pk}), isNotNull(nullIf(nullIf(e.`$session_id`, ''), 'null'))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True",
            )
E           AssertionError: 'SELECT e.event, s.session_id FROM events[451 chars]True' != "SELECT e.event, s.session_id FROM events[313 chars]True"
E           - SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(e.properties, %(hogql_val_0)s), ''), 'null'), '^"|"$', '')) WHERE and(equals(s.team_id, 815), equals(e.team_id, 815), isNotNull(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(e.properties, %(hogql_val_1)s), ''), 'null'), '^"|"$', ''))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True
E           + SELECT e.event, s.session_id FROM events AS e LEFT JOIN session_recording_events AS s ON equals(s.session_id, nullIf(nullIf(e.`$session_id`, ''), 'null')) WHERE and(equals(s.team_id, 815), equals(e.team_id, 815), isNotNull(nullIf(nullIf(e.`$session_id`, ''), 'null'))) LIMIT 10 SETTINGS readonly=2, max_execution_time=60, allow_experimental_object_type=True

posthog/hogql/test/test_query.py:599: AssertionError
___ TestOrganization.test_plugins_access_level_is_determined_based_on_realm ____

self = <posthog.models.test.test_organization_model.TestOrganization testMethod=test_plugins_access_level_is_determined_based_on_realm>

    def test_plugins_access_level_is_determined_based_on_realm(self):
        with self.is_cloud(True):
            new_org, _, _ = Organization.objects.bootstrap(self.user)
>           assert new_org.plugins_access_level == Organization.PluginsAccessLevel.CONFIG
E           AssertionError: assert <PluginsAccessLevel.ROOT: 9> == <PluginsAccessLevel.CONFIG: 3>
E            +  where <PluginsAccessLevel.ROOT: 9> = <Organization at 0x7ff31fe12ad0: id=UUIDT('018944bb-6c54-0000-1986-bf0304789395'), name=''>.plugins_access_level
E            +  and   <PluginsAccessLevel.CONFIG: 3> = <enum 'PluginsAccessLevel'>.CONFIG
E            +    where <enum 'PluginsAccessLevel'> = Organization.PluginsAccessLevel

posthog/models/test/test_organization_model.py:58: AssertionError
________ TestOrganization.test_plugins_are_preinstalled_on_self_hosted _________

__wrapped_mock_method__ = <function NonCallableMock.assert_any_call at 0x7ff34ee39480>
args = (<MagicMock name='get' id='140682194482176'>, 'https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) c...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140682194482176'>
actual_args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
actual_kwargs = {'headers': {'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140682194482176'>
args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
kwargs = {'headers': {}}
expected = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
cause = None
actual = [call('https://api.github.com/repos/PostHog/helloworldplugin/commits?sha=&path=', headers={'Authorization': 'Bearer gh...d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})]
expected_string = "get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.
    
        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) call not found

/usr/local/lib/python3.10/unittest/mock.py:1000: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.models.test.test_organization_model.TestOrganization testMethod=test_plugins_are_preinstalled_on_self_hosted>
mock_get = <MagicMock name='get' id='140682194482176'>

    @mock.patch("requests.get", side_effect=mocked_plugin_requests_get)
    def test_plugins_are_preinstalled_on_self_hosted(self, mock_get):
        with self.is_cloud(False):
            with self.settings(PLUGINS_PREINSTALLED_URLS=["https://github.com/PostHog/helloworldplugin/"]):
                new_org, _, _ = Organization.objects.bootstrap(
                    self.user, plugins_access_level=Organization.PluginsAccessLevel.INSTALL
                )
    
        self.assertEqual(Plugin.objects.filter(organization=new_org, is_preinstalled=True).count(), 1)
        self.assertEqual(
            Plugin.objects.filter(organization=new_org, is_preinstalled=True).get().name, "helloworldplugin"
        )
        self.assertEqual(mock_get.call_count, 2)
>       mock_get.assert_any_call(
            f"https://github.com/PostHog/helloworldplugin/archive/{HELLO_WORLD_PLUGIN_GITHUB_ZIP[0]}.zip", headers={}
        )
E       AssertionError: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={}) call not found
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'token ghu                                  
E          -_IZ4LlvIifZhNwYFDdkk6t74h85cXBv                                  
E          -0A9SrM',                                                         
E          -  },                                                             
E          -}

posthog/models/test/test_organization_model.py:40: AssertionError
_______________________ TestUser.test_analytics_metadata _______________________

self = <posthog.models.test.test_user_model.TestUser testMethod=test_analytics_metadata>

    def test_analytics_metadata(self):
        # One org, one team, anonymized
        organization, team, user = User.objects.bootstrap(
            organization_name="Test Org", email="test_org@posthog.com", password="12345678", anonymize_data=True
        )
    
        with self.is_cloud(True):
>           self.assertEqual(
                user.get_analytics_metadata(),
                {
                    "realm": "cloud",
                    "email_opt_in": False,
                    "anonymize_data": True,
                    "email": None,
                    "is_signed_up": True,
                    "organization_count": 1,
                    "project_count": 1,
                    "team_member_count_all": 1,
                    "completed_onboarding_once": False,
                    "organization_id": str(organization.id),
                    "project_id": str(team.uuid),
                    "project_setup_complete": False,
                    "has_password_set": True,
                    "joined_at": user.date_joined,
                    "has_social_auth": False,
                    "social_providers": [],
                    "instance_url": "http://localhost:8000",
                    "instance_tag": "none",
                    "is_email_verified": None,
                    "has_seen_product_intro_for": None,
                },
            )
E           AssertionError: {'realm': 'hosted-clickhouse', 'email_opt_in': False[595 chars]None} != {'realm': 'cloud', 'email_opt_in': False, 'anonymize[583 chars]None}
E           Diff is 733 characters long. Set self.maxDiff to None to see it.

posthog/models/test/test_user_model.py:19: AssertionError
_____________ TestPluginsUtils.test_download_plugin_archive_github _____________

__wrapped_mock_method__ = <function NonCallableMock.assert_called_with at 0x7ff34ee392d0>
args = (<MagicMock name='get' id='140682190400400'>, 'https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "expected call not found.\nExpected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140682190400400'>
actual_args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
actual_kwargs = {'headers': {'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140682190400400'>
args = ('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip',)
kwargs = {'headers': {}}
expected = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
actual = call('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})
_error_message = <function NonCallableMock.assert_called_with.<locals>._error_message at 0x7ff31f8b7be0>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\nActual: %s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
E           Actual: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})

/usr/local/lib/python3.10/unittest/mock.py:929: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.plugins.test.test_utils.TestPluginsUtils testMethod=test_download_plugin_archive_github>
mock_get = <MagicMock name='get' id='140682190400400'>

    def test_download_plugin_archive_github(self, mock_get):
        plugin_github_zip_1 = download_plugin_archive(
            "https://www.github.com/PostHog/helloworldplugin/commit/82c9218ee40f561b7f37a22d6b6a0ca82887ee3e",
            HELLO_WORLD_PLUGIN_GITHUB_ZIP[0],
        )
        self.assertEqual(plugin_github_zip_1, base64.b64decode(HELLO_WORLD_PLUGIN_GITHUB_ZIP[1]))
        self.assertEqual(mock_get.call_count, 1)
>       mock_get.assert_called_with(
            "https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip",
            headers={},
        )
E       AssertionError: expected call not found.
E       Expected: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={})
E       Actual: get('https://github.com/PostHog/helloworldplugin/archive/d5aa1d2b8a534f37cd93be48b214f490ef9ee904.zip', headers={'Authorization': 'token ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'token ghu                                  
E          -_IZ4LlvIifZhNwYFDdkk6t74h85cXBv                                  
E          -0A9SrM',                                                         
E          -  },                                                             
E          -}

posthog/plugins/test/test_utils.py:428: AssertionError
___________________ TestPluginsUtils.test_parse_github_urls ____________________

__wrapped_mock_method__ = <function NonCallableMock.assert_called_with at 0x7ff34ee392d0>
args = (<MagicMock name='get' id='140682179539872'>, 'https://api.github.com/repos/PostHog/posthog/commits?sha=&path=')
kwargs = {'headers': {}}, __tracebackhide__ = True
msg = "expected call not found.\nExpected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={}...                                        \n  \x00-}\x01                                                                "
__mock_self = <MagicMock name='get' id='140682179539872'>
actual_args = ('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=',)
actual_kwargs = {'headers': {'Authorization': 'Bearer ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'}}
introspection = "\nKwargs:\nassert equals failed\n  \x00-{\x01                                                                \n  \x00...                                        \n  \x00-}\x01                                                                "
@py_assert2 = None, @py_assert1 = False

    def assert_wrapper(
        __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> None:
        __tracebackhide__ = True
        try:
>           __wrapped_mock_method__(*args, **kwargs)

env/lib/python3.10/site-packages/pytest_mock/plugin.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='get' id='140682179539872'>
args = ('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=',)
kwargs = {'headers': {}}
expected = call('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
actual = call('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})
_error_message = <function NonCallableMock.assert_called_with.<locals>._error_message at 0x7ff3511a0280>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.
    
        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\nActual: %s'
                    % (expected, actual))
            raise AssertionError(error_message)
    
        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
E           Actual: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})

/usr/local/lib/python3.10/unittest/mock.py:929: AssertionError

During handling of the above exception, another exception occurred:

self = <posthog.plugins.test.test_utils.TestPluginsUtils testMethod=test_parse_github_urls>
mock_get = <MagicMock name='get' id='140682179539872'>

    def test_parse_github_urls(self, mock_get):
        parsed_url = parse_url("https://github.com/PostHog/posthog")
        self.assertEqual(parsed_url["type"], "github")
        self.assertEqual(parsed_url["user"], "PostHog")
        self.assertEqual(parsed_url["repo"], "posthog")
        self.assertEqual(parsed_url.get("tag", None), None)
        self.assertEqual(parsed_url.get("path", None), None)
        self.assertEqual(mock_get.call_count, 0)
        mock_get.reset_mock()
    
        parsed_url = parse_url("https://github.com/PostHog/posthog", get_latest_if_none=True)
        self.assertEqual(parsed_url["type"], "github")
        self.assertEqual(parsed_url["user"], "PostHog")
        self.assertEqual(parsed_url["repo"], "posthog")
        self.assertEqual(parsed_url["tag"], "MOCKLATESTCOMMIT")
        self.assertEqual(parsed_url.get("path", None), None)
        self.assertEqual(mock_get.call_count, 1)
>       mock_get.assert_called_with("https://api.github.com/repos/PostHog/posthog/commits?sha=&path=", headers={})
E       AssertionError: expected call not found.
E       Expected: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={})
E       Actual: get('https://api.github.com/repos/PostHog/posthog/commits?sha=&path=', headers={'Authorization': 'Bearer ghu_IZ4LlvIifZhNwYFDdkk6t74h85cXBv0A9SrM'})
E       
E       pytest introspection follows:
E       
E       Kwargs:
E       assert equals failed
E          -{                                                                
E          ^  'headers': {                    ^{'headers': { +}}                 
E          -    'Authorization': 'Bearer gh                                  
E          -u_IZ4LlvIifZhNwYFDdkk6t74h85cXB                                  
E          -v0A9SrM',                                                        
E          -  },                                                             
E          -}

posthog/plugins/test/test_utils.py:44: AssertionError
___ TestClickhouseSessionRecordingsListFromSessionReplay.test_action_filter ____
posthog/test/base.py:802: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_session_recording_list_from_session_replay.TestClickhouseSessionRecordingsListFromSessionReplay testMethod=test_action_filter>
query = '\n\nWITH\n        events_session_ids AS (\n            SELECT\n                groupUniqArray(event) as event_names,\...RE 1=1\n        GROUP BY session_id\n        HAVING 1=1\n        ORDER BY start_time DESC\n        LIMIT 51 OFFSET 0\n'
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m        ...[0m[0m
E       [0m[2m              AND (has(['Firefox'], replaceRegexpAll(JSONExtractRaw(properties, '$browser'), '^"|"$', ''))[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mD[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mh[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90m[[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mc[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m][0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m)[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mD[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mh[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90m[[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mc[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m][0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mw[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;225m[38;5;90m"[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mD[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mh[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23m[[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m][0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mD[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mh[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23m[[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m-[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m][0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mJ[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mx[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mc[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mR[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mw[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m^[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m|[0m[48;5;195m[38;5;23m"[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[0m
E       [0m[2m     GROUP BY session_id[0m[0m
E       [0m[2m   ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:452: AssertionError
___________________ TestTrends.test_breakdown_by_group_props ___________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebd9f0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '829_None_JWA8J32b'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebd9f0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '829_None_JWA8J32b', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebd9f0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f5645b0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebd9f0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebd9f0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 829) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props>

    def test_breakdown_by_group_props(self):
        self._create_groups()
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5"},
                    "group0_properties": {"industry": "finance"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 13),
                    "properties": {"$group_0": "org:6"},
                    "group0_properties": {"industry": "technology"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 15),
                    "properties": {"$group_0": "org:7", "$group_1": "company:10"},
                    "group0_properties": {"industry": "finance"},
                    "group1_properties": {"industry": "finance"},
                },
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
            }
        )
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6420: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 829) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
__________ TestTrends.test_breakdown_by_group_props_person_on_events ___________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682187383296'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '830_None_yqNDjqIS', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682187383296'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '830_None_yqNDjqIS', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682187383296'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '830_None_yqNDjqIS', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = ()
kwargs = {'params': None, 'query_id': '830_None_yqNDjqIS', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff3204cb5b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '830_None_yqNDjqIS'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff3204cb5b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '830_None_yqNDjqIS', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff3204cb5b0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31fb4a650>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff3204cb5b0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff3204cb5b0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 830) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance', 'technology']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_person_on_events>

    @also_test_with_materialized_columns(
        group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_by_group_props_person_on_events(self):
        self._create_groups()
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5"},
                    "group0_properties": {"industry": "finance"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 13),
                    "properties": {"$group_0": "org:6"},
                    "group0_properties": {"industry": "technology"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 15),
                    "properties": {"$group_0": "org:7", "$group_1": "company:10"},
                    "group0_properties": {"industry": "finance"},
                    "group1_properties": {"industry": "finance"},
                },
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01",
                "date_to": "2020-01-12",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6481: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 830) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance', 'technology']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________ TestTrends.test_breakdown_by_group_props_with_person_filter __________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe950>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '831_None_sKxMs783'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe950>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'...ime('2020-01-12 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '831_None_sKxMs783', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe950>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f3936d0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe950>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe950>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM (SELECT timestamp, event, team_id, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 831 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND ((event = 'sign up') AND (team_id = 831)))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 831) AND (id IN (SELECT id FROM person WHERE (team_id = 831) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', ''))) AS person ON `--pdi.person_id` = id WHERE (team_id = 831) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0' '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_with_person_filter>

    def test_breakdown_by_group_props_with_person_filter(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={"key": "value"},
            group0_properties={"industry": "finance"},
        )
        _create_event(
            event="sign up",
            distinct_id="person2",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={},
            group0_properties={"industry": "technology"},
        )
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [{"key": "key", "value": "value", "type": "person"}],
            }
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6534: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(group_properties_0, %(key)s), ..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS value, count() AS count FROM (SELECT timestamp, event, team_id, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 831 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND ((event = 'sign up') AND (team_id = 831)))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 831) AND (id IN (SELECT id FROM person WHERE (team_id = 831) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', ''))) AS person ON `--pdi.person_id` = id WHERE (team_id = 831) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0' '--pdi.person_id' 'id' 'team_id' 'event' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_ TestTrends.test_breakdown_by_group_props_with_person_filter_person_on_events _

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682180993584'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '832_None_e6P2qiHV', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682180993584'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '832_None_e6P2qiHV', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682180993584'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as...t, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n',)
kwargs = {'params': None, 'query_id': '832_None_e6P2qiHV', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = ()
kwargs = {'params': None, 'query_id': '832_None_e6P2qiHV', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebec50>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '832_None_e6P2qiHV'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebec50>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
params = None, with_column_types = False, external_tables = None
query_id = '832_None_e6P2qiHV', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebec50>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f6ca860>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebec50>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebec50>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 832) AND (event = 'sign up') AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', '')) AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'person_properties' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'person_properties', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_by_group_props_with_person_filter_person_on_events>

    @also_test_with_materialized_columns(
        person_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_by_group_props_with_person_filter_person_on_events(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={"key": "value"},
            group0_properties={"industry": "finance"},
        )
        _create_event(
            event="sign up",
            distinct_id="person2",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
            person_properties={},
            group0_properties={"industry": "technology"},
        )
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "breakdown": "industry",
                "breakdown_type": "group",
                "breakdown_group_type_index": 0,
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [{"key": "key", "value": "value", "type": "person"}],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6675: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total, breakdown_value FROM (\n    SELECT SUM(total) as ...art, breakdown_value\n    ORDER BY breakdown_value, day_start\n)\nGROUP BY breakdown_value\nORDER BY breakdown_value\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event': 'sign up', 'key': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS day_start, replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') AS breakdown_value FROM events AS e WHERE (team_id = 832) AND (event = 'sign up') AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', '')) AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND notEmpty(person_id) AND (replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '') IN ['finance']) AND notEmpty(person_id) GROUP BY day_start, breakdown_value', required columns: 'team_id' 'event' 'person_properties' 'timestamp' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'person_properties', 'timestamp', 'person_id' or 'group0_properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               21. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               22. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               23. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               24. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               25. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               26. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               27. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               28. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               29. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               30. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               31. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________________ TestTrends.test_breakdown_with_filter_groups _________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebead0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '833_None_8QwEliwb'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebead0>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '833_None_8QwEliwb', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebead0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31e5c22c0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebead0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebead0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 833) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups>

    def test_breakdown_with_filter_groups(self):
        self._create_groups()
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
        )
    
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 833) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
________ TestTrends.test_breakdown_with_filter_groups_person_on_events _________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682179851504'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '834_None_4Oo1PPd8', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682179851504'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '834_None_4Oo1PPd8', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682179851504'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '834_None_4Oo1PPd8', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '834_None_4Oo1PPd8', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe770>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '834_None_4Oo1PPd8'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe770>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'industry\'), \'^"|"$\', \'\')))\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '834_None_4Oo1PPd8', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe770>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31ed1e650>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe770>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe770>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 834) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups_person_on_events>

    @also_test_with_materialized_columns(
        event_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_breakdown_with_filter_groups_person_on_events(self):
        self._create_groups()
    
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
        )
        _create_event(
            event="sign up",
            distinct_id="person1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
        )
    
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6301: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...^"|"$\', \'\'))) \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e WHERE (team_id = 834) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_______ TestTrends.test_breakdown_with_filter_groups_person_on_events_v2 _______

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...mpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682171217424'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '835_None_3h4SzSy5', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682171217424'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '835_None_3h4SzSy5', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682171217424'>
args = ('\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$...\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n',)
kwargs = {'params': None, 'query_id': '835_None_3h4SzSy5', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '835_None_3h4SzSy5', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe650>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '835_None_3h4SzSy5'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe650>
query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...\'\'))) AND notEmpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '835_None_3h4SzSy5', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe650>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31ed76860>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe650>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebe650>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e ALL LEFT JOIN (SELECT argMax(override_person_id, version) AS person_id, old_person_id FROM person_overrides WHERE team_id = 835 GROUP BY old_person_id) AS overrides ON person_id = old_person_id WHERE (team_id = 835) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND notEmpty(person_id) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'person_id' 'team_id' 'event' 'old_person_id' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'person_id', 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties', joined columns: 'overrides.person_id' 'old_person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_breakdown_with_filter_groups_person_on_events_v2>

    @override_settings(PERSON_ON_EVENTS_V2_OVERRIDE=True)
    @snapshot_clickhouse_queries
    def test_breakdown_with_filter_groups_person_on_events_v2(self):
        self._create_groups()
    
        id1 = str(uuid.uuid4())
        id2 = str(uuid.uuid4())
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "oh", "$group_0": "org:7", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:5"},
            timestamp="2020-01-02T12:00:01Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d1",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
            person_id=id1,
        )
        _create_event(
            event="sign up",
            distinct_id="test_breakdown_d2",
            team=self.team,
            properties={"key": "uh", "$group_0": "org:6"},
            timestamp="2020-01-02T12:00:02Z",
            person_id=id2,
        )
    
        create_person_id_override_by_distinct_id("test_breakdown_d1", "test_breakdown_d2", self.team.pk)
>       response = Trends().run(
            Filter(
                data={
                    "date_from": "2020-01-01T00:00:00Z",
                    "date_to": "2020-01-12T00:00:00Z",
                    "breakdown": "key",
                    "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0, "math": "dau"}],
                    "properties": [{"key": "industry", "value": "finance", "type": "group", "group_type_index": 0}],
                }
            ),
            self.team,
        )

posthog/queries/test/test_trends.py:6361: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:126: in _run_query
    query_type, sql, params, parse_function = self._get_sql_for_entity(adjusted_filter, team, entity)
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        replaceRegexpAll(JSONExtractRaw(properties, \'key\'), \'^"|"$\...mpty(e.person_id)\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'ke_brkdwn_0': 'industry', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '') AS value, count() AS count FROM events AS e ALL LEFT JOIN (SELECT argMax(override_person_id, version) AS person_id, old_person_id FROM person_overrides WHERE team_id = 835 GROUP BY old_person_id) AS overrides ON person_id = old_person_id WHERE (team_id = 835) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2020-01-01 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND notEmpty(person_id) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'person_id' 'team_id' 'event' 'old_person_id' 'timestamp' 'group_properties_0' 'properties', maybe you meant: 'person_id', 'team_id', 'event', 'timestamp', 'group0_properties' or 'properties', joined columns: 'overrides.person_id' 'old_person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
________ TestTrends.test_filtering_by_multiple_groups_person_on_events _________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682181648272'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '836_None_sNHRCiPx', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682181648272'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '836_None_sNHRCiPx', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682181648272'>
args = ('\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n',)
kwargs = {'params': None, 'query_id': '836_None_sNHRCiPx', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = ()
kwargs = {'params': None, 'query_id': '836_None_sNHRCiPx', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf670>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '836_None_sNHRCiPx'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf670>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '836_None_sNHRCiPx', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf670>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f76abf0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf670>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf670>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' 'group_properties_2' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 836) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['six'], replaceRegexpAll(JSONExtractRaw(group_properties_2, 'name'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'group_properties_2' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f134ac45609 in ?
E           30. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_by_multiple_groups_person_on_events>

    @also_test_with_materialized_columns(
        group_properties=[(0, "industry"), (2, "name")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_filtering_by_multiple_groups_person_on_events(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        GroupTypeMapping.objects.create(team=self.team, group_type="company", group_type_index=2)
    
        create_group(team_id=self.team.pk, group_type_index=0, group_key="org:5", properties={"industry": "finance"})
        create_group(team_id=self.team.pk, group_type_index=0, group_key="org:6", properties={"industry": "technology"})
        create_group(team_id=self.team.pk, group_type_index=2, group_key="company:5", properties={"name": "five"})
        create_group(team_id=self.team.pk, group_type_index=2, group_key="company:6", properties={"name": "six"})
    
        journey = {
            "person1": [
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12),
                    "properties": {"$group_0": "org:5", "$group_2": "company:6"},
                },
                {
                    "event": "sign up",
                    "timestamp": datetime(2020, 1, 2, 12, 30),
                    "properties": {"$group_2": "company:6"},
                },
                {"event": "sign up", "timestamp": datetime(2020, 1, 2, 13), "properties": {"$group_0": "org:6"}},
                {"event": "sign up", "timestamp": datetime(2020, 1, 3, 15), "properties": {"$group_2": "company:5"}},
            ]
        }
    
        journeys_for(events_by_person=journey, team=self.team)
    
        filter = Filter(
            data={
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12",
                "events": [{"id": "sign up", "name": "sign up", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "name", "value": "six", "type": "group", "group_type_index": 2},
                ],
            }
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6774: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': 'sign up', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' 'group_properties_2' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 836) AND (event = 'sign up') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['six'], replaceRegexpAll(JSONExtractRaw(group_properties_2, 'name'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'group_properties_2' 'person_id' 'group_properties_0', maybe you meant: 'team_id', 'event', 'timestamp', 'group0_properties' or 'person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f134ac45609 in ?
E               30. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
__________________ TestTrends.test_filtering_with_group_props __________________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf5b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '837_None_85xcDDKG'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf5b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '837_None_85xcDDKG', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf5b0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f49fee0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf5b0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf5b0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 837 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 837))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 837) AND (id IN (SELECT id FROM person WHERE (team_id = 837) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 837) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f134ac45609 in ?
E           30. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props>

    def test_filtering_with_group_props(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6581: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 837 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 837))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 837) AND (id IN (SELECT id FROM person WHERE (team_id = 837) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 837) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f134ac45609 in ?
E               30. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_____ TestTrends.test_filtering_with_group_props_event_with_no_group_data ______

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf8b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '838_None_sVO4MNRd'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf8b0>
query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...0, \'industry\'), \'^"|"$\', \'\'))))\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
params = None, with_column_types = False, external_tables = None
query_id = '838_None_sVO4MNRd', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf8b0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff32022dc00>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf8b0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf8b0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 838 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 838))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 838) AND (id IN (SELECT id FROM person WHERE (team_id = 838) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 838) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f134ac45609 in ?
E           30. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props_event_with_no_group_data>

    def test_filtering_with_group_props_event_with_no_group_data(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {
                        "key": "industry",
                        "operator": "is_not",
                        "value": "textiles",
                        "type": "group",
                        "group_type_index": 0,
                    },
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
>       response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...^"|"$\', \'\'))))\n            \n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM (SELECT event, team_id, timestamp, e.distinct_id AS `--e.distinct_id`, pdi.person_id AS `--pdi.person_id`, pdi.distinct_id AS `--pdi.distinct_id` FROM events AS e INNER JOIN (SELECT distinct_id, argMax(person_id, version) AS person_id FROM person_distinct_id2 WHERE team_id = 838 GROUP BY distinct_id HAVING argMax(is_deleted, version) = 0) AS pdi ON `--e.distinct_id` = `--pdi.distinct_id` HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND ((toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND ((event = '$pageview') AND (team_id = 838))))) AS `--.s` ALL INNER JOIN (SELECT id FROM person WHERE (team_id = 838) AND (id IN (SELECT id FROM person WHERE (team_id = 838) AND has(['value'], replaceRegexpAll(JSONExtractRaw(properties, 'key'), '^"|"$', '')))) GROUP BY id HAVING (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) AND ((max(is_deleted) = 0) AND has(['value'], replaceRegexpAll(JSONExtractRaw(argMax(person.properties, version), 'key'), '^"|"$', '')))) AS person ON id = `--pdi.person_id` WHERE (team_id = 838) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (NOT has(['textiles'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', ''))) GROUP BY date', required columns: 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0' 'id' 'team_id' 'event' '--pdi.person_id' 'timestamp' 'group_properties_0', joined columns: 'id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f134ac45609 in ?
E               30. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_________ TestTrends.test_filtering_with_group_props_person_on_events __________

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163303520'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '839_None_0TOwbvLJ', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163303520'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '839_None_0TOwbvLJ', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163303520'>
args = ("\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start...            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n",)
kwargs = {'params': None, 'query_id': '839_None_0TOwbvLJ', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
args = ()
kwargs = {'params': None, 'query_id': '839_None_0TOwbvLJ', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...out_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf730>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '839_None_0TOwbvLJ'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf730>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...\n            AND notEmpty(e.person_id)\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '839_None_0TOwbvLJ', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf730>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31e5eb760>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf730>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf730>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 839) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'person_properties' 'group_properties_0' 'person_id', maybe you meant: 'team_id', 'event', 'timestamp', 'person_properties', 'group0_properties' or 'person_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           29. ? @ 0x00007f134ac45609 in ?
E           30. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_filtering_with_group_props_person_on_events>

    @also_test_with_materialized_columns(
        person_properties=["key"], group_properties=[(0, "industry")], materialize_only_with_person_on_events=True
    )
    @snapshot_clickhouse_queries
    def test_filtering_with_group_props_person_on_events(self):
        self._create_groups()
    
        Person.objects.create(team_id=self.team.pk, distinct_ids=["person1"], properties={"key": "value"})
        _create_event(event="$pageview", distinct_id="person1", team=self.team, timestamp="2020-01-02T12:00:00Z")
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:5"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6"},
            timestamp="2020-01-02T12:00:00Z",
        )
        _create_event(
            event="$pageview",
            distinct_id="person1",
            team=self.team,
            properties={"$group_0": "org:6", "$group_1": "company:10"},
            timestamp="2020-01-02T12:00:00Z",
        )
    
        filter = Filter(
            {
                "date_from": "2020-01-01T00:00:00Z",
                "date_to": "2020-01-12T00:00:00Z",
                "events": [{"id": "$pageview", "type": "events", "order": 0}],
                "properties": [
                    {"key": "industry", "value": "finance", "type": "group", "group_type_index": 0},
                    {"key": "key", "value": "value", "type": "person"},
                ],
            },
            team=self.team,
        )
    
        with override_instance_config("PERSON_ON_EVENTS_ENABLED", True):
>           response = Trends().run(filter, self.team)

posthog/queries/test/test_trends.py:6726: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/trends.py:232: in run
    result.extend(handle_compare(filter, self._run_query, team, entity=entity))
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/trends.py:131: in _run_query
    result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...      AND notEmpty(e.person_id)\n        \nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n'
args = {'date_from': '2020-01-01 00:00:00', 'date_to': '2020-01-12 23:59:59', 'event_0': '$pageview', 'interval': 'day', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ax_ignored_errors":1000,"timeout_before_checking_execution_speed":60}}', 'timeout_before_checking_execution_speed': 60}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'group_properties_0' while processing query: 'SELECT count() AS total, toStartOfDay(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date FROM events AS e WHERE (team_id = 839) AND (event = '$pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime(toStartOfDay(toDateTime('2020-01-01 00:00:00', 'UTC')), 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-12 23:59:59', 'UTC')) AND (has(['finance'], replaceRegexpAll(JSONExtractRaw(group_properties_0, 'industry'), '^"|"$', '')) AND has(['value'], replaceRegexpAll(JSONExtractRaw(person_properties, 'key'), '^"|"$', ''))) AND notEmpty(person_id) GROUP BY date', required columns: 'team_id' 'event' 'timestamp' 'person_properties' 'group_properties_0' 'person_id', maybe you meant: 'team_id', 'event', 'timestamp', 'person_properties', 'group0_properties' or 'person_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               15. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               16. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               18. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               19. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               20. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               21. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               22. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               23. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               24. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               25. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               26. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               27. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               28. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               29. ? @ 0x00007f134ac45609 in ?
E               30. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
____________________ TestTrends.test_trends_with_hogql_math ____________________
posthog/test/base.py:802: in wrapped
    self.assertQueryMatchesSnapshot(query)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <posthog.queries.test.test_trends.TestTrends testMethod=test_trends_with_hogql_math>
query = "\nSELECT groupArray(day_start) as date, groupArray(count) AS total FROM (\n    SELECT SUM(total) AS count, day_start\...eTime('2020-01-04 23:59:59', 'UTC')\n\n\n\nGROUP BY date\n\n    )\n    GROUP BY day_start\n    ORDER BY day_start\n)\n"
params = None, replace_all_numbers = False

    def assertQueryMatchesSnapshot(self, query, params=None, replace_all_numbers=False):
        # :TRICKY: team_id changes every test, avoid it messing with snapshots.
        if replace_all_numbers:
            query = re.sub(r"(\"?) = \d+", r"\1 = 2", query)
            query = re.sub(r"(\"?) IN \(\d+(, \d+)*\)", r"\1 IN (1, 2, 3, 4, 5 /* ... */)", query)
            # feature flag conditions use primary keys as columns in queries, so replace those too
            query = re.sub(r"flag_\d+_condition", r"flag_X_condition", query)
            query = re.sub(r"flag_\d+_super_condition", r"flag_X_super_condition", query)
        else:
            query = re.sub(r"(team|cohort)_id(\"?) = \d+", r"\1_id\2 = 2", query)
            query = re.sub(r"\d+ as (team|cohort)_id(\"?)", r"2 as \1_id\2", query)
    
        # hog ql checks team ids differently
        query = re.sub(
            r"equals\(([^.]+\.)?team_id?, \d+\)",
            r"equals(\1team_id, 2)",
            query,
        )
    
        # Replace organization_id and notebook_id lookups, for postgres
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") = '[^']+'::uuid""",
            r"""\1 = '00000000-0000-0000-0000-000000000000'::uuid""",
            query,
        )
        query = re.sub(
            rf"""("organization_id"|"posthog_organization"\."id"|"posthog_notebook"."id") IN \('[^']+'::uuid\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid)""",
            query,
        )
    
        # Replace notebook short_id lookups, for postgres
        query = re.sub(
            r"\"posthog_notebook\".\"short_id\" = '[a-zA-Z0-9]{8}'",
            '"posthog_notebook"."short_id" = \'00000000\'',
            query,
        )
    
        # Replace person id (when querying session recording replay events)
        query = re.sub(
            "and person_id = '[0-9A-Fa-f]{8}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{4}-[0-9A-Fa-f]{12}'",
            r"and person_id = '00000000-0000-0000-0000-000000000000'",
            query,
        )
    
        # Replace tag id lookups for postgres
        query = re.sub(
            rf"""("posthog_tag"\."id") IN \(('[^']+'::uuid)+(, ('[^']+'::uuid)+)*\)""",
            r"""\1 IN ('00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid, '00000000-0000-0000-0000-000000000000'::uuid /* ... */)""",
            query,
        )
    
        query = re.sub(rf"""user_id:([0-9]+) request:[a-zA-Z0-9-_]+""", r"""user_id:0 request:_snapshot_""", query)
    
        # ee license check has varying datetime
        # e.g. WHERE "ee_license"."valid_until" >= '2023-03-02T21:13:59.298031+00:00'::timestamptz
        query = re.sub(
            r"ee_license\"\.\"valid_until\" >= '\d{4}-\d\d-\d\dT\d\d:\d\d:\d\d.\d{6}\+\d\d:\d\d'::timestamptz",
            '"ee_license"."valid_until">=\'LICENSE-TIMESTAMP\'::timestamptz"',
            query,
        )
    
        # insight cache key varies with team id
        query = re.sub(
            r"WHERE \(\"posthog_insightcachingstate\".\"cache_key\" = 'cache_\w{32}'",
            """WHERE ("posthog_insightcachingstate"."cache_key" = 'cache_THE_CACHE_KEY'""",
            query,
        )
    
        # replace Savepoint numbers
        query = re.sub(r"SAVEPOINT \".+\"", "SAVEPOINT _snapshot_", query)
    
        # test_formula has some values that change on every run
        query = re.sub(r"\SELECT \[\d+, \d+] as breakdown_value", "SELECT [1, 2] as breakdown_value", query)
        query = re.sub(
            r"SELECT distinct_id,[\n\r\s]+\d+ as value",
            "SELECT distinct_id, 1 as value",
            query,
        )
    
>       assert sqlparse.format(query, reindent=True) == self.snapshot, "\n".join(self.snapshot.get_assert_diff())
E       AssertionError: [0m[2m  '[0m[0m
E       [0m[2m             ...[0m[0m
E       [0m[2m                         toStartOfWeek(toDateTime('2019-12-28 00:00:00', 'UTC'))[0m[0m
E       [0m[48;5;225m[38;5;90m-[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mU[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mO[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mS[0m[48;5;225m[38;5;90mE[0m[48;5;225m[38;5;90mL[0m[48;5;225m[38;5;90mE[0m[48;5;225m[38;5;90mC[0m[48;5;225m[38;5;90mT[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mp[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90mv[0m[48;5;225m[38;5;90mg[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90m6[0m[48;5;225m[38;5;90m4[0m[48;5;225m[38;5;90mO[0m[48;5;225m[38;5;90mr[0m[48;5;225m[38;5;90mN[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90mI[0m[48;5;225m[38;5;90mf[0m[48;5;225m[38;5;90m([0m[48;5;225m[38;5;90me[0m[48;5;90m[38;5;225mv[0m[48;5;225m[38;5;90me[0m[48;5;90m[38;5;225mn[0m[48;5;90m[38;5;225mt[0m[48;5;225m[38;5;90ms[0m[48;5;90m[38;5;225m.[0m[48;5;90m[38;5;225m`[0m[48;5;225m[38;5;90m$[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90me[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90ms[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90m_[0m[48;5;225m[38;5;90mi[0m[48;5;225m[38;5;90md[0m[48;5;90m[38;5;225m`[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90mn[0m[48;5;225m[38;5;90mu[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m'[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m,[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90m1[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m0[0m[48;5;225m[38;5;90m)[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mA[0m[48;5;225m[38;5;90mS[0m[48;5;225m[38;5;90m [0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90mo[0m[48;5;225m[38;5;90mt[0m[48;5;225m[38;5;90ma[0m[48;5;225m[38;5;90ml[0m[48;5;225m[38;5;90m,[0m[0m
E       [0m[48;5;195m[38;5;23m+[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mU[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mL[0m[48;5;195m[38;5;23mE[0m[48;5;195m[38;5;23mC[0m[48;5;195m[38;5;23mT[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mp[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23mv[0m[48;5;195m[38;5;23mg[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23m6[0m[48;5;195m[38;5;23m4[0m[48;5;195m[38;5;23mO[0m[48;5;195m[38;5;23mr[0m[48;5;195m[38;5;23mN[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m([0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mc[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mR[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mg[0m[48;5;23m[38;5;195me[0m[48;5;23m[38;5;195mx[0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195mA[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195ml[0m[48;5;23m[38;5;195m([0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23m([0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23mI[0m[48;5;195m[38;5;23mf[0m[48;5;195m[38;5;23m([0m[48;5;23m[38;5;195mJ[0m[48;5;23m[38;5;195mS[0m[48;5;23m[38;5;195mO[0m[48;5;23m[38;5;195mN[0m[48;5;23m[38;5;195mE[0m[48;5;23m[38;5;195mx[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mc[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mR[0m[48;5;23m[38;5;195ma[0m[48;5;23m[38;5;195mw[0m[48;5;23m[38;5;195m([0m[48;5;23m[38;5;195mp[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195mo[0m[48;5;23m[38;5;195mp[0m[48;5;195m[38;5;23me[0m[48;5;23m[38;5;195mr[0m[48;5;23m[38;5;195mt[0m[48;5;23m[38;5;195mi[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;195m[38;5;23m$[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23me[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23ms[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23m_[0m[48;5;195m[38;5;23mi[0m[48;5;195m[38;5;23md[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23mn[0m[48;5;195m[38;5;23mu[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m'[0m[48;5;195m[38;5;23m)[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m^[0m[48;5;23m[38;5;195m"[0m[48;5;23m[38;5;195m|[0m[48;5;23m[38;5;195m"[0m[48;5;23m[38;5;195m$[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m,[0m[48;5;23m[38;5;195m [0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m'[0m[48;5;23m[38;5;195m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m,[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23m1[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m0[0m[48;5;195m[38;5;23m)[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mA[0m[48;5;195m[38;5;23mS[0m[48;5;195m[38;5;23m [0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23mo[0m[48;5;195m[38;5;23mt[0m[48;5;195m[38;5;23ma[0m[48;5;195m[38;5;23ml[0m[48;5;195m[38;5;23m,[0m[0m
E       [0m[2m                         toStartOfWeek(toTimeZone(toDateTime(timestamp, 'UTC'), 'UTC')) AS date[0m[0m
E       [0m[2m             ...[0m[0m
E       [0m[2m  '[0m[0m

posthog/test/base.py:452: AssertionError
_______________________ TestFormula.test_breakdown_hogql _______________________

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2019-12-28 00:00:00', 'date_to': '2020-01-04 23:59:59', 'e_0_math_prop': 'session duration', 'event_None': 'session start', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163129264'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '841_None_CecduUZY', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163129264'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '841_None_CecduUZY', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682163129264'>
args = ("\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExt...e('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n",)
kwargs = {'params': None, 'query_id': '841_None_CecduUZY', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
args = ()
kwargs = {'params': None, 'query_id': '841_None_CecduUZY', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf7f0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '841_None_CecduUZY'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf7f0>
query = "\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr...ime('2020-01-04 23:59:59', 'UTC')\n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT 25 OFFSET 0\n)\n"
params = None, with_column_types = False, external_tables = None
query_id = '841_None_CecduUZY', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf7f0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31ed1db10>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf7f0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebf7f0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, '$some_prop'), ''), 'null'), '^"|"$', '')), ''), ' : ', ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'location'), ''), 'null'), '^"|"$', '')), '')) AS value, sum(toFloat64OrNull(replaceRegexpAll(JSONExtractRaw(properties, 'session duration'), '^"|"$', ''))) AS count FROM events AS e WHERE (team_id = 841) AND (event = 'session start') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2019-12-28 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-04 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'person_props' 'properties', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E           9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E           10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E           11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E           12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           23. ? @ 0x00007f134ac45609 in ?
E           24. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <test_formula.TestFormula testMethod=test_breakdown_hogql>

    @snapshot_clickhouse_queries
    def test_breakdown_hogql(self):
>       response = self._run(
            {"breakdown": "concat(person.properties.$some_prop, ' : ', properties.location)", "breakdown_type": "hogql"}
        )

posthog/queries/trends/test/test_formula.py:380: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/trends/test/test_formula.py:97: in _run
    action_response = Trends().run(
posthog/queries/trends/trends.py:220: in run
    return handle_compare(filter, self._run_formula_query, team)
posthog/queries/base.py:72: in handle_compare
    base_entitites = func(filter=filter, team=team, **kwargs)
posthog/queries/trends/formula.py:27: in _run_formula_query
    _, sql, entity_params, _ = self._get_sql_for_entity(filter, team, entity)  # type: ignore
posthog/queries/trends/trends.py:40: in _get_sql_for_entity
    ).get_query()
posthog/queries/trends/breakdown.py:197: in get_query
    _params, breakdown_filter, _breakdown_filter_params, breakdown_value = self._breakdown_prop_params(
posthog/queries/trends/breakdown.py:394: in _breakdown_prop_params
    values_arr = get_breakdown_prop_values(
posthog/queries/breakdown_props.py:201: in get_breakdown_prop_values
    return insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT groupArray(value) FROM (\n    SELECT\n        concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtr..., %(timezone)s)  \n    GROUP BY value\n    ORDER BY count DESC, value DESC\n    LIMIT %(limit)s OFFSET %(offset)s\n)\n'
args = {'date_from': '2019-12-28 00:00:00', 'date_to': '2020-01-04 23:59:59', 'e_0_math_prop': 'session duration', 'event_None': 'session start', ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: 'person_props' while processing query: 'SELECT concat(ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(person_props, '$some_prop'), ''), 'null'), '^"|"$', '')), ''), ' : ', ifNull(toString(replaceRegexpAll(nullIf(nullIf(JSONExtractRaw(properties, 'location'), ''), 'null'), '^"|"$', '')), '')) AS value, sum(toFloat64OrNull(replaceRegexpAll(JSONExtractRaw(properties, 'session duration'), '^"|"$', ''))) AS count FROM events AS e WHERE (team_id = 841) AND (event = 'session start') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2019-12-28 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2020-01-04 23:59:59', 'UTC')) GROUP BY value ORDER BY count DESC, value DESC LIMIT 0, 25', required columns: 'team_id' 'event' 'timestamp' 'person_props' 'properties', maybe you meant: 'team_id', 'event', 'timestamp' or 'properties'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context const>, bool, bool) @ 0x0000000013aead26 in /usr/bin/clickhouse
E               9. DB::getDatabaseAndTablesWithColumns(std::vector<DB::ASTTableExpression const*, std::allocator<DB::ASTTableExpression const*>> const&, std::shared_ptr<DB::Context const>, bool, bool, bool) @ 0x0000000013e346ad in /usr/bin/clickhouse
E               10. DB::JoinedTables::resolveTables() @ 0x0000000013b3178d in /usr/bin/clickhouse
E               11. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a411c5 in /usr/bin/clickhouse
E               12. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               13. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               14. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               15. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               16. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               17. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               18. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               19. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               20. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               21. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               22. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               23. ? @ 0x00007f134ac45609 in ?
E               24. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
____________ TestPerson.test_group_query_includes_recording_events _____________

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session...RDER BY actor_value DESC, actor_id DESC /* Also sorting by ID for determinism */\nLIMIT %(limit)s\nOFFSET %(offset)s\n'
args = {'date_from': '2021-01-21 00:00:00', 'date_to': '2021-01-21 23:59:59', 'event_0': 'pageview', 'limit': 100, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
>               result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )

posthog/clickhouse/client/execute.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682184186592'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '842_None_0YFjU0Sj', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def __call__(self, /, *args, **kwargs):
        # can't use self in-case a function / method we are mocking uses self
        # in the signature
        self._mock_check_sig(*args, **kwargs)
        self._increment_mock_call(*args, **kwargs)
>       return self._mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682184186592'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '842_None_0YFjU0Sj', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def _mock_call(self, /, *args, **kwargs):
>       return self._execute_mock_call(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <MagicMock name='execute' id='140682184186592'>
args = ('\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $sessio...ND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n',)
kwargs = {'params': None, 'query_id': '842_None_0YFjU0Sj', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}
effect = None

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method
    
        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
                result = next(effect)
                if _is_exception(result):
                    raise result
            else:
                result = effect(*args, **kwargs)
    
            if result is not DEFAULT:
                return result
    
        if self._mock_return_value is not DEFAULT:
            return self.return_value
    
        if self._mock_wraps is not None:
>           return self._mock_wraps(*args, **kwargs)

/usr/local/lib/python3.10/unittest/mock.py:1188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
args = ()
kwargs = {'params': None, 'query_id': '842_None_0YFjU0Sj', 'settings': {'distributed_replica_max_ignored_errors': 1000, 'join_a...{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}, 'with_column_types': False}

    def execute_wrapper(query, *args, **kwargs):
        if sqlparse.format(query, strip_comments=True).strip().startswith(query_prefixes):
            queries.append(query)
>       return original_client_execute(query, *args, **kwargs)

posthog/test/base.py:673: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebfbb0>
query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, with_column_types = False, external_tables = None
query_id = '842_None_0YFjU0Sj'
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
types_check = False, columnar = False

    def execute(self, query, params=None, with_column_types=False,
                external_tables=None, query_id=None, settings=None,
                types_check=False, columnar=False):
        """
        Executes query.
    
        Establishes new connection if it wasn't established yet.
        After query execution connection remains intact for next queries.
        If connection can't be reused it will be closed and new connection will
        be created.
    
        :param query: query that will be send to server.
        :param params: substitution parameters for SELECT queries and data for
                       INSERT queries. Data for INSERT can be `list`, `tuple`
                       or :data:`~types.GeneratorType`.
                       Defaults to ``None`` (no parameters  or data).
        :param with_column_types: if specified column names and types will be
                                  returned alongside with result.
                                  Defaults to ``False``.
        :param external_tables: external tables to send.
                                Defaults to ``None`` (no external tables).
        :param query_id: the query identifier. If no query id specified
                         ClickHouse server will generate it.
        :param settings: dictionary of query settings.
                         Defaults to ``None`` (no additional settings).
        :param types_check: enables type checking of data for INSERT queries.
                            Causes additional overhead. Defaults to ``False``.
        :param columnar: if specified the result of the SELECT query will be
                         returned in column-oriented form.
                         It also allows to INSERT data in columnar form.
                         Defaults to ``False`` (row-like form).
    
        :return: * number of inserted rows for INSERT queries with data.
                   Returning rows count from INSERT FROM SELECT is not
                   supported.
                 * if `with_column_types=False`: `list` of `tuples` with
                   rows/columns.
                 * if `with_column_types=True`: `tuple` of 2 elements:
                    * The first element is `list` of `tuples` with
                      rows/columns.
                    * The second element information is about columns: names
                      and types.
        """
    
        start_time = time()
    
        with self.disconnect_on_error(query, settings):
            # INSERT queries can use list/tuple/generator of list/tuples/dicts.
            # For SELECT parameters can be passed in only in dict right now.
            is_insert = isinstance(params, (list, tuple, types.GeneratorType))
    
            if is_insert:
                rv = self.process_insert_query(
                    query, params, external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )
            else:
>               rv = self.process_ordinary_query(
                    query, params=params, with_column_types=with_column_types,
                    external_tables=external_tables,
                    query_id=query_id, types_check=types_check,
                    columnar=columnar
                )

env/lib/python3.10/site-packages/clickhouse_driver/client.py:304: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebfbb0>
query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session... AND "$group_0" != \'\'\n        )\nGROUP BY actor_id\nORDER BY actor_value DESC, actor_id DESC\nLIMIT 100\nOFFSET 0\n'
params = None, with_column_types = False, external_tables = None
query_id = '842_None_0YFjU0Sj', types_check = False, columnar = False

    def process_ordinary_query(
            self, query, params=None, with_column_types=False,
            external_tables=None, query_id=None,
            types_check=False, columnar=False):
    
        if params is not None:
            query = self.substitute_params(
                query, params, self.connection.context
            )
    
        self.connection.send_query(query, query_id=query_id)
        self.connection.send_external_tables(external_tables,
                                             types_check=types_check)
>       return self.receive_result(with_column_types=with_column_types,
                                   columnar=columnar)

env/lib/python3.10/site-packages/clickhouse_driver/client.py:491: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebfbb0>
with_column_types = False, progress = False, columnar = False

    def receive_result(self, with_column_types=False, progress=False,
                       columnar=False):
    
        gen = self.packet_generator()
    
        if progress:
            return self.progress_query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
    
        else:
            result = self.query_result_cls(
                gen, with_column_types=with_column_types, columnar=columnar
            )
>           return result.get_result()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.result.QueryResult object at 0x7ff31f349ae0>

    def get_result(self):
        """
        :return: stored query result.
        """
    
>       for packet in self.packet_generator:

env/lib/python3.10/site-packages/clickhouse_driver/result.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebfbb0>

    def packet_generator(self):
        while True:
            try:
>               packet = self.receive_packet()

env/lib/python3.10/site-packages/clickhouse_driver/client.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <clickhouse_driver.client.Client object at 0x7ff348ebfbb0>

    def receive_packet(self):
        packet = self.connection.receive_packet()
    
        if packet.type == ServerPacketTypes.EXCEPTION:
>           raise packet.exception
E           clickhouse_driver.errors.ServerException: Code: 47.
E           DB::Exception: Missing columns: '$group_0' while processing query: 'SELECT `$group_0` AS actor_id, count() AS actor_value, groupUniqArray(100)((timestamp, uuid, `$session_id`, `$window_id`)) AS matching_events FROM (SELECT e.timestamp AS timestamp, replaceRegexpAll(JSONExtractRaw(e.properties, '$window_id'), '^"|"$', '') AS `$window_id`, replaceRegexpAll(JSONExtractRaw(e.properties, '$session_id'), '^"|"$', '') AS `$session_id`, e.uuid AS uuid FROM events AS e WHERE (team_id = 842) AND (event = 'pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2021-01-21 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2021-01-21 23:59:59', 'UTC')) AND (`$group_0` != '')) GROUP BY actor_id ORDER BY actor_value DESC, actor_id DESC LIMIT 0, 100', required columns: '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id' '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id'. Stack trace:
E           
E           0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E           1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E           2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E           3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E           4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E           5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E           6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E           7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E           8. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E           9. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E           10. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E           11. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E           12. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E           13. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E           14. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E           15. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E           16. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E           17. ? @ 0x00007f134ac45609 in ?
E           18. __clone @ 0x00007f134ab6a133 in ?

env/lib/python3.10/site-packages/clickhouse_driver/client.py:184: ServerException

During handling of the above exception, another exception occurred:

self = <test_person.TestPerson testMethod=test_group_query_includes_recording_events>

    @snapshot_clickhouse_queries
    @freeze_time("2021-01-21T20:00:00.000Z")
    def test_group_query_includes_recording_events(self):
        GroupTypeMapping.objects.create(team=self.team, group_type="organization", group_type_index=0)
        create_group(team_id=self.team.pk, group_type_index=0, group_key="bla", properties={})
        create_session_recording_events(self.team.pk, timezone.now(), "u1", "s1")
    
        _create_event(
            event="pageview", distinct_id="u1", team=self.team, timestamp=timezone.now(), properties={"$group_0": "bla"}
        )
        _create_event(
            event="pageview",
            distinct_id="u1",
            team=self.team,
            timestamp=timezone.now() + relativedelta(hours=2),
            properties={"$session_id": "s1", "$window_id": "w1", "$group_0": "bla"},
            event_uuid="b06e5a5e-e001-4293-af81-ac73e194569d",
        )
    
        event = {
            "id": "pageview",
            "name": "pageview",
            "type": "events",
            "order": 0,
            "math": "unique_group",
            "math_group_type_index": 0,
        }
    
        filter = Filter(
            data={
                "date_from": "2021-01-21T00:00:00Z",
                "date_to": "2021-01-21T23:59:59Z",
                "events": [event],
                "include_recordings": "true",
            }
        )
        entity = Entity(event)
    
>       _, serialized_actors, _ = TrendsActors(self.team, entity, filter).get_actors()

posthog/queries/trends/test/test_person.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
posthog/queries/actor_base_query.py:102: in get_actors
    raw_result = insight_sync_execute(
posthog/queries/insight.py:15: in insight_sync_execute
    return sync_execute(query, args=args, team_id=team_id, **kwargs)
posthog/utils.py:1263: in inner
    return inner._impl(*args, **kwargs)  # type: ignore
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

query = '\nSELECT\n    $group_0 AS actor_id,\n    count() AS actor_value\n    , groupUniqArray(100)((timestamp, uuid, $session...RDER BY actor_value DESC, actor_id DESC /* Also sorting by ID for determinism */\nLIMIT %(limit)s\nOFFSET %(offset)s\n'
args = {'date_from': '2021-01-21 00:00:00', 'date_to': '2021-01-21 23:59:59', 'event_0': 'pageview', 'limit': 100, ...}
settings = {'distributed_replica_max_ignored_errors': 1000, 'join_algorithm': 'direct,parallel_hash', 'log_comment': '{"trend_vol...ad.DEFAULT","query_settings":{"join_algorithm":"direct,parallel_hash","distributed_replica_max_ignored_errors":1000}}'}
with_column_types = False, flush = True

    @patchable
    def sync_execute(
        query,
        args=None,
        settings=None,
        with_column_types=False,
        flush=True,
        *,
        workload: Workload = Workload.DEFAULT,
        team_id: Optional[int] = None,
        readonly=False,
    ):
        if TEST and flush:
            try:
                from posthog.test.base import flush_persons_and_events
    
                flush_persons_and_events()
            except ModuleNotFoundError:  # when we run plugin server tests it tries to run above, ignore
                pass
    
        with get_pool(workload, team_id, readonly).get_client() as client:
            start_time = perf_counter()
    
            prepared_sql, prepared_args, tags = _prepare_query(client=client, query=query, args=args, workload=workload)
            query_id = validated_client_query_id()
            core_settings = {**default_settings(), **(settings or {})}
            tags["query_settings"] = core_settings
            settings = {**core_settings, "log_comment": json.dumps(tags, separators=(",", ":"))}
            try:
                result = client.execute(
                    prepared_sql,
                    params=prepared_args,
                    settings=settings,
                    with_column_types=with_column_types,
                    query_id=query_id,
                )
            except Exception as err:
                err = wrap_query_error(err)
                statsd.incr("clickhouse_sync_execution_failure", tags={"failed": True, "reason": type(err).__name__})
    
>               raise err
E               posthog.errors.CHQueryErrorUnknownIdentifier: Code: 47.
E               DB::Exception: Missing columns: '$group_0' while processing query: 'SELECT `$group_0` AS actor_id, count() AS actor_value, groupUniqArray(100)((timestamp, uuid, `$session_id`, `$window_id`)) AS matching_events FROM (SELECT e.timestamp AS timestamp, replaceRegexpAll(JSONExtractRaw(e.properties, '$window_id'), '^"|"$', '') AS `$window_id`, replaceRegexpAll(JSONExtractRaw(e.properties, '$session_id'), '^"|"$', '') AS `$session_id`, e.uuid AS uuid FROM events AS e WHERE (team_id = 842) AND (event = 'pageview') AND (toTimeZone(timestamp, 'UTC') >= toDateTime('2021-01-21 00:00:00', 'UTC')) AND (toTimeZone(timestamp, 'UTC') <= toDateTime('2021-01-21 23:59:59', 'UTC')) AND (`$group_0` != '')) GROUP BY actor_id ORDER BY actor_value DESC, actor_id DESC LIMIT 0, 100', required columns: '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id' '$group_0' '$session_id' 'timestamp' 'uuid' '$window_id'. Stack trace:
E               
E               0. DB::Exception::Exception(DB::Exception::MessageMasked&&, int, bool) @ 0x000000000e1fc3f5 in /usr/bin/clickhouse
E               1. ? @ 0x0000000008877911 in /usr/bin/clickhouse
E               2. DB::TreeRewriterResult::collectUsedColumns(std::shared_ptr<DB::IAST> const&, bool, bool) @ 0x0000000013d79e19 in /usr/bin/clickhouse
E               3. DB::TreeRewriter::analyzeSelect(std::shared_ptr<DB::IAST>&, DB::TreeRewriterResult&&, DB::SelectQueryOptions const&, std::vector<DB::TableWithColumnNamesAndTypes, std::allocator<DB::TableWithColumnNamesAndTypes>> const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::TableJoin>, bool, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>, std::unordered_map<String, String, std::hash<String>, std::equal_to<String>, std::allocator<std::pair<String const, String>>>) const @ 0x0000000013d7ed7d in /usr/bin/clickhouse
E               4. ? @ 0x0000000013a47d72 in /usr/bin/clickhouse
E               5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context> const&, std::optional<DB::Pipe>, std::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&, std::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::shared_ptr<DB::PreparedSets>) @ 0x0000000013a4375d in /usr/bin/clickhouse
E               6. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::shared_ptr<DB::IAST> const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae9b66 in /usr/bin/clickhouse
E               7. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::shared_ptr<DB::IAST> const&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&, std::vector<String, std::allocator<String>> const&) @ 0x0000000013ae7773 in /usr/bin/clickhouse
E               8. DB::InterpreterFactory::get(std::shared_ptr<DB::IAST>&, std::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) @ 0x00000000139fd2be in /usr/bin/clickhouse
E               9. ? @ 0x0000000013e186d6 in /usr/bin/clickhouse
E               10. DB::executeQuery(String const&, std::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x0000000013e14f2e in /usr/bin/clickhouse
E               11. DB::TCPHandler::runImpl() @ 0x0000000014c3f9e4 in /usr/bin/clickhouse
E               12. DB::TCPHandler::run() @ 0x0000000014c55c79 in /usr/bin/clickhouse
E               13. Poco::Net::TCPServerConnection::start() @ 0x0000000017bc9ad4 in /usr/bin/clickhouse
E               14. Poco::Net::TCPServerDispatcher::run() @ 0x0000000017bcacf1 in /usr/bin/clickhouse
E               15. Poco::PooledThread::run() @ 0x0000000017d4d147 in /usr/bin/clickhouse
E               16. Poco::ThreadImpl::runnableEntry(void*) @ 0x0000000017d4ab7c in /usr/bin/clickhouse
E               17. ? @ 0x00007f134ac45609 in ?
E               18. __clone @ 0x00007f134ab6a133 in ?

posthog/clickhouse/client/execute.py:113: CHQueryErrorUnknownIdentifier
_ TestTeam.test_team_on_cloud_uses_feature_flag_to_determine_person_on_events __

self = <posthog.test.test_team.TestTeam testMethod=test_team_on_cloud_uses_feature_flag_to_determine_person_on_events>
mock_feature_enabled = <MagicMock name='feature_enabled' id='140682170402896'>

    @mock.patch("posthoganalytics.feature_enabled", return_value=True)
    def test_team_on_cloud_uses_feature_flag_to_determine_person_on_events(self, mock_feature_enabled):
        with self.is_cloud(True):
            with override_instance_config("PERSON_ON_EVENTS_ENABLED", False):
                team = Team.objects.create_with_data(organization=self.organization)
>               self.assertEqual(team.person_on_events_mode, PersonOnEventsMode.V2_ENABLED)
E               AssertionError: <PersonOnEventsMode.DISABLED: 'disabled'> != <PersonOnEventsMode.V2_ENABLED: 'v2_enabled'>

posthog/test/test_team.py:124: AssertionError
_____________________ TestReset.test_can_reset_export_run ______________________

self = <test_reset.TestReset testMethod=test_can_reset_export_run>

    def test_can_reset_export_run(self):
        """Test calling the reset endpoint to reset a BatchExportRun a couple of times."""
        temporal = sync_connect()
    
        destination_data = {
            "type": "S3",
            "config": {
                "bucket_name": "my-production-s3-bucket",
                "region": "us-east-1",
                "prefix": "posthog-events/",
                "batch_window_size": 3600,
                "aws_access_key_id": "abc123",
                "aws_secret_access_key": "secret",
            },
        }
    
        batch_export_data = {
            "name": "my-production-s3-bucket-destination",
            "destination": destination_data,
            "interval": "hour",
            "trigger_immediately": True,
        }
    
        organization = create_organization("Test Org")
        team = create_team(organization)
        user = create_user("reset.test@user.com", "Reset test user", organization)
        self.client.force_login(user)
    
        with start_test_worker(temporal):
            batch_export = create_batch_export_ok(
                self.client,
                team.pk,
                batch_export_data,
            )
    
>           batch_export_runs = wait_for_runs(self.client, team.pk, batch_export["id"])

posthog/api/test/batch_exports/test_reset.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

client = <django.test.client.Client object at 0x7ff3200200d0>, team_id = 845
batch_export_id = '018944bb-b3e8-0000-58a4-dedc6a20c529', timeout = 10
number_of_runs = 1

    def wait_for_runs(client, team_id, batch_export_id, timeout=10, number_of_runs=1):
        """Wait for BatchExportRuns to be created.
    
        As these rows are created by Temporal, and the worker is running in a separate thread, we allow it
        to take a few seconds.
    
        Raises:
            TimeoutError: If there are less than number_of_runs BatchExportRuns after around timeout seconds.
    
        Returns:
            The BatchExportRuns response.
        """
        start = dt.datetime.utcnow()
        batch_export_runs = get_batch_export_runs_ok(client, team_id, batch_export_id)
    
        while batch_export_runs["count"] < number_of_runs:
            batch_export_runs = get_batch_export_runs_ok(client, team_id, batch_export_id)
            time.sleep(1)
            if (dt.datetime.utcnow() - start).seconds > timeout:
>               raise TimeoutError(f"BatchExportRuns never created. count:{batch_export_runs['count']}. Number of runs: {number_of_runs}. batch_export_runs: {batch_export_runs}")
E               TimeoutError: BatchExportRuns never created. count:0. Number of runs: 1. batch_export_runs: {'count': 0, 'next': None, 'previous': None, 'results': []}

posthog/api/test/batch_exports/test_reset.py:39: TimeoutError
--------------------------- snapshot report summary ----------------------------
4 snapshots failed. 6 snapshots passed. 633 snapshots unused.

Re-run pytest with --snapshot-update to delete unused snapshots.
=========================== short test summary info ============================
FAILED posthog/api/test/test_event.py::TestEvents::test_filter_events_by_properties
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_all_selected
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_incorrect_group_type
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_multiple_queries
FAILED posthog/api/test/test_feature_flag.py::TestBlastRadius::test_user_blast_radius_with_groups_zero_selected
FAILED posthog/api/test/test_insight.py::TestInsight::test_insight_funnels_hogql_breakdown
FAILED posthog/api/test/test_insight.py::TestInsight::test_insight_funnels_hogql_breakdown_single
FAILED posthog/api/test/test_persons_trends.py::TestPersonTrends::test_trends_people_endpoint_filters_search
FAILED posthog/api/test/test_persons_trends.py::TestPersonTrends::test_trends_people_endpoint_includes_recordings
FAILED posthog/api/test/test_plugin.py::TestPluginAPI::test_create_plugin_version_range_gt_next_major_ignore_on_cloud
FAILED posthog/api/test/test_query.py::TestQuery::test_valid_recent_performance_pageviews
FAILED posthog/api/test/test_signup.py::TestInviteSignupAPI::test_api_invite_sign_up_where_there_are_no_default_non_private_projects
FAILED posthog/hogql/test/test_query.py::TestQuery::test_join_with_property_materialized_session_id
FAILED posthog/models/test/test_organization_model.py::TestOrganization::test_plugins_access_level_is_determined_based_on_realm
FAILED posthog/models/test/test_organization_model.py::TestOrganization::test_plugins_are_preinstalled_on_self_hosted
FAILED posthog/models/test/test_user_model.py::TestUser::test_analytics_metadata
FAILED posthog/plugins/test/test_utils.py::TestPluginsUtils::test_download_plugin_archive_github
FAILED posthog/plugins/test/test_utils.py::TestPluginsUtils::test_parse_github_urls
FAILED posthog/queries/session_recordings/test/test_session_recording_list_from_session_replay.py::TestClickhouseSessionRecordingsListFromSessionReplay::test_action_filter
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_with_person_filter
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_by_group_props_with_person_filter_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_breakdown_with_filter_groups_person_on_events_v2
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_by_multiple_groups_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props_event_with_no_group_data
FAILED posthog/queries/test/test_trends.py::TestTrends::test_filtering_with_group_props_person_on_events
FAILED posthog/queries/test/test_trends.py::TestTrends::test_trends_with_hogql_math
FAILED posthog/queries/trends/test/test_formula.py::TestFormula::test_breakdown_hogql
FAILED posthog/queries/trends/test/test_person.py::TestPerson::test_group_query_includes_recording_events
FAILED posthog/test/test_team.py::TestTeam::test_team_on_cloud_uses_feature_flag_to_determine_person_on_events
FAILED posthog/api/test/batch_exports/test_reset.py::TestReset::test_can_reset_export_run
===== 36 failed, 5 passed, 1 skipped, 2601 deselected in 63.11s (0:01:03) ======
